{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#%% Data Loading and Preprocessing\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Handle NaN values before using ast.literal_eval\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")  # Replace 'nan' with None\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None  # Return None if it cannot be evaluated\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):  # Skip if t5 is NaN\n",
    "            continue\n",
    "\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\n",
    "            \"MDR\": None,\n",
    "            \"t5\": row[\"t5\"],\n",
    "            \"class\": None\n",
    "        }\n",
    "        \n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals,\n",
    "                \"S1\": S1,\n",
    "                \"S2\": S2\n",
    "            })\n",
    "            \n",
    "            # Interpolate and fill missing values\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "            \n",
    "            data[batch_number][\"MDR\"] = MDR\n",
    "        else:\n",
    "            # Skip entry if sequences are missing\n",
    "            continue\n",
    "\n",
    "        # Assign classification label based on t5 bounds\n",
    "        if data[batch_number][\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif data[batch_number][\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "    \n",
    "    # Remove entries with empty MDR data\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "\n",
    "def iPlotCooperStandard(data, ID):\n",
    "    t = data[ID][\"MDR\"][\"time\"]\n",
    "    S1 = data[ID][\"MDR\"][\"S1\"]\n",
    "    S2 = data[ID][\"MDR\"][\"S2\"]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(t, S1, color='blue', label=r'$S_1$')\n",
    "    plt.plot(t, S2, color='red', label=r'$S_2$')\n",
    "    plt.xlabel('time')\n",
    "    plt.legend()\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "# Example plot for a specific batch ID\n",
    "ID = list(data.keys())[0]\n",
    "iPlotCooperStandard(data, ID)\n",
    "\n",
    "#%% Filtering Data Based on Sequence Length\n",
    "lens = [v[\"MDR\"].shape[0] for v in data.values()]\n",
    "max_len = max(lens)\n",
    "plt.scatter(np.arange(len(lens)), lens)\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "def len_condition(data, len_threshold):\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"].shape[0] >= len_threshold}\n",
    "    return data\n",
    "\n",
    "data = len_condition(data, 290)\n",
    "\n",
    "# Print counts for each class\n",
    "print(f'# low: {len([k for k, v in data.items() if v[\"class\"]==\"low\"])}')\n",
    "print(f'# high: {len([k for k, v in data.items() if v[\"class\"]==\"high\"])}')\n",
    "print(f'# normal: {len([k for k, v in data.items() if v[\"class\"]==\"normal\"])}')\n",
    "\n",
    "#%% Prepare Sequences and Targets for Classification\n",
    "def prepare_sequences(data_dict, max_len=max_len):\n",
    "    \"\"\"\n",
    "    Process data dictionary into padded sequences and classification targets.\n",
    "    Returns:\n",
    "        X: Padded and normalized sequences (n_samples, max_len, num_features)\n",
    "        y: Array of class labels (as integers)\n",
    "        scalers: List of fitted StandardScalers for each feature\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for batch_id in data_dict:\n",
    "        df_seq = data_dict[batch_id][\"MDR\"]\n",
    "        # Using features S1 and S2 only (exclude time)\n",
    "        seq = df_seq[['S1', 'S2']].values.astype('float32')\n",
    "        sequences.append(seq)\n",
    "        targets.append(data_dict[batch_id][\"class\"])  # class label as string\n",
    "    \n",
    "    # Filter out empty sequences if any\n",
    "    non_empty = [s for s in sequences if len(s) > 0]\n",
    "    filtered_targets = [t for s, t in zip(sequences, targets) if len(s) > 0]\n",
    "    \n",
    "    # Map string labels to integer codes\n",
    "    class_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "    filtered_targets = [class_map[t] for t in filtered_targets]\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequences(\n",
    "        non_empty,\n",
    "        maxlen=max_len,\n",
    "        dtype='float32',\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    \n",
    "    # Normalize features\n",
    "    scalers = []\n",
    "    normalized = []\n",
    "    for feature_idx in range(padded_sequences.shape[2]):\n",
    "        feature_data = padded_sequences[:, :, feature_idx].reshape(-1, 1)\n",
    "        scaler = StandardScaler().fit(feature_data)\n",
    "        scalers.append(scaler)\n",
    "        normalized_feature = scaler.transform(feature_data).reshape(\n",
    "            padded_sequences.shape[0], padded_sequences.shape[1], 1)\n",
    "        normalized.append(normalized_feature)\n",
    "    \n",
    "    X = np.concatenate(normalized, axis=2)\n",
    "    y = np.array(filtered_targets)\n",
    "    return X, y, scalers\n",
    "\n",
    "X, y, scalers = prepare_sequences(data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "#%% Define the LSTM Model for Classification\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Masking(mask_value=0., input_shape=input_shape),\n",
    "        LSTM(32, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(16),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(3, activation='softmax')  # 3 output classes: low, normal, high\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',  # using integer labels\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "model.summary()\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights based on the training labels\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(class_weight_dict)\n",
    "\n",
    "# Then pass the computed class_weight_dict to model.fit:\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#%% Evaluate the Model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#%% Compute and Display the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions (as probabilities) and convert to class labels\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Define class labels for display (ensure order matches the mapping: 0: low, 1: normal, 2: high)\n",
    "labels = [\"low\", \"normal\", \"high\"]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
