{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "\n",
      "Training binary model for class 0 (low)\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 58s 281ms/step - loss: 0.7033 - accuracy: 0.5769 - val_loss: 0.8032 - val_accuracy: 0.1078\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 48s 255ms/step - loss: 0.6998 - accuracy: 0.4230 - val_loss: 0.6835 - val_accuracy: 0.9296\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 49s 259ms/step - loss: 0.7005 - accuracy: 0.6207 - val_loss: 0.7386 - val_accuracy: 0.1078\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 46s 245ms/step - loss: 0.6963 - accuracy: 0.3920 - val_loss: 0.6732 - val_accuracy: 0.9207\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6975 - accuracy: 0.5154 - val_loss: 0.6980 - val_accuracy: 0.3129\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 48s 257ms/step - loss: 0.6978 - accuracy: 0.5267 - val_loss: 0.7295 - val_accuracy: 0.1183\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 50s 264ms/step - loss: 0.7000 - accuracy: 0.5352 - val_loss: 0.6857 - val_accuracy: 0.8608\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 47s 252ms/step - loss: 0.6976 - accuracy: 0.5001 - val_loss: 0.7182 - val_accuracy: 0.1078\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6974 - accuracy: 0.4083 - val_loss: 0.6820 - val_accuracy: 0.8413\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6965 - accuracy: 0.5034 - val_loss: 0.6986 - val_accuracy: 0.2680\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6967 - accuracy: 0.4419 - val_loss: 0.7005 - val_accuracy: 0.2904\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6958 - accuracy: 0.4931 - val_loss: 0.7030 - val_accuracy: 0.1766\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6969 - accuracy: 0.5341 - val_loss: 0.7149 - val_accuracy: 0.1183\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6964 - accuracy: 0.3686 - val_loss: 0.6728 - val_accuracy: 0.9296\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6960 - accuracy: 0.5687 - val_loss: 0.6970 - val_accuracy: 0.1946\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6956 - accuracy: 0.4849 - val_loss: 0.7020 - val_accuracy: 0.1452\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6996 - accuracy: 0.4504 - val_loss: 0.6927 - val_accuracy: 0.6183\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6969 - accuracy: 0.3132 - val_loss: 0.6944 - val_accuracy: 0.4237\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6974 - accuracy: 0.5479 - val_loss: 0.6948 - val_accuracy: 0.4970\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6947 - accuracy: 0.5022 - val_loss: 0.6941 - val_accuracy: 0.4895\n",
      "\n",
      "Training binary model for class 1 (normal)\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 55s 260ms/step - loss: 0.7014 - accuracy: 0.5042 - val_loss: 0.7343 - val_accuracy: 0.1123\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6967 - accuracy: 0.4596 - val_loss: 0.6885 - val_accuracy: 0.7934\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.7010 - accuracy: 0.4719 - val_loss: 0.6960 - val_accuracy: 0.7710\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 47s 248ms/step - loss: 0.6982 - accuracy: 0.5142 - val_loss: 0.7354 - val_accuracy: 0.1123\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.6962 - accuracy: 0.4468 - val_loss: 0.7064 - val_accuracy: 0.2470\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6968 - accuracy: 0.5017 - val_loss: 0.6948 - val_accuracy: 0.6347\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6973 - accuracy: 0.4066 - val_loss: 0.6709 - val_accuracy: 0.8204\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6988 - accuracy: 0.5727 - val_loss: 0.7015 - val_accuracy: 0.5524\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6976 - accuracy: 0.4834 - val_loss: 0.7020 - val_accuracy: 0.3593\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6980 - accuracy: 0.5322 - val_loss: 0.7092 - val_accuracy: 0.1617\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6973 - accuracy: 0.4038 - val_loss: 0.7006 - val_accuracy: 0.4251\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6967 - accuracy: 0.5439 - val_loss: 0.7187 - val_accuracy: 0.1168\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6962 - accuracy: 0.4709 - val_loss: 0.7162 - val_accuracy: 0.1302\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6974 - accuracy: 0.4414 - val_loss: 0.7085 - val_accuracy: 0.2290\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 51s 274ms/step - loss: 0.6962 - accuracy: 0.5176 - val_loss: 0.6918 - val_accuracy: 0.6781\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 53s 279ms/step - loss: 0.6973 - accuracy: 0.4888 - val_loss: 0.7053 - val_accuracy: 0.2530\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 48s 257ms/step - loss: 0.6971 - accuracy: 0.4794 - val_loss: 0.6992 - val_accuracy: 0.4296\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 45s 241ms/step - loss: 0.6965 - accuracy: 0.5001 - val_loss: 0.7051 - val_accuracy: 0.2710\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 45s 240ms/step - loss: 0.6962 - accuracy: 0.4926 - val_loss: 0.7120 - val_accuracy: 0.1617\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 45s 240ms/step - loss: 0.6973 - accuracy: 0.4278 - val_loss: 0.6824 - val_accuracy: 0.8129\n",
      "\n",
      "Training binary model for class 2 (high)\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 53s 254ms/step - loss: 0.7045 - accuracy: 0.4538 - val_loss: 0.7144 - val_accuracy: 0.0823\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.7018 - accuracy: 0.4354 - val_loss: 0.6995 - val_accuracy: 0.8099\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.7004 - accuracy: 0.4359 - val_loss: 0.6937 - val_accuracy: 0.7799\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.7000 - accuracy: 0.5199 - val_loss: 0.7404 - val_accuracy: 0.0719\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6971 - accuracy: 0.5129 - val_loss: 0.7260 - val_accuracy: 0.0719\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6951 - accuracy: 0.3593 - val_loss: 0.6668 - val_accuracy: 0.8518\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6967 - accuracy: 0.6244 - val_loss: 0.7539 - val_accuracy: 0.0719\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6985 - accuracy: 0.3943 - val_loss: 0.6857 - val_accuracy: 0.8548\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6957 - accuracy: 0.5839 - val_loss: 0.7166 - val_accuracy: 0.1512\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6973 - accuracy: 0.4934 - val_loss: 0.7153 - val_accuracy: 0.2231\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6953 - accuracy: 0.5227 - val_loss: 0.7076 - val_accuracy: 0.3817\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6969 - accuracy: 0.5132 - val_loss: 0.6784 - val_accuracy: 0.8518\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6996 - accuracy: 0.6797 - val_loss: 0.7062 - val_accuracy: 0.4162\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6974 - accuracy: 0.4188 - val_loss: 0.6795 - val_accuracy: 0.8488\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6978 - accuracy: 0.6165 - val_loss: 0.7115 - val_accuracy: 0.3368\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6981 - accuracy: 0.5680 - val_loss: 0.7195 - val_accuracy: 0.2186\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6977 - accuracy: 0.5002 - val_loss: 0.7123 - val_accuracy: 0.3368\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6969 - accuracy: 0.5227 - val_loss: 0.7100 - val_accuracy: 0.4162\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6975 - accuracy: 0.4613 - val_loss: 0.6930 - val_accuracy: 0.6901\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6975 - accuracy: 0.5982 - val_loss: 0.7083 - val_accuracy: 0.3234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  8  45  20]\n",
      " [ 54 966 440]\n",
      " [  3  82  50]]\n",
      "Accuracy for class 0 (low): 0.1096\n",
      "Accuracy for class 1 (normal): 0.6616\n",
      "Accuracy for class 2 (high): 0.3704\n",
      "\n",
      "Average Classification Accuracy: 0.3805\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.12      0.11      0.12        73\n",
      "      normal       0.88      0.66      0.76      1460\n",
      "        high       0.10      0.37      0.16       135\n",
      "\n",
      "    accuracy                           0.61      1668\n",
      "   macro avg       0.37      0.38      0.34      1668\n",
      "weighted avg       0.79      0.61      0.68      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Read Excel Data and Organize It\n",
    "\n",
    "# %%\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack the tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Prepare Data for Training\n",
    "# We convert the variable-length sequences to a padded format.\n",
    "# We also scale the valid (non-padded) parts of the sequences.\n",
    "\n",
    "# %%\n",
    "X = []\n",
    "y = []\n",
    "# Map the labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    # Use only the S1 and S2 columns as features\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences (using -10 as pad value, so that Masking will ignore these)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale the data using a global scaler, ignoring the padded values.\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Split Data into Train and Test Sets\n",
    "# We'll split the dataset while preserving the class distribution.\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# For later reporting, create an inverse label map\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Build and Train Individual Binary (One-vs-All) Models\n",
    "# We build a binary classifier for each class.\n",
    "# Each model outputs the probability that the input belongs to that class.\n",
    "# We use binary cross-entropy loss and a sigmoid output.\n",
    "# You can adjust the architecture, epochs, and other parameters as needed.\n",
    "\n",
    "# %%\n",
    "def build_binary_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-10., input_shape=input_shape))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "binary_models = {}\n",
    "\n",
    "# Train one binary model per class (one-vs-all)\n",
    "for cls in range(3):\n",
    "    print(f\"\\nTraining binary model for class {cls} ({inv_label_map[cls]})\")\n",
    "    # Create binary labels: 1 if the sample belongs to this class, else 0.\n",
    "    y_train_bin = (y_train == cls).astype(int)\n",
    "    \n",
    "    # Compute class weights for the binary problem\n",
    "    classes_bin = np.unique(y_train_bin)\n",
    "    class_weights_bin = compute_class_weight(class_weight='balanced', classes=classes_bin, y=y_train_bin)\n",
    "    class_weight_dict_bin = {cls_val: weight for cls_val, weight in zip(classes_bin, class_weights_bin)}\n",
    "    \n",
    "    model_bin = build_binary_model((max_len, 2))\n",
    "    model_bin.fit(X_train, y_train_bin, \n",
    "                  epochs=20, \n",
    "                  batch_size=32, \n",
    "                  validation_split=0.1, \n",
    "                  class_weight=class_weight_dict_bin,\n",
    "                  verbose=1)\n",
    "    binary_models[cls] = model_bin\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Combine the Models for Final Decision\n",
    "# For each test sample, we obtain the probability from each binary model.\n",
    "# Then we select the class with the highest probability.\n",
    "# (You could also apply thresholds if desired.)\n",
    "\n",
    "# %%\n",
    "# For each model, predict probability on test data.\n",
    "y_pred_probs = np.zeros((len(X_test), 3))\n",
    "for cls in range(3):\n",
    "    y_pred_probs[:, cls] = binary_models[cls].predict(X_test).flatten()\n",
    "\n",
    "# Final prediction: choose the class with the highest probability.\n",
    "y_pred_combined = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6. Evaluate the Combined Model\n",
    "# We compute the confusion matrix and report per-class accuracy.\n",
    "\n",
    "# %%\n",
    "cm = confusion_matrix(y_test, y_pred_combined)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute accuracy for each individual class\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_combined, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Training set shape: (6671, 304, 2)\n",
      "Test set shape: (1668, 304, 2)\n",
      "Training class distribution before augmentation: {0: 292, 1: 5837, 2: 542}\n",
      "Training class distribution after augmentation: {0: 5837, 1: 5837, 2: 5837}\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_4 (Masking)         (None, 304, 2)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                17152     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,347\n",
      "Trainable params: 17,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "493/493 [==============================] - 136s 264ms/step - loss: 1.1035 - accuracy: 0.3380 - val_loss: 1.0959 - val_accuracy: 0.3676\n",
      "Epoch 2/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0999 - accuracy: 0.3416 - val_loss: 1.0955 - val_accuracy: 0.3744\n",
      "Epoch 3/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0986 - accuracy: 0.3392 - val_loss: 1.0947 - val_accuracy: 0.3476\n",
      "Epoch 4/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0979 - accuracy: 0.3430 - val_loss: 1.0948 - val_accuracy: 0.3773\n",
      "Epoch 5/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0968 - accuracy: 0.3508 - val_loss: 1.0958 - val_accuracy: 0.3465\n",
      "Epoch 6/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0969 - accuracy: 0.3510 - val_loss: 1.0944 - val_accuracy: 0.3607\n",
      "Epoch 7/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0957 - accuracy: 0.3574 - val_loss: 1.0929 - val_accuracy: 0.3596\n",
      "Epoch 8/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0976 - accuracy: 0.3668 - val_loss: 1.0929 - val_accuracy: 0.3716\n",
      "Epoch 9/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0953 - accuracy: 0.3637 - val_loss: 1.0929 - val_accuracy: 0.3659\n",
      "Epoch 10/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0939 - accuracy: 0.3684 - val_loss: 1.0915 - val_accuracy: 0.3796\n",
      "Epoch 11/20\n",
      "493/493 [==============================] - 124s 252ms/step - loss: 1.0956 - accuracy: 0.3591 - val_loss: 1.0914 - val_accuracy: 0.3590\n",
      "Epoch 12/20\n",
      "493/493 [==============================] - 124s 251ms/step - loss: 1.0934 - accuracy: 0.3693 - val_loss: 1.0931 - val_accuracy: 0.3796\n",
      "Epoch 13/20\n",
      "493/493 [==============================] - 134s 271ms/step - loss: 1.0906 - accuracy: 0.3764 - val_loss: 1.0816 - val_accuracy: 0.3881\n",
      "Epoch 14/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0882 - accuracy: 0.3753 - val_loss: 1.0828 - val_accuracy: 0.3950\n",
      "Epoch 15/20\n",
      "493/493 [==============================] - 127s 257ms/step - loss: 1.0853 - accuracy: 0.3766 - val_loss: 1.1101 - val_accuracy: 0.3779\n",
      "Epoch 16/20\n",
      "493/493 [==============================] - 133s 269ms/step - loss: 1.0837 - accuracy: 0.3807 - val_loss: 1.0853 - val_accuracy: 0.3955\n",
      "Epoch 17/20\n",
      "493/493 [==============================] - 126s 255ms/step - loss: 1.0839 - accuracy: 0.3843 - val_loss: 1.0851 - val_accuracy: 0.3710\n",
      "Epoch 18/20\n",
      "493/493 [==============================] - 126s 255ms/step - loss: 1.0947 - accuracy: 0.3599 - val_loss: 1.0913 - val_accuracy: 0.3676\n",
      "Epoch 19/20\n",
      "493/493 [==============================] - 126s 255ms/step - loss: 1.0896 - accuracy: 0.3757 - val_loss: 1.0798 - val_accuracy: 0.3836\n",
      "Epoch 20/20\n",
      "493/493 [==============================] - 121s 245ms/step - loss: 1.0853 - accuracy: 0.3918 - val_loss: 1.0778 - val_accuracy: 0.3984\n",
      "Test Loss: 1.1165893077850342\n",
      "Test Accuracy: 0.17386090755462646\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 35   7  31]\n",
      " [523 176 761]\n",
      " [ 39  17  79]]\n",
      "Accuracy for class 0 (low): 0.4795\n",
      "Accuracy for class 1 (normal): 0.1205\n",
      "Accuracy for class 2 (high): 0.5852\n",
      "\n",
      "Average Classification Accuracy: 0.3951\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.06      0.48      0.10        73\n",
      "      normal       0.88      0.12      0.21      1460\n",
      "        high       0.09      0.59      0.16       135\n",
      "\n",
      "    accuracy                           0.17      1668\n",
      "   macro avg       0.34      0.40      0.16      1668\n",
      "weighted avg       0.78      0.17      0.20      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Read Excel Data and Organize It\n",
    "\n",
    "# %%\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack the tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Prepare Data for Training\n",
    "# Convert the variable-length time-series sequences into padded arrays.\n",
    "# We use -10 as the pad value so that the Masking layer can ignore it.\n",
    "# After padding, we scale the valid (non-padded) values with a global StandardScaler.\n",
    "\n",
    "# %%\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    # Use only the S1 and S2 columns as features\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences (padding/truncating at the end)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale the data: first, gather all valid (non-padded) rows\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "# Apply the scaler to each sequence (only the valid timesteps)\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Split Data into Train and Test Sets\n",
    "# We use stratification so that the class imbalance is preserved in the split.\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting purposes, create an inverse label map:\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Data Augmentation for Minority Classes\n",
    "# We define a simple jittering function to add Gaussian noise to the valid parts\n",
    "# of a time-series. Then, we oversample the minority classes (here, \"low\" and \"high\")\n",
    "# to match the number of samples in the majority class (normally \"normal\").\n",
    "\n",
    "# %%\n",
    "def augment_time_series(sequence, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to valid (non-padded) timesteps in the sequence.\n",
    "    The noise_level parameter controls the standard deviation of the noise.\n",
    "    \"\"\"\n",
    "    augmented = sequence.copy()\n",
    "    # Identify valid rows (those not equal to the pad value)\n",
    "    valid_mask = ~np.all(sequence == -10., axis=1)\n",
    "    if np.any(valid_mask):\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_level, size=augmented[valid_mask].shape)\n",
    "        augmented[valid_mask] += noise\n",
    "    return augmented\n",
    "\n",
    "def augment_minority_class(X, y, target_class, target_count, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    For the given target_class, generate augmented samples using jittering\n",
    "    until the total count for that class reaches target_count.\n",
    "    \"\"\"\n",
    "    indices = np.where(y == target_class)[0]\n",
    "    current_count = len(indices)\n",
    "    num_to_augment = target_count - current_count\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for _ in range(num_to_augment):\n",
    "        # Randomly choose one sample from the existing target_class samples\n",
    "        idx = np.random.choice(indices)\n",
    "        sample = X[idx]\n",
    "        aug_sample = augment_time_series(sample, noise_level)\n",
    "        augmented_X.append(aug_sample)\n",
    "        augmented_y.append(target_class)\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# Check current training class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "train_counts = dict(zip(unique, counts))\n",
    "print(\"Training class distribution before augmentation:\", train_counts)\n",
    "\n",
    "# Define the target count as the count of the majority class\n",
    "target_count = max(train_counts.values())\n",
    "\n",
    "# Augment the minority classes (for example, classes 0 and 2)\n",
    "augmented_X = []\n",
    "augmented_y = []\n",
    "\n",
    "for cls in [0, 2]:\n",
    "    if train_counts[cls] < target_count:\n",
    "        X_aug, y_aug = augment_minority_class(X_train, y_train, cls, target_count, noise_level=0.1)\n",
    "        augmented_X.append(X_aug)\n",
    "        augmented_y.append(y_aug)\n",
    "\n",
    "if augmented_X:\n",
    "    X_augmented = np.concatenate(augmented_X, axis=0)\n",
    "    y_augmented = np.concatenate(augmented_y, axis=0)\n",
    "    # Combine with the original training set\n",
    "    X_train_aug = np.concatenate([X_train, X_augmented], axis=0)\n",
    "    y_train_aug = np.concatenate([y_train, y_augmented], axis=0)\n",
    "else:\n",
    "    X_train_aug = X_train\n",
    "    y_train_aug = y_train\n",
    "\n",
    "# Shuffle the augmented training set\n",
    "shuffle_idx = np.random.permutation(len(X_train_aug))\n",
    "X_train_aug = X_train_aug[shuffle_idx]\n",
    "y_train_aug = y_train_aug[shuffle_idx]\n",
    "\n",
    "# Print new training set distribution\n",
    "unique, counts = np.unique(y_train_aug, return_counts=True)\n",
    "print(\"Training class distribution after augmentation:\", dict(zip(unique, counts)))\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Build and Train the Model\n",
    "# We use an LSTM-based network with a Masking layer (ignoring the padded value)\n",
    "# and a Dense output layer with softmax activation. We train on the augmented dataset.\n",
    "\n",
    "# %%\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6. Evaluate the Model\n",
    "# We calculate the confusion matrix, per-class accuracy, and a full classification report.\n",
    "\n",
    "# %%\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Training set shape: (6671, 304, 2)\n",
      "Test set shape: (1668, 304, 2)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_5 (Masking)         (None, 304, 2)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                17152     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,347\n",
      "Trainable params: 17,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 60s 286ms/step - loss: 0.5669 - accuracy: 0.8667 - val_loss: 0.4704 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 49s 262ms/step - loss: 0.5284 - accuracy: 0.8736 - val_loss: 0.4700 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.5226 - accuracy: 0.8736 - val_loss: 0.4699 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.5224 - accuracy: 0.8736 - val_loss: 0.4709 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 47s 252ms/step - loss: 0.5201 - accuracy: 0.8736 - val_loss: 0.4665 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 47s 252ms/step - loss: 0.5180 - accuracy: 0.8736 - val_loss: 0.4667 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 51s 271ms/step - loss: 0.5162 - accuracy: 0.8736 - val_loss: 0.4659 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 54s 288ms/step - loss: 0.5147 - accuracy: 0.8736 - val_loss: 0.4698 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 51s 273ms/step - loss: 0.5149 - accuracy: 0.8736 - val_loss: 0.4675 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 52s 277ms/step - loss: 0.5141 - accuracy: 0.8736 - val_loss: 0.4719 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 52s 275ms/step - loss: 0.5138 - accuracy: 0.8736 - val_loss: 0.4669 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 48s 255ms/step - loss: 0.5122 - accuracy: 0.8736 - val_loss: 0.4724 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 48s 256ms/step - loss: 0.5130 - accuracy: 0.8736 - val_loss: 0.4713 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 48s 256ms/step - loss: 0.5132 - accuracy: 0.8736 - val_loss: 0.4671 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 50s 264ms/step - loss: 0.5123 - accuracy: 0.8736 - val_loss: 0.4684 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.5117 - accuracy: 0.8736 - val_loss: 0.4664 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.5121 - accuracy: 0.8736 - val_loss: 0.4662 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.5113 - accuracy: 0.8736 - val_loss: 0.4667 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.5110 - accuracy: 0.8736 - val_loss: 0.4667 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 49s 259ms/step - loss: 0.5130 - accuracy: 0.8732 - val_loss: 0.4663 - val_accuracy: 0.8877\n",
      "Test Loss: 0.5008330941200256\n",
      "Test Accuracy: 0.8752997517585754\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0   73    0]\n",
      " [   0 1460    0]\n",
      " [   0  135    0]]\n",
      "Accuracy for class 0 (low): 0.0000\n",
      "Accuracy for class 1 (normal): 1.0000\n",
      "Accuracy for class 2 (high): 0.0000\n",
      "\n",
      "Average Classification Accuracy: 0.3333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.00      0.00      0.00        73\n",
      "      normal       0.88      1.00      0.93      1460\n",
      "        high       0.00      0.00      0.00       135\n",
      "\n",
      "    accuracy                           0.88      1668\n",
      "   macro avg       0.29      0.33      0.31      1668\n",
      "weighted avg       0.77      0.88      0.82      1668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Define the Custom Focal Loss Function\n",
    "# This focal loss is designed for sparse (integer) labels. It converts the labels to one-hot,\n",
    "# applies clipping for numerical stability, computes cross-entropy, and weights it based on the focal loss idea.\n",
    "\n",
    "def sparse_focal_loss(gamma=2., alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-class classification with sparse labels.\n",
    "    \n",
    "    Args:\n",
    "        gamma (float): Focusing parameter for modulating factor (1-p).\n",
    "        alpha (float): Weighting factor for the rare class.\n",
    "    \n",
    "    Returns:\n",
    "        A loss function that computes the focal loss.\n",
    "    \"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        # Convert sparse labels to one-hot encoding.\n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "        epsilon = 1e-7\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        # Compute cross-entropy loss.\n",
    "        cross_entropy = -y_true_one_hot * tf.math.log(y_pred)\n",
    "        # Compute the modulating factor.\n",
    "        weights = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        loss = weights * cross_entropy\n",
    "        # Sum the loss over classes, then average over the batch.\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "    return loss_fn\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Read Excel Data and Organize It\n",
    "# We read two sheets from the Excel file, concatenate them, and process each row to extract:\n",
    "# - The multivariate time-series (MDR) with two columns S1 and S2.\n",
    "# - The t5 scalar value.\n",
    "# - The class label based on t5 thresholds (low, normal, high).\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Prepare Data for Training\n",
    "# We build a list of sequences (each sequence is a 2D array with S1 and S2) and a list of labels.\n",
    "# Because the sequences have varying lengths, we pad them to the same length (using -10 as the pad value).\n",
    "# Then, we scale the valid (non-padded) parts of the sequences using a global StandardScaler.\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences so that all have the same length.\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) points.\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Split Data into Train and Test Sets\n",
    "# We split the data using stratification to preserve the class imbalance in both sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# Create an inverse label map for reporting.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Build and Train the Model Using Focal Loss\n",
    "# We build a simple LSTM model with a Masking layer to ignore the padded values.\n",
    "# The output layer uses softmax activation for the three classes.\n",
    "# We compile the model with our custom sparse focal loss function.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Use our custom focal loss in the compile step.\n",
    "model.compile(loss=sparse_focal_loss(gamma=2.0, alpha=0.25), optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6. Evaluate the Model\n",
    "# We evaluate on the test set, print the confusion matrix, per-class accuracies, and a full classification report.\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Get predictions on the test set.\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix.\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate per-class accuracy.\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Training set shape: (6671, 304, 2)\n",
      "Test set shape: (1668, 304, 2)\n",
      "\n",
      "Training model 1/5\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 55s 263ms/step - loss: 0.5176 - accuracy: 0.8592 - val_loss: 0.4292 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 45s 242ms/step - loss: 0.4799 - accuracy: 0.8736 - val_loss: 0.4269 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4767 - accuracy: 0.8736 - val_loss: 0.4266 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 48s 258ms/step - loss: 0.4737 - accuracy: 0.8736 - val_loss: 0.4267 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 49s 259ms/step - loss: 0.4726 - accuracy: 0.8736 - val_loss: 0.4279 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 47s 252ms/step - loss: 0.4727 - accuracy: 0.8736 - val_loss: 0.4256 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.4731 - accuracy: 0.8734 - val_loss: 0.4300 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 48s 255ms/step - loss: 0.4682 - accuracy: 0.8736 - val_loss: 0.4255 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 48s 253ms/step - loss: 0.4658 - accuracy: 0.8736 - val_loss: 0.4255 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4710 - accuracy: 0.8736 - val_loss: 0.4275 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4698 - accuracy: 0.8736 - val_loss: 0.4266 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4697 - accuracy: 0.8736 - val_loss: 0.4257 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4682 - accuracy: 0.8736 - val_loss: 0.4267 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4661 - accuracy: 0.8736 - val_loss: 0.4281 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4679 - accuracy: 0.8736 - val_loss: 0.4269 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4678 - accuracy: 0.8736 - val_loss: 0.4271 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4651 - accuracy: 0.8736 - val_loss: 0.4298 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4689 - accuracy: 0.8736 - val_loss: 0.4326 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4688 - accuracy: 0.8736 - val_loss: 0.4272 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4662 - accuracy: 0.8736 - val_loss: 0.4272 - val_accuracy: 0.8877\n",
      "\n",
      "Training model 2/5\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 55s 262ms/step - loss: 0.5121 - accuracy: 0.8664 - val_loss: 0.4304 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 48s 257ms/step - loss: 0.4769 - accuracy: 0.8736 - val_loss: 0.4277 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 48s 255ms/step - loss: 0.4739 - accuracy: 0.8736 - val_loss: 0.4297 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4744 - accuracy: 0.8736 - val_loss: 0.4261 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4735 - accuracy: 0.8736 - val_loss: 0.4264 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 48s 253ms/step - loss: 0.4728 - accuracy: 0.8734 - val_loss: 0.4276 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 50s 266ms/step - loss: 0.4721 - accuracy: 0.8736 - val_loss: 0.4315 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 54s 289ms/step - loss: 0.4676 - accuracy: 0.8736 - val_loss: 0.4270 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 52s 278ms/step - loss: 0.4690 - accuracy: 0.8736 - val_loss: 0.4279 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 52s 274ms/step - loss: 0.4682 - accuracy: 0.8736 - val_loss: 0.4260 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 51s 274ms/step - loss: 0.4724 - accuracy: 0.8736 - val_loss: 0.4274 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 52s 275ms/step - loss: 0.4676 - accuracy: 0.8736 - val_loss: 0.4270 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 52s 279ms/step - loss: 0.4690 - accuracy: 0.8736 - val_loss: 0.4263 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 52s 279ms/step - loss: 0.4698 - accuracy: 0.8736 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 52s 275ms/step - loss: 0.4693 - accuracy: 0.8736 - val_loss: 0.4273 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 51s 273ms/step - loss: 0.4673 - accuracy: 0.8736 - val_loss: 0.4261 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 52s 275ms/step - loss: 0.4687 - accuracy: 0.8736 - val_loss: 0.4280 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 55s 292ms/step - loss: 0.4674 - accuracy: 0.8736 - val_loss: 0.4261 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 57s 301ms/step - loss: 0.4662 - accuracy: 0.8736 - val_loss: 0.4288 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 54s 288ms/step - loss: 0.4656 - accuracy: 0.8736 - val_loss: 0.4264 - val_accuracy: 0.8877\n",
      "\n",
      "Training model 3/5\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 67s 322ms/step - loss: 0.5095 - accuracy: 0.8619 - val_loss: 0.4286 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 51s 269ms/step - loss: 0.4772 - accuracy: 0.8736 - val_loss: 0.4298 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 55s 292ms/step - loss: 0.4737 - accuracy: 0.8736 - val_loss: 0.4288 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 56s 296ms/step - loss: 0.4718 - accuracy: 0.8736 - val_loss: 0.4274 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 56s 296ms/step - loss: 0.4719 - accuracy: 0.8736 - val_loss: 0.4275 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 50s 267ms/step - loss: 0.4711 - accuracy: 0.8736 - val_loss: 0.4271 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4689 - accuracy: 0.8736 - val_loss: 0.4274 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4715 - accuracy: 0.8736 - val_loss: 0.4284 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 47s 253ms/step - loss: 0.4705 - accuracy: 0.8736 - val_loss: 0.4261 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4698 - accuracy: 0.8736 - val_loss: 0.4270 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 50s 266ms/step - loss: 0.4682 - accuracy: 0.8736 - val_loss: 0.4317 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4719 - accuracy: 0.8736 - val_loss: 0.4342 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 49s 261ms/step - loss: 0.4667 - accuracy: 0.8736 - val_loss: 0.4268 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 52s 275ms/step - loss: 0.4689 - accuracy: 0.8736 - val_loss: 0.4291 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 48s 254ms/step - loss: 0.4690 - accuracy: 0.8736 - val_loss: 0.4258 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 50s 266ms/step - loss: 0.4672 - accuracy: 0.8736 - val_loss: 0.4341 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 50s 265ms/step - loss: 0.4675 - accuracy: 0.8736 - val_loss: 0.4274 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 48s 258ms/step - loss: 0.4656 - accuracy: 0.8736 - val_loss: 0.4294 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 49s 261ms/step - loss: 0.4643 - accuracy: 0.8736 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 48s 257ms/step - loss: 0.4663 - accuracy: 0.8736 - val_loss: 0.4299 - val_accuracy: 0.8877\n",
      "\n",
      "Training model 4/5\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 53s 258ms/step - loss: 0.5200 - accuracy: 0.8636 - val_loss: 0.4279 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 49s 263ms/step - loss: 0.4783 - accuracy: 0.8736 - val_loss: 0.4280 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 50s 266ms/step - loss: 0.4774 - accuracy: 0.8736 - val_loss: 0.4273 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 53s 281ms/step - loss: 0.4728 - accuracy: 0.8736 - val_loss: 0.4310 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 47s 252ms/step - loss: 0.4711 - accuracy: 0.8736 - val_loss: 0.4292 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 50s 266ms/step - loss: 0.4718 - accuracy: 0.8736 - val_loss: 0.4277 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 49s 262ms/step - loss: 0.4699 - accuracy: 0.8736 - val_loss: 0.4305 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 48s 257ms/step - loss: 0.4709 - accuracy: 0.8736 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4692 - accuracy: 0.8736 - val_loss: 0.4282 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 49s 258ms/step - loss: 0.4678 - accuracy: 0.8736 - val_loss: 0.4260 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 49s 259ms/step - loss: 0.4701 - accuracy: 0.8736 - val_loss: 0.4254 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 52s 276ms/step - loss: 0.4702 - accuracy: 0.8736 - val_loss: 0.4295 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 54s 288ms/step - loss: 0.4693 - accuracy: 0.8736 - val_loss: 0.4270 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 54s 288ms/step - loss: 0.4673 - accuracy: 0.8736 - val_loss: 0.4264 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 53s 281ms/step - loss: 0.4662 - accuracy: 0.8736 - val_loss: 0.4267 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 52s 279ms/step - loss: 0.4696 - accuracy: 0.8736 - val_loss: 0.4270 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 52s 274ms/step - loss: 0.4697 - accuracy: 0.8736 - val_loss: 0.4269 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 54s 288ms/step - loss: 0.4681 - accuracy: 0.8736 - val_loss: 0.4287 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 51s 273ms/step - loss: 0.4648 - accuracy: 0.8736 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 53s 282ms/step - loss: 0.4655 - accuracy: 0.8736 - val_loss: 0.4260 - val_accuracy: 0.8877\n",
      "\n",
      "Training model 5/5\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 64s 306ms/step - loss: 0.5070 - accuracy: 0.8679 - val_loss: 0.4283 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 48s 253ms/step - loss: 0.4836 - accuracy: 0.8736 - val_loss: 0.4325 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4740 - accuracy: 0.8736 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.4756 - accuracy: 0.8736 - val_loss: 0.4271 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4728 - accuracy: 0.8736 - val_loss: 0.4291 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4699 - accuracy: 0.8736 - val_loss: 0.4270 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 48s 255ms/step - loss: 0.4718 - accuracy: 0.8734 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 43s 227ms/step - loss: 0.4697 - accuracy: 0.8736 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 43s 227ms/step - loss: 0.4683 - accuracy: 0.8736 - val_loss: 0.4262 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 43s 228ms/step - loss: 0.4701 - accuracy: 0.8736 - val_loss: 0.4257 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 45s 240ms/step - loss: 0.4708 - accuracy: 0.8736 - val_loss: 0.4261 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4692 - accuracy: 0.8736 - val_loss: 0.4276 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 51s 269ms/step - loss: 0.4666 - accuracy: 0.8736 - val_loss: 0.4262 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 50s 266ms/step - loss: 0.4713 - accuracy: 0.8736 - val_loss: 0.4271 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 50s 264ms/step - loss: 0.4666 - accuracy: 0.8736 - val_loss: 0.4256 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 50s 263ms/step - loss: 0.4687 - accuracy: 0.8736 - val_loss: 0.4267 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 50s 263ms/step - loss: 0.4686 - accuracy: 0.8736 - val_loss: 0.4256 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 51s 270ms/step - loss: 0.4678 - accuracy: 0.8736 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 47s 252ms/step - loss: 0.4662 - accuracy: 0.8736 - val_loss: 0.4263 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 47s 248ms/step - loss: 0.4679 - accuracy: 0.8736 - val_loss: 0.4282 - val_accuracy: 0.8877\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0   73    0]\n",
      " [   0 1460    0]\n",
      " [   0  135    0]]\n",
      "Accuracy for class 0 (low): 0.0000\n",
      "Accuracy for class 1 (normal): 1.0000\n",
      "Accuracy for class 2 (high): 0.0000\n",
      "\n",
      "Average Classification Accuracy: 0.3333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.00      0.00      0.00        73\n",
      "      normal       0.88      1.00      0.93      1460\n",
      "        high       0.00      0.00      0.00       135\n",
      "\n",
      "    accuracy                           0.88      1668\n",
      "   macro avg       0.29      0.33      0.31      1668\n",
      "weighted avg       0.77      0.88      0.82      1668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility (across Python, NumPy and TensorFlow)\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Read Excel Data and Organize It\n",
    "# We read the Excel sheets, combine them, and then process each row to extract:\n",
    "# - The MDR time-series with S1 and S2\n",
    "# - The t5 scalar value\n",
    "# - The class label (\"low\", \"normal\", \"high\") based on t5 thresholds\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Prepare Data for Training\n",
    "# Convert the variable-length sequences into padded arrays.\n",
    "# We use -10 as the pad value so that the Masking layer ignores it.\n",
    "# Then we scale only the valid (non-padded) data points using a global StandardScaler.\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32',\n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) points\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Split Data into Train and Test Sets\n",
    "# We use stratification to preserve the class distribution.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=seed_value, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting, create an inverse label map.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Define the Model Architecture and Ensemble Training\n",
    "# We define a function to build our LSTM-based model.\n",
    "# Then we train multiple models (ensemble members) with the same architecture.\n",
    "# Their predictions will later be combined for a final decision.\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-10., input_shape=input_shape))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Number of ensemble members\n",
    "num_ensemble = 5\n",
    "ensemble_models = []\n",
    "\n",
    "for i in range(num_ensemble):\n",
    "    print(f\"\\nTraining model {i+1}/{num_ensemble}\")\n",
    "    # To ensure some diversity, you could reinitialize seeds or use different hyperparameters.\n",
    "    tf.random.set_seed(seed_value + i)\n",
    "    model = build_model((max_len, 2))\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Ensemble Predictions and Evaluation\n",
    "# For each test sample, we average the predictions from all ensemble members\n",
    "# and then choose the class with the highest averaged probability.\n",
    "# We then compute the confusion matrix, per-class accuracies, and classification report.\n",
    "\n",
    "# Get ensemble predictions: average softmax probabilities over models.\n",
    "ensemble_probs = np.zeros((len(X_test), 3))\n",
    "for model in ensemble_models:\n",
    "    ensemble_probs += model.predict(X_test)\n",
    "ensemble_probs /= num_ensemble\n",
    "\n",
    "# Final prediction: choose the class with the highest probability.\n",
    "y_pred_ensemble = np.argmax(ensemble_probs, axis=1)\n",
    "\n",
    "# Evaluate the ensemble predictions.\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Full training set shape: (6671, 304, 2)\n",
      "Test set shape: (1668, 304, 2)\n",
      "Base training set shape: (5336, 304, 2)\n",
      "Stacking set shape: (1335, 304, 2)\n",
      "\n",
      "Training base model 1/3\n",
      "Epoch 1/20\n",
      "151/151 [==============================] - 43s 254ms/step - loss: 0.5246 - accuracy: 0.8569 - val_loss: 0.4627 - val_accuracy: 0.8745\n",
      "Epoch 2/20\n",
      "151/151 [==============================] - 37s 244ms/step - loss: 0.4732 - accuracy: 0.8751 - val_loss: 0.4614 - val_accuracy: 0.8745\n",
      "Epoch 3/20\n",
      "151/151 [==============================] - 37s 242ms/step - loss: 0.4709 - accuracy: 0.8751 - val_loss: 0.4710 - val_accuracy: 0.8745\n",
      "Epoch 4/20\n",
      "151/151 [==============================] - 37s 244ms/step - loss: 0.4708 - accuracy: 0.8751 - val_loss: 0.4637 - val_accuracy: 0.8745\n",
      "Epoch 5/20\n",
      "151/151 [==============================] - 37s 244ms/step - loss: 0.4708 - accuracy: 0.8751 - val_loss: 0.4621 - val_accuracy: 0.8745\n",
      "Epoch 6/20\n",
      "151/151 [==============================] - 37s 243ms/step - loss: 0.4716 - accuracy: 0.8751 - val_loss: 0.4617 - val_accuracy: 0.8745\n",
      "Epoch 7/20\n",
      "151/151 [==============================] - 38s 252ms/step - loss: 0.4678 - accuracy: 0.8751 - val_loss: 0.4630 - val_accuracy: 0.8745\n",
      "Epoch 8/20\n",
      "151/151 [==============================] - 42s 275ms/step - loss: 0.4669 - accuracy: 0.8751 - val_loss: 0.4651 - val_accuracy: 0.8745\n",
      "Epoch 9/20\n",
      "151/151 [==============================] - 37s 248ms/step - loss: 0.4659 - accuracy: 0.8751 - val_loss: 0.4643 - val_accuracy: 0.8745\n",
      "Epoch 10/20\n",
      "151/151 [==============================] - 37s 242ms/step - loss: 0.4680 - accuracy: 0.8751 - val_loss: 0.4619 - val_accuracy: 0.8745\n",
      "Epoch 11/20\n",
      "151/151 [==============================] - 37s 245ms/step - loss: 0.4635 - accuracy: 0.8751 - val_loss: 0.4614 - val_accuracy: 0.8745\n",
      "Epoch 12/20\n",
      "151/151 [==============================] - 38s 249ms/step - loss: 0.4677 - accuracy: 0.8751 - val_loss: 0.4622 - val_accuracy: 0.8745\n",
      "Epoch 13/20\n",
      "151/151 [==============================] - 37s 242ms/step - loss: 0.4654 - accuracy: 0.8751 - val_loss: 0.4620 - val_accuracy: 0.8745\n",
      "Epoch 14/20\n",
      "151/151 [==============================] - 37s 246ms/step - loss: 0.4651 - accuracy: 0.8751 - val_loss: 0.4619 - val_accuracy: 0.8745\n",
      "Epoch 15/20\n",
      "151/151 [==============================] - 36s 240ms/step - loss: 0.4642 - accuracy: 0.8751 - val_loss: 0.4641 - val_accuracy: 0.8745\n",
      "Epoch 16/20\n",
      "151/151 [==============================] - 37s 243ms/step - loss: 0.4643 - accuracy: 0.8751 - val_loss: 0.4662 - val_accuracy: 0.8745\n",
      "Epoch 17/20\n",
      "151/151 [==============================] - 37s 244ms/step - loss: 0.4623 - accuracy: 0.8751 - val_loss: 0.4648 - val_accuracy: 0.8745\n",
      "Epoch 18/20\n",
      "151/151 [==============================] - 37s 244ms/step - loss: 0.4655 - accuracy: 0.8751 - val_loss: 0.4627 - val_accuracy: 0.8745\n",
      "Epoch 19/20\n",
      "151/151 [==============================] - 36s 238ms/step - loss: 0.4645 - accuracy: 0.8751 - val_loss: 0.4625 - val_accuracy: 0.8745\n",
      "Epoch 20/20\n",
      "151/151 [==============================] - 37s 245ms/step - loss: 0.4635 - accuracy: 0.8751 - val_loss: 0.4634 - val_accuracy: 0.8745\n",
      "\n",
      "Training base model 2/3\n",
      "Epoch 1/20\n",
      "151/151 [==============================] - 42s 247ms/step - loss: 0.5085 - accuracy: 0.8676 - val_loss: 0.4656 - val_accuracy: 0.8745\n",
      "Epoch 2/20\n",
      "151/151 [==============================] - 36s 240ms/step - loss: 0.4738 - accuracy: 0.8751 - val_loss: 0.4621 - val_accuracy: 0.8745\n",
      "Epoch 3/20\n",
      "151/151 [==============================] - 41s 273ms/step - loss: 0.4687 - accuracy: 0.8751 - val_loss: 0.4624 - val_accuracy: 0.8745\n",
      "Epoch 4/20\n",
      "151/151 [==============================] - 38s 252ms/step - loss: 0.4725 - accuracy: 0.8751 - val_loss: 0.4634 - val_accuracy: 0.8745\n",
      "Epoch 5/20\n",
      "151/151 [==============================] - 35s 232ms/step - loss: 0.4683 - accuracy: 0.8751 - val_loss: 0.4663 - val_accuracy: 0.8745\n",
      "Epoch 6/20\n",
      "151/151 [==============================] - 35s 230ms/step - loss: 0.4677 - accuracy: 0.8751 - val_loss: 0.4653 - val_accuracy: 0.8745\n",
      "Epoch 7/20\n",
      "151/151 [==============================] - 35s 230ms/step - loss: 0.4713 - accuracy: 0.8751 - val_loss: 0.4623 - val_accuracy: 0.8745\n",
      "Epoch 8/20\n",
      "151/151 [==============================] - 35s 229ms/step - loss: 0.4647 - accuracy: 0.8751 - val_loss: 0.4624 - val_accuracy: 0.8745\n",
      "Epoch 9/20\n",
      "151/151 [==============================] - 35s 230ms/step - loss: 0.4679 - accuracy: 0.8751 - val_loss: 0.4625 - val_accuracy: 0.8745\n",
      "Epoch 10/20\n",
      "151/151 [==============================] - 34s 228ms/step - loss: 0.4683 - accuracy: 0.8751 - val_loss: 0.4630 - val_accuracy: 0.8745\n",
      "Epoch 11/20\n",
      "151/151 [==============================] - 35s 230ms/step - loss: 0.4662 - accuracy: 0.8751 - val_loss: 0.4649 - val_accuracy: 0.8745\n",
      "Epoch 12/20\n",
      "151/151 [==============================] - 35s 229ms/step - loss: 0.4660 - accuracy: 0.8751 - val_loss: 0.4633 - val_accuracy: 0.8745\n",
      "Epoch 13/20\n",
      "151/151 [==============================] - 35s 229ms/step - loss: 0.4633 - accuracy: 0.8751 - val_loss: 0.4635 - val_accuracy: 0.8745\n",
      "Epoch 14/20\n",
      "151/151 [==============================] - 35s 229ms/step - loss: 0.4624 - accuracy: 0.8751 - val_loss: 0.4635 - val_accuracy: 0.8745\n",
      "Epoch 15/20\n",
      "151/151 [==============================] - 34s 228ms/step - loss: 0.4625 - accuracy: 0.8751 - val_loss: 0.4621 - val_accuracy: 0.8745\n",
      "Epoch 16/20\n",
      "151/151 [==============================] - 37s 242ms/step - loss: 0.4659 - accuracy: 0.8751 - val_loss: 0.4618 - val_accuracy: 0.8745\n",
      "Epoch 17/20\n",
      "151/151 [==============================] - 39s 260ms/step - loss: 0.4647 - accuracy: 0.8751 - val_loss: 0.4615 - val_accuracy: 0.8745\n",
      "Epoch 18/20\n",
      "151/151 [==============================] - 38s 252ms/step - loss: 0.4645 - accuracy: 0.8751 - val_loss: 0.4624 - val_accuracy: 0.8745\n",
      "Epoch 19/20\n",
      "151/151 [==============================] - 37s 245ms/step - loss: 0.4650 - accuracy: 0.8751 - val_loss: 0.4614 - val_accuracy: 0.8745\n",
      "Epoch 20/20\n",
      "151/151 [==============================] - 37s 246ms/step - loss: 0.4624 - accuracy: 0.8751 - val_loss: 0.4615 - val_accuracy: 0.8745\n",
      "\n",
      "Training base model 3/3\n",
      "Epoch 1/20\n",
      "151/151 [==============================] - 43s 255ms/step - loss: 0.5181 - accuracy: 0.8594 - val_loss: 0.4652 - val_accuracy: 0.8745\n",
      "Epoch 2/20\n",
      "151/151 [==============================] - 37s 243ms/step - loss: 0.4740 - accuracy: 0.8751 - val_loss: 0.4622 - val_accuracy: 0.8745\n",
      "Epoch 3/20\n",
      "151/151 [==============================] - 37s 243ms/step - loss: 0.4738 - accuracy: 0.8751 - val_loss: 0.4664 - val_accuracy: 0.8745\n",
      "Epoch 4/20\n",
      "151/151 [==============================] - 37s 242ms/step - loss: 0.4724 - accuracy: 0.8751 - val_loss: 0.4620 - val_accuracy: 0.8745\n",
      "Epoch 5/20\n",
      "151/151 [==============================] - 36s 242ms/step - loss: 0.4683 - accuracy: 0.8751 - val_loss: 0.4622 - val_accuracy: 0.8745\n",
      "Epoch 6/20\n",
      "151/151 [==============================] - 37s 244ms/step - loss: 0.4661 - accuracy: 0.8751 - val_loss: 0.4732 - val_accuracy: 0.8745\n",
      "Epoch 7/20\n",
      "151/151 [==============================] - 38s 250ms/step - loss: 0.4732 - accuracy: 0.8751 - val_loss: 0.4614 - val_accuracy: 0.8745\n",
      "Epoch 8/20\n",
      "151/151 [==============================] - 45s 299ms/step - loss: 0.4675 - accuracy: 0.8751 - val_loss: 0.4620 - val_accuracy: 0.8745\n",
      "Epoch 9/20\n",
      "151/151 [==============================] - 45s 296ms/step - loss: 0.4652 - accuracy: 0.8751 - val_loss: 0.4623 - val_accuracy: 0.8745\n",
      "Epoch 10/20\n",
      "151/151 [==============================] - 44s 295ms/step - loss: 0.4648 - accuracy: 0.8751 - val_loss: 0.4640 - val_accuracy: 0.8745\n",
      "Epoch 11/20\n",
      "151/151 [==============================] - 45s 295ms/step - loss: 0.4639 - accuracy: 0.8751 - val_loss: 0.4613 - val_accuracy: 0.8745\n",
      "Epoch 12/20\n",
      "151/151 [==============================] - 45s 295ms/step - loss: 0.4662 - accuracy: 0.8751 - val_loss: 0.4629 - val_accuracy: 0.8745\n",
      "Epoch 13/20\n",
      "151/151 [==============================] - 44s 295ms/step - loss: 0.4669 - accuracy: 0.8751 - val_loss: 0.4631 - val_accuracy: 0.8745\n",
      "Epoch 14/20\n",
      "151/151 [==============================] - 44s 294ms/step - loss: 0.4645 - accuracy: 0.8751 - val_loss: 0.4631 - val_accuracy: 0.8745\n",
      "Epoch 15/20\n",
      "151/151 [==============================] - 44s 293ms/step - loss: 0.4621 - accuracy: 0.8751 - val_loss: 0.4622 - val_accuracy: 0.8745\n",
      "Epoch 16/20\n",
      "151/151 [==============================] - 44s 291ms/step - loss: 0.4658 - accuracy: 0.8748 - val_loss: 0.4621 - val_accuracy: 0.8745\n",
      "Epoch 17/20\n",
      "151/151 [==============================] - 44s 294ms/step - loss: 0.4672 - accuracy: 0.8751 - val_loss: 0.4618 - val_accuracy: 0.8745\n",
      "Epoch 18/20\n",
      "151/151 [==============================] - 44s 293ms/step - loss: 0.4654 - accuracy: 0.8751 - val_loss: 0.4618 - val_accuracy: 0.8745\n",
      "Epoch 19/20\n",
      "151/151 [==============================] - 44s 294ms/step - loss: 0.4658 - accuracy: 0.8751 - val_loss: 0.4610 - val_accuracy: 0.8745\n",
      "Epoch 20/20\n",
      "151/151 [==============================] - 44s 293ms/step - loss: 0.4650 - accuracy: 0.8751 - val_loss: 0.4649 - val_accuracy: 0.8745\n",
      "Meta training features shape: (1335, 9)\n",
      "Epoch 1/20\n",
      "76/76 [==============================] - 1s 6ms/step - loss: 0.7639 - accuracy: 0.7161 - val_loss: 0.4757 - val_accuracy: 0.8881\n",
      "Epoch 2/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4999 - accuracy: 0.8734 - val_loss: 0.4373 - val_accuracy: 0.8881\n",
      "Epoch 3/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.8734 - val_loss: 0.4377 - val_accuracy: 0.8881\n",
      "Epoch 4/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4767 - accuracy: 0.8734 - val_loss: 0.4387 - val_accuracy: 0.8881\n",
      "Epoch 5/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4680 - accuracy: 0.8734 - val_loss: 0.4384 - val_accuracy: 0.8881\n",
      "Epoch 6/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.8734 - val_loss: 0.4380 - val_accuracy: 0.8881\n",
      "Epoch 7/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4670 - accuracy: 0.8734 - val_loss: 0.4390 - val_accuracy: 0.8881\n",
      "Epoch 8/20\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.4755 - accuracy: 0.8734 - val_loss: 0.4385 - val_accuracy: 0.8881\n",
      "Epoch 9/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4734 - accuracy: 0.8734 - val_loss: 0.4400 - val_accuracy: 0.8881\n",
      "Epoch 10/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.8734 - val_loss: 0.4395 - val_accuracy: 0.8881\n",
      "Epoch 11/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4729 - accuracy: 0.8734 - val_loss: 0.4397 - val_accuracy: 0.8881\n",
      "Epoch 12/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4755 - accuracy: 0.8734 - val_loss: 0.4384 - val_accuracy: 0.8881\n",
      "Epoch 13/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4701 - accuracy: 0.8734 - val_loss: 0.4379 - val_accuracy: 0.8881\n",
      "Epoch 14/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4731 - accuracy: 0.8734 - val_loss: 0.4374 - val_accuracy: 0.8881\n",
      "Epoch 15/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4722 - accuracy: 0.8734 - val_loss: 0.4389 - val_accuracy: 0.8881\n",
      "Epoch 16/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4685 - accuracy: 0.8734 - val_loss: 0.4392 - val_accuracy: 0.8881\n",
      "Epoch 17/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.8734 - val_loss: 0.4397 - val_accuracy: 0.8881\n",
      "Epoch 18/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4714 - accuracy: 0.8734 - val_loss: 0.4391 - val_accuracy: 0.8881\n",
      "Epoch 19/20\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.4704 - accuracy: 0.8734 - val_loss: 0.4404 - val_accuracy: 0.8881\n",
      "Epoch 20/20\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.4723 - accuracy: 0.8734 - val_loss: 0.4395 - val_accuracy: 0.8881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0   73    0]\n",
      " [   0 1460    0]\n",
      " [   0  135    0]]\n",
      "Accuracy for class 0 (low): 0.0000\n",
      "Accuracy for class 1 (normal): 1.0000\n",
      "Accuracy for class 2 (high): 0.0000\n",
      "\n",
      "Average Classification Accuracy: 0.3333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.00      0.00      0.00        73\n",
      "      normal       0.88      1.00      0.93      1460\n",
      "        high       0.00      0.00      0.00       135\n",
      "\n",
      "    accuracy                           0.88      1668\n",
      "   macro avg       0.29      0.33      0.31      1668\n",
      "weighted avg       0.77      0.88      0.82      1668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# -------------------------\n",
    "# Set random seeds for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read Excel Data and Organize It\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare Data for Training\n",
    "# Create sequences and labels. Map labels to integers: low -> 0, normal -> 1, high -> 2.\n",
    "X = []\n",
    "y = []\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences with a pad value of -10 (so the Masking layer can ignore them)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) data using StandardScaler\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split Data into Train and Test Sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=seed_value, stratify=y\n",
    ")\n",
    "print(\"Full training set shape:\", X_train_full.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# Further split training set into base training and stacking sets (for meta model)\n",
    "X_train_base, X_train_stack, y_train_base, y_train_stack = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=seed_value, stratify=y_train_full\n",
    ")\n",
    "print(\"Base training set shape:\", X_train_base.shape)\n",
    "print(\"Stacking set shape:\", X_train_stack.shape)\n",
    "\n",
    "# Inverse label map for reporting\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# -------------------------\n",
    "# 4. Define Base Model Architecture\n",
    "def build_base_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-10., input_shape=input_shape))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train Base Models\n",
    "num_base_models = 3  # You can increase the number for more diversity\n",
    "base_models = []\n",
    "for i in range(num_base_models):\n",
    "    print(f\"\\nTraining base model {i+1}/{num_base_models}\")\n",
    "    # Optionally vary the seed for diversity among base models\n",
    "    tf.random.set_seed(seed_value + i)\n",
    "    model = build_base_model((max_len, 2))\n",
    "    model.fit(X_train_base, y_train_base, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    base_models.append(model)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Generate Meta-Features for the Stacking Set\n",
    "# For each base model, predict probabilities on the stacking set and then concatenate them.\n",
    "meta_features_train = []\n",
    "for model in base_models:\n",
    "    preds = model.predict(X_train_stack)  # shape: (num_stack_samples, 3)\n",
    "    meta_features_train.append(preds)\n",
    "# Concatenate along the feature axis\n",
    "meta_X_train = np.concatenate(meta_features_train, axis=1)  # shape: (num_stack_samples, 3*num_base_models)\n",
    "meta_y_train = y_train_stack\n",
    "\n",
    "print(\"Meta training features shape:\", meta_X_train.shape)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Train Meta Model\n",
    "# Here we use a simple MLP as the meta learner.\n",
    "def build_meta_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "meta_model = build_meta_model(meta_X_train.shape[1])\n",
    "meta_model.fit(meta_X_train, meta_y_train, epochs=20, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 8. Evaluate the Stacking Ensemble on the Test Set\n",
    "# First, generate meta features for the test set by obtaining predictions from each base model.\n",
    "meta_features_test = []\n",
    "for model in base_models:\n",
    "    preds = model.predict(X_test)\n",
    "    meta_features_test.append(preds)\n",
    "meta_X_test = np.concatenate(meta_features_test, axis=1)\n",
    "\n",
    "# Use the meta model to get final predictions.\n",
    "y_pred_meta = meta_model.predict(meta_X_test)\n",
    "y_pred_final = np.argmax(y_pred_meta, axis=1)\n",
    "\n",
    "# Evaluate the final predictions\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Per-class accuracies\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Training set shape: (6671, 304, 2)\n",
      "Test set shape: (1668, 304, 2)\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_14 (Masking)        (None, 304, 2)            0         \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 304, 64)           17152     \n",
      "                                                                 \n",
      " attention_layer (AttentionL  (None, 64)               4224      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,571\n",
      "Trainable params: 21,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 66s 314ms/step - loss: 0.5134 - accuracy: 0.8627 - val_loss: 0.4287 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 56s 299ms/step - loss: 0.4789 - accuracy: 0.8731 - val_loss: 0.4258 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 57s 301ms/step - loss: 0.4733 - accuracy: 0.8731 - val_loss: 0.4265 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 57s 301ms/step - loss: 0.4686 - accuracy: 0.8734 - val_loss: 0.4269 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 56s 298ms/step - loss: 0.4707 - accuracy: 0.8734 - val_loss: 0.4257 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 56s 299ms/step - loss: 0.4710 - accuracy: 0.8736 - val_loss: 0.4269 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 56s 299ms/step - loss: 0.4675 - accuracy: 0.8734 - val_loss: 0.4255 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 57s 301ms/step - loss: 0.4712 - accuracy: 0.8736 - val_loss: 0.4278 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 56s 300ms/step - loss: 0.4721 - accuracy: 0.8736 - val_loss: 0.4254 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 56s 298ms/step - loss: 0.4693 - accuracy: 0.8736 - val_loss: 0.4260 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 56s 300ms/step - loss: 0.4665 - accuracy: 0.8736 - val_loss: 0.4312 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 56s 299ms/step - loss: 0.4707 - accuracy: 0.8736 - val_loss: 0.4338 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 58s 309ms/step - loss: 0.4668 - accuracy: 0.8736 - val_loss: 0.4264 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 56s 300ms/step - loss: 0.4677 - accuracy: 0.8736 - val_loss: 0.4306 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 47s 252ms/step - loss: 0.4677 - accuracy: 0.8736 - val_loss: 0.4257 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.4654 - accuracy: 0.8736 - val_loss: 0.4341 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 57s 301ms/step - loss: 0.4675 - accuracy: 0.8736 - val_loss: 0.4276 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 60s 321ms/step - loss: 0.4661 - accuracy: 0.8736 - val_loss: 0.4289 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 58s 308ms/step - loss: 0.4654 - accuracy: 0.8736 - val_loss: 0.4263 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 48s 256ms/step - loss: 0.4669 - accuracy: 0.8736 - val_loss: 0.4288 - val_accuracy: 0.8877\n",
      "Test Loss: 0.45754295587539673\n",
      "Test Accuracy: 0.8752997517585754\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0   73    0]\n",
      " [   0 1460    0]\n",
      " [   0  135    0]]\n",
      "Accuracy for class 0 (low): 0.0000\n",
      "Accuracy for class 1 (normal): 1.0000\n",
      "Accuracy for class 2 (high): 0.0000\n",
      "\n",
      "Average Classification Accuracy: 0.3333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.00      0.00      0.00        73\n",
      "      normal       0.88      1.00      0.93      1460\n",
      "        high       0.00      0.00      0.00       135\n",
      "\n",
      "    accuracy                           0.88      1668\n",
      "   macro avg       0.29      0.33      0.31      1668\n",
      "weighted avg       0.77      0.88      0.82      1668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------\n",
    "# 1. Define a Custom Attention Layer\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch_size, time_steps, hidden_size)\n",
    "        self.W = self.add_weight(name=\"att_weight\",\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\",\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=\"zeros\",\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name=\"att_u\",\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Compute u_t = tanh(W.h_t + b) for each timestep\n",
    "        uit = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)  # (batch, time_steps, hidden_size)\n",
    "        # Compute scores for each timestep\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)  # (batch, time_steps, 1)\n",
    "        ait = tf.squeeze(ait, -1)  # (batch, time_steps)\n",
    "        a = tf.nn.softmax(ait, axis=1)  # (batch, time_steps)\n",
    "        a = tf.expand_dims(a, -1)       # (batch, time_steps, 1)\n",
    "        # Compute the weighted sum of the inputs\n",
    "        output = tf.reduce_sum(inputs * a, axis=1)  # (batch, hidden_size)\n",
    "        return output\n",
    "\n",
    "# -------------------------\n",
    "# 2. Read Excel Data and Organize It\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), t5 value, and assign a label:\n",
    "    'low' if t5 < t5_lb, 'high' if t5 > t5_ub, else 'normal'.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "        \n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "        \n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "    \n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Prepare Data for Training\n",
    "# Build sequences (each with features S1 and S2) and corresponding labels.\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences with a pad value of -10 (so that the Masking layer ignores them)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) points using StandardScaler\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Split Data into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting purposes, create an inverse label map.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# -------------------------\n",
    "# 5. Build the Model with Attention Mechanism\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "# Use return_sequences=True so that attention can operate over the timesteps.\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "# Apply the custom attention layer\n",
    "model.add(AttentionLayer())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# 6. Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Evaluate the Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Training set shape: (6671, 304, 2)\n",
      "Test set shape: (1668, 304, 2)\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_15 (Masking)        (None, 304, 2)            0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 304, 64)           448       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 152, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 152, 32)           6176      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 76, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 64)                24832     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,651\n",
      "Trainable params: 31,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 16s 69ms/step - loss: 0.5003 - accuracy: 0.8722 - val_loss: 0.4263 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.4743 - accuracy: 0.8736 - val_loss: 0.4271 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.4734 - accuracy: 0.8736 - val_loss: 0.4285 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.4720 - accuracy: 0.8736 - val_loss: 0.4276 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.4699 - accuracy: 0.8736 - val_loss: 0.4273 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.4723 - accuracy: 0.8736 - val_loss: 0.4272 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.4692 - accuracy: 0.8736 - val_loss: 0.4277 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.4712 - accuracy: 0.8736 - val_loss: 0.4299 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.4683 - accuracy: 0.8736 - val_loss: 0.4278 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.4669 - accuracy: 0.8736 - val_loss: 0.4282 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 12s 62ms/step - loss: 0.4635 - accuracy: 0.8736 - val_loss: 0.4334 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.4694 - accuracy: 0.8736 - val_loss: 0.4363 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.4676 - accuracy: 0.8736 - val_loss: 0.4281 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.4677 - accuracy: 0.8736 - val_loss: 0.4326 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.4661 - accuracy: 0.8736 - val_loss: 0.4274 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 12s 62ms/step - loss: 0.4653 - accuracy: 0.8736 - val_loss: 0.4365 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 12s 62ms/step - loss: 0.4667 - accuracy: 0.8736 - val_loss: 0.4304 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.4638 - accuracy: 0.8736 - val_loss: 0.4307 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.4642 - accuracy: 0.8736 - val_loss: 0.4292 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 12s 64ms/step - loss: 0.4659 - accuracy: 0.8736 - val_loss: 0.4316 - val_accuracy: 0.8877\n",
      "Test Loss: 0.45903074741363525\n",
      "Test Accuracy: 0.8752997517585754\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0   73    0]\n",
      " [   0 1460    0]\n",
      " [   0  135    0]]\n",
      "Accuracy for class 0 (low): 0.0000\n",
      "Accuracy for class 1 (normal): 1.0000\n",
      "Accuracy for class 2 (high): 0.0000\n",
      "\n",
      "Average Classification Accuracy: 0.3333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.00      0.00      0.00        73\n",
      "      normal       0.88      1.00      0.93      1460\n",
      "        high       0.00      0.00      0.00       135\n",
      "\n",
      "    accuracy                           0.88      1668\n",
      "   macro avg       0.29      0.33      0.31      1668\n",
      "weighted avg       0.77      0.88      0.82      1668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read Excel Data and Organize It\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from the bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), the t5 value, and assign a label:\n",
    "    'low' if t5 < t5_lb, 'high' if t5 > t5_ub, else 'normal'.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "        \n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack the tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "        \n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "    \n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare Data for Training\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences to have the same length using a pad value of -10 (for the Masking layer)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) points using a global StandardScaler\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split Data into Train and Test Sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting purposes, create an inverse label map.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# -------------------------\n",
    "# 4. Build the CNN-LSTM Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "\n",
    "# Convolutional block\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Optionally, add another Conv1D block\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# LSTM block to capture temporal dependencies\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Final classification layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train the Model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Evaluate the Model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute per-class accuracy\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Training set shape: (6671, 304, 2)\n",
      "Test set shape: (1668, 304, 2)\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_16 (Masking)        (None, 304, 2)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              34304     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,691\n",
      "Trainable params: 34,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 63s 279ms/step - loss: 0.4925 - accuracy: 0.8691 - val_loss: 0.4297 - val_accuracy: 0.8877\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4705 - accuracy: 0.8736 - val_loss: 0.4293 - val_accuracy: 0.8877\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 49s 262ms/step - loss: 0.4716 - accuracy: 0.8736 - val_loss: 0.4304 - val_accuracy: 0.8877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 49s 259ms/step - loss: 0.4664 - accuracy: 0.8736 - val_loss: 0.4280 - val_accuracy: 0.8877\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 50s 263ms/step - loss: 0.4708 - accuracy: 0.8736 - val_loss: 0.4269 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 49s 261ms/step - loss: 0.4708 - accuracy: 0.8736 - val_loss: 0.4292 - val_accuracy: 0.8877\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4680 - accuracy: 0.8736 - val_loss: 0.4295 - val_accuracy: 0.8877\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4687 - accuracy: 0.8736 - val_loss: 0.4305 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4671 - accuracy: 0.8736 - val_loss: 0.4278 - val_accuracy: 0.8877\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 49s 261ms/step - loss: 0.4690 - accuracy: 0.8736 - val_loss: 0.4277 - val_accuracy: 0.8877\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 49s 261ms/step - loss: 0.4673 - accuracy: 0.8736 - val_loss: 0.4339 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 49s 262ms/step - loss: 0.4669 - accuracy: 0.8736 - val_loss: 0.4346 - val_accuracy: 0.8877\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4689 - accuracy: 0.8732 - val_loss: 0.4281 - val_accuracy: 0.8877\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 49s 259ms/step - loss: 0.4646 - accuracy: 0.8736 - val_loss: 0.4348 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 49s 263ms/step - loss: 0.4682 - accuracy: 0.8736 - val_loss: 0.4277 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 49s 261ms/step - loss: 0.4657 - accuracy: 0.8736 - val_loss: 0.4367 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4636 - accuracy: 0.8736 - val_loss: 0.4314 - val_accuracy: 0.8877\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 49s 260ms/step - loss: 0.4656 - accuracy: 0.8736 - val_loss: 0.4303 - val_accuracy: 0.8877\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 49s 262ms/step - loss: 0.4655 - accuracy: 0.8736 - val_loss: 0.4289 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 49s 262ms/step - loss: 0.4647 - accuracy: 0.8736 - val_loss: 0.4318 - val_accuracy: 0.8877\n",
      "Test Loss: 0.4568394124507904\n",
      "Test Accuracy: 0.8752997517585754\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0   73    0]\n",
      " [   0 1460    0]\n",
      " [   0  135    0]]\n",
      "Accuracy for class 0 (low): 0.0000\n",
      "Accuracy for class 1 (normal): 1.0000\n",
      "Accuracy for class 2 (high): 0.0000\n",
      "\n",
      "Average Classification Accuracy: 0.3333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.00      0.00      0.00        73\n",
      "      normal       0.88      1.00      0.93      1460\n",
      "        high       0.00      0.00      0.00       135\n",
      "\n",
      "    accuracy                           0.88      1668\n",
      "   macro avg       0.29      0.33      0.31      1668\n",
      "weighted avg       0.77      0.88      0.82      1668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mohammad/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read Excel Data and Organize It\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract:\n",
    "    - The multivariate time-series (MDR) with columns S1 and S2.\n",
    "    - The t5 value.\n",
    "    - A class label ('low', 'normal', or 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "        \n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "        \n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "    \n",
    "    # Remove batches with empty or invalid MDR data\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare Data for Training\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    # Use only the S1 and S2 columns as features\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences (using -10 as the pad value so that Masking can ignore these)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale only valid (non-padded) points using a global StandardScaler\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split Data into Train and Test Sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting, create an inverse label map.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# -------------------------\n",
    "# 4. Build the Bidirectional LSTM Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "# Use a Bidirectional LSTM to capture both forward and backward dependencies\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train the Model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Evaluate the Model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
