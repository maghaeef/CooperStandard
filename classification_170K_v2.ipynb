{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:26:32.522880: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-02-13 18:26:32.522990: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "\n",
      "Class distribution:\n",
      "Low: 365\n",
      "Normal: 7297\n",
      "High: 677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:29:14.848185: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-02-13 18:29:14.848221: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-02-13 18:29:14.848269: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2025-02-13 18:29:14.848604: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "178/178 [==============================] - 135s 686ms/step - loss: 1.1080 - accuracy: 0.3198 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0924 - val_accuracy: 0.8711 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/100\n",
      "178/178 [==============================] - 123s 691ms/step - loss: 1.1028 - accuracy: 0.3873 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0973 - val_accuracy: 0.0440 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/100\n",
      "178/178 [==============================] - 147s 826ms/step - loss: 1.1023 - accuracy: 0.0820 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1033 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/100\n",
      "178/178 [==============================] - 121s 678ms/step - loss: 1.1018 - accuracy: 0.1194 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1031 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/100\n",
      "178/178 [==============================] - 137s 769ms/step - loss: 1.1016 - accuracy: 0.0679 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1023 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/100\n",
      "178/178 [==============================] - 133s 746ms/step - loss: 1.1017 - accuracy: 0.0649 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1006 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/100\n",
      "178/178 [==============================] - 132s 742ms/step - loss: 1.1017 - accuracy: 0.0820 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1026 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/100\n",
      "178/178 [==============================] - 130s 729ms/step - loss: 1.1016 - accuracy: 0.0820 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1027 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/100\n",
      "178/178 [==============================] - 191s 1s/step - loss: 1.1017 - accuracy: 0.1321 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1039 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/100\n",
      "178/178 [==============================] - 240s 1s/step - loss: 1.1017 - accuracy: 0.0653 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1041 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/100\n",
      "178/178 [==============================] - 119s 664ms/step - loss: 1.1017 - accuracy: 0.0589 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1050 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/100\n",
      "178/178 [==============================] - 101s 568ms/step - loss: 1.1017 - accuracy: 0.0908 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1017 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/100\n",
      "178/178 [==============================] - 101s 568ms/step - loss: 1.1032 - accuracy: 0.2286 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1981 - val_accuracy: 0.0440 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/100\n",
      "178/178 [==============================] - 101s 566ms/step - loss: 1.1032 - accuracy: 0.1157 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1236 - val_accuracy: 0.0440 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/100\n",
      "178/178 [==============================] - 101s 570ms/step - loss: 1.1030 - accuracy: 0.1365 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0890 - val_accuracy: 0.8392 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 16/100\n",
      "178/178 [==============================] - 102s 573ms/step - loss: 1.1032 - accuracy: 0.2028 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0989 - val_accuracy: 0.0470 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 17/100\n",
      "178/178 [==============================] - 103s 578ms/step - loss: 1.1016 - accuracy: 0.2808 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0851 - val_accuracy: 0.8791 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 18/100\n",
      "178/178 [==============================] - 102s 572ms/step - loss: 1.1032 - accuracy: 0.2039 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1002 - val_accuracy: 0.0749 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 19/100\n",
      "178/178 [==============================] - 102s 573ms/step - loss: 1.1017 - accuracy: 0.1063 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1020 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 20/100\n",
      "178/178 [==============================] - 102s 573ms/step - loss: 1.1024 - accuracy: 0.1095 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0973 - val_accuracy: 0.5325 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 21/100\n",
      "178/178 [==============================] - 102s 573ms/step - loss: 1.1018 - accuracy: 0.4372 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1012 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 22/100\n",
      "178/178 [==============================] - 102s 572ms/step - loss: 1.1019 - accuracy: 0.1180 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1065 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 23/100\n",
      "178/178 [==============================] - 102s 572ms/step - loss: 1.1019 - accuracy: 0.0683 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1089 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 24/100\n",
      "178/178 [==============================] - 102s 574ms/step - loss: 1.1020 - accuracy: 0.0972 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1095 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 25/100\n",
      "178/178 [==============================] - 125s 702ms/step - loss: 1.1017 - accuracy: 0.0679 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1046 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 26/100\n",
      "178/178 [==============================] - 113s 638ms/step - loss: 1.1018 - accuracy: 0.0734 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1008 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 27/100\n",
      "178/178 [==============================] - 114s 643ms/step - loss: 1.1017 - accuracy: 0.0831 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1023 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 28/100\n",
      "178/178 [==============================] - 112s 630ms/step - loss: 1.1021 - accuracy: 0.0656 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1016 - val_accuracy: 0.0769 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 29/100\n",
      " 54/178 [========>.....................] - ETA: 1:16 - loss: 1.0998 - accuracy: 0.0833 - precision: 0.0000e+00 - recall: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### 1. Import Required Libraries\n",
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Data Loading and Preparation\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate string representations of lists.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        return ast.literal_eval(str(value).replace(\"nan\", \"None\"))\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"Process raw data into structured format.\"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        \n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "        \n",
    "        # Process MDR data\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 and t_S2:\n",
    "            t, S1 = zip(*t_S1)\n",
    "            t, S2 = zip(*t_S2)\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t[1:],  # Exclude first element\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:]\n",
    "            }).interpolate(method='linear').ffill().bfill()\n",
    "            data[batch_number][\"MDR\"] = MDR\n",
    "        \n",
    "        # Assign class label\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "    \n",
    "    return {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"Low: {len([v for v in data.values() if v['class'] == 'low'])}\")\n",
    "print(f\"Normal: {len([v for v in data.values() if v['class'] == 'normal'])}\")\n",
    "print(f\"High: {len([v for v in data.values() if v['class'] == 'high'])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Data Preprocessing\n",
    "# Split data\n",
    "keys = list(data.keys())\n",
    "labels = [data[k]['class'] for k in keys]\n",
    "X_train_keys, X_test_keys = train_test_split(\n",
    "    keys, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate normalization parameters\n",
    "train_mdr = pd.concat([data[k]['MDR'] for k in X_train_keys])\n",
    "global_min = {'S1': train_mdr['S1'].min(), 'S2': train_mdr['S2'].min()}\n",
    "global_max = {'S1': train_mdr['S1'].max(), 'S2': train_mdr['S2'].max()}\n",
    "\n",
    "def normalize_and_pad(keys):\n",
    "    \"\"\"Normalize and pad sequences to equal length.\"\"\"\n",
    "    sequences = []\n",
    "    for k in keys:\n",
    "        mdr = data[k]['MDR'].copy()\n",
    "        # Normalize\n",
    "        mdr['S1'] = (mdr['S1'] - global_min['S1']) / (global_max['S1'] - global_min['S1'])\n",
    "        mdr['S2'] = (mdr['S2'] - global_min['S2']) / (global_max['S2'] - global_min['S2'])\n",
    "        sequences.append(mdr[['S1', 'S2']].values)\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    return pad_sequences(sequences, maxlen=max_length, padding='post', dtype='float32')\n",
    "\n",
    "X_train = normalize_and_pad(X_train_keys)\n",
    "X_test = normalize_and_pad(X_test_keys)\n",
    "\n",
    "# Prepare labels\n",
    "class_mapping = {'low': 0, 'normal': 1, 'high': 2}\n",
    "y_train = np.array([class_mapping[data[k]['class']] for k in X_train_keys])\n",
    "y_test = np.array([class_mapping[data[k]['class']] for k in X_test_keys])\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Handle Class Imbalance\n",
    "class_counts = Counter(y_train)\n",
    "total = sum(class_counts.values())\n",
    "class_weights = {\n",
    "    0: total / (3 * class_counts[0]),  # low\n",
    "    1: total / (3 * class_counts[1]),  # normal\n",
    "    2: total / (3 * class_counts[2])   # high\n",
    "}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Build LSTM Model\n",
    "model = Sequential([\n",
    "    Masking(mask_value=0., input_shape=(X_train.shape[1], 2)),\n",
    "    LSTM(128, return_sequences=True, dropout=0.3),\n",
    "    LSTM(64, dropout=0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy',\n",
    "                      tf.keras.metrics.Precision(name='precision'),\n",
    "                      tf.keras.metrics.Recall(name='recall')])\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6. Train the Model\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=15, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.15,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 7. Evaluation Metrics\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat = tf.math.confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Calculate metrics\n",
    "class_acc = {}\n",
    "for i, class_name in enumerate(['low', 'normal', 'high']):\n",
    "    correct = conf_mat[i,i].numpy()\n",
    "    total = conf_mat[i].numpy().sum()\n",
    "    class_acc[class_name] = correct / total\n",
    "\n",
    "avg_acc = np.trace(conf_mat) / np.sum(conf_mat)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nClassification Report:\")\n",
    "print(f\"{'Class':<10} {'Accuracy':<10}\")\n",
    "for cls, acc in class_acc.items():\n",
    "    print(f\"{cls:<10} {acc:.2%}\")\n",
    "print(f\"\\nAverage Accuracy: {avg_acc:.2%}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_mat, cmap='Blues')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(j, i, f\"{conf_mat[i,j]}\", \n",
    "                ha=\"center\", va=\"center\", \n",
    "                color=\"white\" if conf_mat[i,j] > conf_mat.max()/2 else \"black\")\n",
    "plt.xticks([0, 1, 2], ['low', 'normal', 'high'])\n",
    "plt.yticks([0, 1, 2], ['low', 'normal', 'high'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
