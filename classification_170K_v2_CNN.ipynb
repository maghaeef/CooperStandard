{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "\n",
      "Training binary model for class 0 (low)\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 58s 281ms/step - loss: 0.7033 - accuracy: 0.5769 - val_loss: 0.8032 - val_accuracy: 0.1078\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 48s 255ms/step - loss: 0.6998 - accuracy: 0.4230 - val_loss: 0.6835 - val_accuracy: 0.9296\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 49s 259ms/step - loss: 0.7005 - accuracy: 0.6207 - val_loss: 0.7386 - val_accuracy: 0.1078\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 46s 245ms/step - loss: 0.6963 - accuracy: 0.3920 - val_loss: 0.6732 - val_accuracy: 0.9207\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6975 - accuracy: 0.5154 - val_loss: 0.6980 - val_accuracy: 0.3129\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 48s 257ms/step - loss: 0.6978 - accuracy: 0.5267 - val_loss: 0.7295 - val_accuracy: 0.1183\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 50s 264ms/step - loss: 0.7000 - accuracy: 0.5352 - val_loss: 0.6857 - val_accuracy: 0.8608\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 47s 252ms/step - loss: 0.6976 - accuracy: 0.5001 - val_loss: 0.7182 - val_accuracy: 0.1078\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6974 - accuracy: 0.4083 - val_loss: 0.6820 - val_accuracy: 0.8413\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6965 - accuracy: 0.5034 - val_loss: 0.6986 - val_accuracy: 0.2680\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6967 - accuracy: 0.4419 - val_loss: 0.7005 - val_accuracy: 0.2904\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6958 - accuracy: 0.4931 - val_loss: 0.7030 - val_accuracy: 0.1766\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6969 - accuracy: 0.5341 - val_loss: 0.7149 - val_accuracy: 0.1183\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6964 - accuracy: 0.3686 - val_loss: 0.6728 - val_accuracy: 0.9296\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6960 - accuracy: 0.5687 - val_loss: 0.6970 - val_accuracy: 0.1946\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6956 - accuracy: 0.4849 - val_loss: 0.7020 - val_accuracy: 0.1452\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6996 - accuracy: 0.4504 - val_loss: 0.6927 - val_accuracy: 0.6183\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6969 - accuracy: 0.3132 - val_loss: 0.6944 - val_accuracy: 0.4237\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6974 - accuracy: 0.5479 - val_loss: 0.6948 - val_accuracy: 0.4970\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6947 - accuracy: 0.5022 - val_loss: 0.6941 - val_accuracy: 0.4895\n",
      "\n",
      "Training binary model for class 1 (normal)\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 55s 260ms/step - loss: 0.7014 - accuracy: 0.5042 - val_loss: 0.7343 - val_accuracy: 0.1123\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6967 - accuracy: 0.4596 - val_loss: 0.6885 - val_accuracy: 0.7934\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.7010 - accuracy: 0.4719 - val_loss: 0.6960 - val_accuracy: 0.7710\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 47s 248ms/step - loss: 0.6982 - accuracy: 0.5142 - val_loss: 0.7354 - val_accuracy: 0.1123\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 47s 251ms/step - loss: 0.6962 - accuracy: 0.4468 - val_loss: 0.7064 - val_accuracy: 0.2470\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6968 - accuracy: 0.5017 - val_loss: 0.6948 - val_accuracy: 0.6347\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6973 - accuracy: 0.4066 - val_loss: 0.6709 - val_accuracy: 0.8204\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6988 - accuracy: 0.5727 - val_loss: 0.7015 - val_accuracy: 0.5524\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6976 - accuracy: 0.4834 - val_loss: 0.7020 - val_accuracy: 0.3593\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6980 - accuracy: 0.5322 - val_loss: 0.7092 - val_accuracy: 0.1617\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6973 - accuracy: 0.4038 - val_loss: 0.7006 - val_accuracy: 0.4251\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6967 - accuracy: 0.5439 - val_loss: 0.7187 - val_accuracy: 0.1168\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 47s 249ms/step - loss: 0.6962 - accuracy: 0.4709 - val_loss: 0.7162 - val_accuracy: 0.1302\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 0.6974 - accuracy: 0.4414 - val_loss: 0.7085 - val_accuracy: 0.2290\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 51s 274ms/step - loss: 0.6962 - accuracy: 0.5176 - val_loss: 0.6918 - val_accuracy: 0.6781\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 53s 279ms/step - loss: 0.6973 - accuracy: 0.4888 - val_loss: 0.7053 - val_accuracy: 0.2530\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 48s 257ms/step - loss: 0.6971 - accuracy: 0.4794 - val_loss: 0.6992 - val_accuracy: 0.4296\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 45s 241ms/step - loss: 0.6965 - accuracy: 0.5001 - val_loss: 0.7051 - val_accuracy: 0.2710\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 45s 240ms/step - loss: 0.6962 - accuracy: 0.4926 - val_loss: 0.7120 - val_accuracy: 0.1617\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 45s 240ms/step - loss: 0.6973 - accuracy: 0.4278 - val_loss: 0.6824 - val_accuracy: 0.8129\n",
      "\n",
      "Training binary model for class 2 (high)\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 53s 254ms/step - loss: 0.7045 - accuracy: 0.4538 - val_loss: 0.7144 - val_accuracy: 0.0823\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.7018 - accuracy: 0.4354 - val_loss: 0.6995 - val_accuracy: 0.8099\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.7004 - accuracy: 0.4359 - val_loss: 0.6937 - val_accuracy: 0.7799\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.7000 - accuracy: 0.5199 - val_loss: 0.7404 - val_accuracy: 0.0719\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6971 - accuracy: 0.5129 - val_loss: 0.7260 - val_accuracy: 0.0719\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6951 - accuracy: 0.3593 - val_loss: 0.6668 - val_accuracy: 0.8518\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6967 - accuracy: 0.6244 - val_loss: 0.7539 - val_accuracy: 0.0719\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6985 - accuracy: 0.3943 - val_loss: 0.6857 - val_accuracy: 0.8548\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6957 - accuracy: 0.5839 - val_loss: 0.7166 - val_accuracy: 0.1512\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6973 - accuracy: 0.4934 - val_loss: 0.7153 - val_accuracy: 0.2231\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6953 - accuracy: 0.5227 - val_loss: 0.7076 - val_accuracy: 0.3817\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6969 - accuracy: 0.5132 - val_loss: 0.6784 - val_accuracy: 0.8518\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6996 - accuracy: 0.6797 - val_loss: 0.7062 - val_accuracy: 0.4162\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6974 - accuracy: 0.4188 - val_loss: 0.6795 - val_accuracy: 0.8488\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6978 - accuracy: 0.6165 - val_loss: 0.7115 - val_accuracy: 0.3368\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6981 - accuracy: 0.5680 - val_loss: 0.7195 - val_accuracy: 0.2186\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6977 - accuracy: 0.5002 - val_loss: 0.7123 - val_accuracy: 0.3368\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 46s 244ms/step - loss: 0.6969 - accuracy: 0.5227 - val_loss: 0.7100 - val_accuracy: 0.4162\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6975 - accuracy: 0.4613 - val_loss: 0.6930 - val_accuracy: 0.6901\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 46s 243ms/step - loss: 0.6975 - accuracy: 0.5982 - val_loss: 0.7083 - val_accuracy: 0.3234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  8  45  20]\n",
      " [ 54 966 440]\n",
      " [  3  82  50]]\n",
      "Accuracy for class 0 (low): 0.1096\n",
      "Accuracy for class 1 (normal): 0.6616\n",
      "Accuracy for class 2 (high): 0.3704\n",
      "\n",
      "Average Classification Accuracy: 0.3805\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.12      0.11      0.12        73\n",
      "      normal       0.88      0.66      0.76      1460\n",
      "        high       0.10      0.37      0.16       135\n",
      "\n",
      "    accuracy                           0.61      1668\n",
      "   macro avg       0.37      0.38      0.34      1668\n",
      "weighted avg       0.79      0.61      0.68      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Read Excel Data and Organize It\n",
    "\n",
    "# %%\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack the tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Prepare Data for Training\n",
    "# We convert the variable-length sequences to a padded format.\n",
    "# We also scale the valid (non-padded) parts of the sequences.\n",
    "\n",
    "# %%\n",
    "X = []\n",
    "y = []\n",
    "# Map the labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    # Use only the S1 and S2 columns as features\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences (using -10 as pad value, so that Masking will ignore these)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale the data using a global scaler, ignoring the padded values.\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Split Data into Train and Test Sets\n",
    "# We'll split the dataset while preserving the class distribution.\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# For later reporting, create an inverse label map\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Build and Train Individual Binary (One-vs-All) Models\n",
    "# We build a binary classifier for each class.\n",
    "# Each model outputs the probability that the input belongs to that class.\n",
    "# We use binary cross-entropy loss and a sigmoid output.\n",
    "# You can adjust the architecture, epochs, and other parameters as needed.\n",
    "\n",
    "# %%\n",
    "def build_binary_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-10., input_shape=input_shape))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "binary_models = {}\n",
    "\n",
    "# Train one binary model per class (one-vs-all)\n",
    "for cls in range(3):\n",
    "    print(f\"\\nTraining binary model for class {cls} ({inv_label_map[cls]})\")\n",
    "    # Create binary labels: 1 if the sample belongs to this class, else 0.\n",
    "    y_train_bin = (y_train == cls).astype(int)\n",
    "    \n",
    "    # Compute class weights for the binary problem\n",
    "    classes_bin = np.unique(y_train_bin)\n",
    "    class_weights_bin = compute_class_weight(class_weight='balanced', classes=classes_bin, y=y_train_bin)\n",
    "    class_weight_dict_bin = {cls_val: weight for cls_val, weight in zip(classes_bin, class_weights_bin)}\n",
    "    \n",
    "    model_bin = build_binary_model((max_len, 2))\n",
    "    model_bin.fit(X_train, y_train_bin, \n",
    "                  epochs=20, \n",
    "                  batch_size=32, \n",
    "                  validation_split=0.1, \n",
    "                  class_weight=class_weight_dict_bin,\n",
    "                  verbose=1)\n",
    "    binary_models[cls] = model_bin\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Combine the Models for Final Decision\n",
    "# For each test sample, we obtain the probability from each binary model.\n",
    "# Then we select the class with the highest probability.\n",
    "# (You could also apply thresholds if desired.)\n",
    "\n",
    "# %%\n",
    "# For each model, predict probability on test data.\n",
    "y_pred_probs = np.zeros((len(X_test), 3))\n",
    "for cls in range(3):\n",
    "    y_pred_probs[:, cls] = binary_models[cls].predict(X_test).flatten()\n",
    "\n",
    "# Final prediction: choose the class with the highest probability.\n",
    "y_pred_combined = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6. Evaluate the Combined Model\n",
    "# We compute the confusion matrix and report per-class accuracy.\n",
    "\n",
    "# %%\n",
    "cm = confusion_matrix(y_test, y_pred_combined)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute accuracy for each individual class\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_combined, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Training set shape: (6671, 304, 2)\n",
      "Test set shape: (1668, 304, 2)\n",
      "Training class distribution before augmentation: {0: 292, 1: 5837, 2: 542}\n",
      "Training class distribution after augmentation: {0: 5837, 1: 5837, 2: 5837}\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_4 (Masking)         (None, 304, 2)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                17152     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,347\n",
      "Trainable params: 17,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "493/493 [==============================] - 136s 264ms/step - loss: 1.1035 - accuracy: 0.3380 - val_loss: 1.0959 - val_accuracy: 0.3676\n",
      "Epoch 2/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0999 - accuracy: 0.3416 - val_loss: 1.0955 - val_accuracy: 0.3744\n",
      "Epoch 3/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0986 - accuracy: 0.3392 - val_loss: 1.0947 - val_accuracy: 0.3476\n",
      "Epoch 4/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0979 - accuracy: 0.3430 - val_loss: 1.0948 - val_accuracy: 0.3773\n",
      "Epoch 5/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0968 - accuracy: 0.3508 - val_loss: 1.0958 - val_accuracy: 0.3465\n",
      "Epoch 6/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0969 - accuracy: 0.3510 - val_loss: 1.0944 - val_accuracy: 0.3607\n",
      "Epoch 7/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0957 - accuracy: 0.3574 - val_loss: 1.0929 - val_accuracy: 0.3596\n",
      "Epoch 8/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0976 - accuracy: 0.3668 - val_loss: 1.0929 - val_accuracy: 0.3716\n",
      "Epoch 9/20\n",
      "493/493 [==============================] - 125s 253ms/step - loss: 1.0953 - accuracy: 0.3637 - val_loss: 1.0929 - val_accuracy: 0.3659\n",
      "Epoch 10/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0939 - accuracy: 0.3684 - val_loss: 1.0915 - val_accuracy: 0.3796\n",
      "Epoch 11/20\n",
      "493/493 [==============================] - 124s 252ms/step - loss: 1.0956 - accuracy: 0.3591 - val_loss: 1.0914 - val_accuracy: 0.3590\n",
      "Epoch 12/20\n",
      "493/493 [==============================] - 124s 251ms/step - loss: 1.0934 - accuracy: 0.3693 - val_loss: 1.0931 - val_accuracy: 0.3796\n",
      "Epoch 13/20\n",
      "493/493 [==============================] - 134s 271ms/step - loss: 1.0906 - accuracy: 0.3764 - val_loss: 1.0816 - val_accuracy: 0.3881\n",
      "Epoch 14/20\n",
      "493/493 [==============================] - 125s 254ms/step - loss: 1.0882 - accuracy: 0.3753 - val_loss: 1.0828 - val_accuracy: 0.3950\n",
      "Epoch 15/20\n",
      "308/493 [=================>............] - ETA: 45s - loss: 1.0859 - accuracy: 0.3771"
     ]
    }
   ],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Read Excel Data and Organize It\n",
    "\n",
    "# %%\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack the tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Prepare Data for Training\n",
    "# Convert the variable-length time-series sequences into padded arrays.\n",
    "# We use -10 as the pad value so that the Masking layer can ignore it.\n",
    "# After padding, we scale the valid (non-padded) values with a global StandardScaler.\n",
    "\n",
    "# %%\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    # Use only the S1 and S2 columns as features\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences (padding/truncating at the end)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale the data: first, gather all valid (non-padded) rows\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "# Apply the scaler to each sequence (only the valid timesteps)\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Split Data into Train and Test Sets\n",
    "# We use stratification so that the class imbalance is preserved in the split.\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting purposes, create an inverse label map:\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Data Augmentation for Minority Classes\n",
    "# We define a simple jittering function to add Gaussian noise to the valid parts\n",
    "# of a time-series. Then, we oversample the minority classes (here, \"low\" and \"high\")\n",
    "# to match the number of samples in the majority class (normally \"normal\").\n",
    "\n",
    "# %%\n",
    "def augment_time_series(sequence, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to valid (non-padded) timesteps in the sequence.\n",
    "    The noise_level parameter controls the standard deviation of the noise.\n",
    "    \"\"\"\n",
    "    augmented = sequence.copy()\n",
    "    # Identify valid rows (those not equal to the pad value)\n",
    "    valid_mask = ~np.all(sequence == -10., axis=1)\n",
    "    if np.any(valid_mask):\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_level, size=augmented[valid_mask].shape)\n",
    "        augmented[valid_mask] += noise\n",
    "    return augmented\n",
    "\n",
    "def augment_minority_class(X, y, target_class, target_count, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    For the given target_class, generate augmented samples using jittering\n",
    "    until the total count for that class reaches target_count.\n",
    "    \"\"\"\n",
    "    indices = np.where(y == target_class)[0]\n",
    "    current_count = len(indices)\n",
    "    num_to_augment = target_count - current_count\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for _ in range(num_to_augment):\n",
    "        # Randomly choose one sample from the existing target_class samples\n",
    "        idx = np.random.choice(indices)\n",
    "        sample = X[idx]\n",
    "        aug_sample = augment_time_series(sample, noise_level)\n",
    "        augmented_X.append(aug_sample)\n",
    "        augmented_y.append(target_class)\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# Check current training class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "train_counts = dict(zip(unique, counts))\n",
    "print(\"Training class distribution before augmentation:\", train_counts)\n",
    "\n",
    "# Define the target count as the count of the majority class\n",
    "target_count = max(train_counts.values())\n",
    "\n",
    "# Augment the minority classes (for example, classes 0 and 2)\n",
    "augmented_X = []\n",
    "augmented_y = []\n",
    "\n",
    "for cls in [0, 2]:\n",
    "    if train_counts[cls] < target_count:\n",
    "        X_aug, y_aug = augment_minority_class(X_train, y_train, cls, target_count, noise_level=0.1)\n",
    "        augmented_X.append(X_aug)\n",
    "        augmented_y.append(y_aug)\n",
    "\n",
    "if augmented_X:\n",
    "    X_augmented = np.concatenate(augmented_X, axis=0)\n",
    "    y_augmented = np.concatenate(augmented_y, axis=0)\n",
    "    # Combine with the original training set\n",
    "    X_train_aug = np.concatenate([X_train, X_augmented], axis=0)\n",
    "    y_train_aug = np.concatenate([y_train, y_augmented], axis=0)\n",
    "else:\n",
    "    X_train_aug = X_train\n",
    "    y_train_aug = y_train\n",
    "\n",
    "# Shuffle the augmented training set\n",
    "shuffle_idx = np.random.permutation(len(X_train_aug))\n",
    "X_train_aug = X_train_aug[shuffle_idx]\n",
    "y_train_aug = y_train_aug[shuffle_idx]\n",
    "\n",
    "# Print new training set distribution\n",
    "unique, counts = np.unique(y_train_aug, return_counts=True)\n",
    "print(\"Training class distribution after augmentation:\", dict(zip(unique, counts)))\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Build and Train the Model\n",
    "# We use an LSTM-based network with a Masking layer (ignoring the padded value)\n",
    "# and a Dense output layer with softmax activation. We train on the augmented dataset.\n",
    "\n",
    "# %%\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6. Evaluate the Model\n",
    "# We calculate the confusion matrix, per-class accuracy, and a full classification report.\n",
    "\n",
    "# %%\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Define the Custom Focal Loss Function\n",
    "# This focal loss is designed for sparse (integer) labels. It converts the labels to one-hot,\n",
    "# applies clipping for numerical stability, computes cross-entropy, and weights it based on the focal loss idea.\n",
    "\n",
    "def sparse_focal_loss(gamma=2., alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-class classification with sparse labels.\n",
    "    \n",
    "    Args:\n",
    "        gamma (float): Focusing parameter for modulating factor (1-p).\n",
    "        alpha (float): Weighting factor for the rare class.\n",
    "    \n",
    "    Returns:\n",
    "        A loss function that computes the focal loss.\n",
    "    \"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        # Convert sparse labels to one-hot encoding.\n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "        epsilon = 1e-7\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        # Compute cross-entropy loss.\n",
    "        cross_entropy = -y_true_one_hot * tf.math.log(y_pred)\n",
    "        # Compute the modulating factor.\n",
    "        weights = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        loss = weights * cross_entropy\n",
    "        # Sum the loss over classes, then average over the batch.\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "    return loss_fn\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Read Excel Data and Organize It\n",
    "# We read two sheets from the Excel file, concatenate them, and process each row to extract:\n",
    "# - The multivariate time-series (MDR) with two columns S1 and S2.\n",
    "# - The t5 scalar value.\n",
    "# - The class label based on t5 thresholds (low, normal, high).\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Prepare Data for Training\n",
    "# We build a list of sequences (each sequence is a 2D array with S1 and S2) and a list of labels.\n",
    "# Because the sequences have varying lengths, we pad them to the same length (using -10 as the pad value).\n",
    "# Then, we scale the valid (non-padded) parts of the sequences using a global StandardScaler.\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences so that all have the same length.\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) points.\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Split Data into Train and Test Sets\n",
    "# We split the data using stratification to preserve the class imbalance in both sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# Create an inverse label map for reporting.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Build and Train the Model Using Focal Loss\n",
    "# We build a simple LSTM model with a Masking layer to ignore the padded values.\n",
    "# The output layer uses softmax activation for the three classes.\n",
    "# We compile the model with our custom sparse focal loss function.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Use our custom focal loss in the compile step.\n",
    "model.compile(loss=sparse_focal_loss(gamma=2.0, alpha=0.25), optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6. Evaluate the Model\n",
    "# We evaluate on the test set, print the confusion matrix, per-class accuracies, and a full classification report.\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Get predictions on the test set.\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix.\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate per-class accuracy.\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility (across Python, NumPy and TensorFlow)\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Read Excel Data and Organize It\n",
    "# We read the Excel sheets, combine them, and then process each row to extract:\n",
    "# - The MDR time-series with S1 and S2\n",
    "# - The t5 scalar value\n",
    "# - The class label (\"low\", \"normal\", \"high\") based on t5 thresholds\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Prepare Data for Training\n",
    "# Convert the variable-length sequences into padded arrays.\n",
    "# We use -10 as the pad value so that the Masking layer ignores it.\n",
    "# Then we scale only the valid (non-padded) data points using a global StandardScaler.\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32',\n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) points\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Split Data into Train and Test Sets\n",
    "# We use stratification to preserve the class distribution.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=seed_value, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting, create an inverse label map.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Define the Model Architecture and Ensemble Training\n",
    "# We define a function to build our LSTM-based model.\n",
    "# Then we train multiple models (ensemble members) with the same architecture.\n",
    "# Their predictions will later be combined for a final decision.\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-10., input_shape=input_shape))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Number of ensemble members\n",
    "num_ensemble = 5\n",
    "ensemble_models = []\n",
    "\n",
    "for i in range(num_ensemble):\n",
    "    print(f\"\\nTraining model {i+1}/{num_ensemble}\")\n",
    "    # To ensure some diversity, you could reinitialize seeds or use different hyperparameters.\n",
    "    tf.random.set_seed(seed_value + i)\n",
    "    model = build_model((max_len, 2))\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Ensemble Predictions and Evaluation\n",
    "# For each test sample, we average the predictions from all ensemble members\n",
    "# and then choose the class with the highest averaged probability.\n",
    "# We then compute the confusion matrix, per-class accuracies, and classification report.\n",
    "\n",
    "# Get ensemble predictions: average softmax probabilities over models.\n",
    "ensemble_probs = np.zeros((len(X_test), 3))\n",
    "for model in ensemble_models:\n",
    "    ensemble_probs += model.predict(X_test)\n",
    "ensemble_probs /= num_ensemble\n",
    "\n",
    "# Final prediction: choose the class with the highest probability.\n",
    "y_pred_ensemble = np.argmax(ensemble_probs, axis=1)\n",
    "\n",
    "# Evaluate the ensemble predictions.\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# -------------------------\n",
    "# Set random seeds for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read Excel Data and Organize It\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare Data for Training\n",
    "# Create sequences and labels. Map labels to integers: low -> 0, normal -> 1, high -> 2.\n",
    "X = []\n",
    "y = []\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences with a pad value of -10 (so the Masking layer can ignore them)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) data using StandardScaler\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split Data into Train and Test Sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=seed_value, stratify=y\n",
    ")\n",
    "print(\"Full training set shape:\", X_train_full.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# Further split training set into base training and stacking sets (for meta model)\n",
    "X_train_base, X_train_stack, y_train_base, y_train_stack = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=seed_value, stratify=y_train_full\n",
    ")\n",
    "print(\"Base training set shape:\", X_train_base.shape)\n",
    "print(\"Stacking set shape:\", X_train_stack.shape)\n",
    "\n",
    "# Inverse label map for reporting\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# -------------------------\n",
    "# 4. Define Base Model Architecture\n",
    "def build_base_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-10., input_shape=input_shape))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train Base Models\n",
    "num_base_models = 3  # You can increase the number for more diversity\n",
    "base_models = []\n",
    "for i in range(num_base_models):\n",
    "    print(f\"\\nTraining base model {i+1}/{num_base_models}\")\n",
    "    # Optionally vary the seed for diversity among base models\n",
    "    tf.random.set_seed(seed_value + i)\n",
    "    model = build_base_model((max_len, 2))\n",
    "    model.fit(X_train_base, y_train_base, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    base_models.append(model)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Generate Meta-Features for the Stacking Set\n",
    "# For each base model, predict probabilities on the stacking set and then concatenate them.\n",
    "meta_features_train = []\n",
    "for model in base_models:\n",
    "    preds = model.predict(X_train_stack)  # shape: (num_stack_samples, 3)\n",
    "    meta_features_train.append(preds)\n",
    "# Concatenate along the feature axis\n",
    "meta_X_train = np.concatenate(meta_features_train, axis=1)  # shape: (num_stack_samples, 3*num_base_models)\n",
    "meta_y_train = y_train_stack\n",
    "\n",
    "print(\"Meta training features shape:\", meta_X_train.shape)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Train Meta Model\n",
    "# Here we use a simple MLP as the meta learner.\n",
    "def build_meta_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "meta_model = build_meta_model(meta_X_train.shape[1])\n",
    "meta_model.fit(meta_X_train, meta_y_train, epochs=20, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 8. Evaluate the Stacking Ensemble on the Test Set\n",
    "# First, generate meta features for the test set by obtaining predictions from each base model.\n",
    "meta_features_test = []\n",
    "for model in base_models:\n",
    "    preds = model.predict(X_test)\n",
    "    meta_features_test.append(preds)\n",
    "meta_X_test = np.concatenate(meta_features_test, axis=1)\n",
    "\n",
    "# Use the meta model to get final predictions.\n",
    "y_pred_meta = meta_model.predict(meta_X_test)\n",
    "y_pred_final = np.argmax(y_pred_meta, axis=1)\n",
    "\n",
    "# Evaluate the final predictions\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Per-class accuracies\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------\n",
    "# 1. Define a Custom Attention Layer\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch_size, time_steps, hidden_size)\n",
    "        self.W = self.add_weight(name=\"att_weight\",\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\",\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=\"zeros\",\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name=\"att_u\",\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Compute u_t = tanh(W.h_t + b) for each timestep\n",
    "        uit = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)  # (batch, time_steps, hidden_size)\n",
    "        # Compute scores for each timestep\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)  # (batch, time_steps, 1)\n",
    "        ait = tf.squeeze(ait, -1)  # (batch, time_steps)\n",
    "        a = tf.nn.softmax(ait, axis=1)  # (batch, time_steps)\n",
    "        a = tf.expand_dims(a, -1)       # (batch, time_steps, 1)\n",
    "        # Compute the weighted sum of the inputs\n",
    "        output = tf.reduce_sum(inputs * a, axis=1)  # (batch, hidden_size)\n",
    "        return output\n",
    "\n",
    "# -------------------------\n",
    "# 2. Read Excel Data and Organize It\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), t5 value, and assign a label:\n",
    "    'low' if t5 < t5_lb, 'high' if t5 > t5_ub, else 'normal'.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "        \n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "        \n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "    \n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Prepare Data for Training\n",
    "# Build sequences (each with features S1 and S2) and corresponding labels.\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences with a pad value of -10 (so that the Masking layer ignores them)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) points using StandardScaler\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Split Data into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting purposes, create an inverse label map.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# -------------------------\n",
    "# 5. Build the Model with Attention Mechanism\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "# Use return_sequences=True so that attention can operate over the timesteps.\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "# Apply the custom attention layer\n",
    "model.add(AttentionLayer())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# 6. Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Evaluate the Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read Excel Data and Organize It\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from the bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), the t5 value, and assign a label:\n",
    "    'low' if t5 < t5_lb, 'high' if t5 > t5_ub, else 'normal'.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "        \n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack the tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "        \n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "    \n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare Data for Training\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences to have the same length using a pad value of -10 (for the Masking layer)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale valid (non-padded) points using a global StandardScaler\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split Data into Train and Test Sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting purposes, create an inverse label map.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# -------------------------\n",
    "# 4. Build the CNN-LSTM Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "\n",
    "# Convolutional block\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Optionally, add another Conv1D block\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# LSTM block to capture temporal dependencies\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Final classification layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train the Model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Evaluate the Model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute per-class accuracy\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bounds import bounds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read Excel Data and Organize It\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract:\n",
    "    - The multivariate time-series (MDR) with columns S1 and S2.\n",
    "    - The t5 value.\n",
    "    - A class label ('low', 'normal', or 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "        \n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "        \n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "    \n",
    "    # Remove batches with empty or invalid MDR data\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare Data for Training\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# Map labels to integers: low -> 0, normal -> 1, high -> 2\n",
    "label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "for key, item in data.items():\n",
    "    df_mdr = item[\"MDR\"]\n",
    "    # Use only the S1 and S2 columns as features\n",
    "    sequence = df_mdr[[\"S1\", \"S2\"]].values\n",
    "    X.append(sequence)\n",
    "    y.append(label_map[item[\"class\"]])\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad sequences (using -10 as the pad value so that Masking can ignore these)\n",
    "max_len = max(seq.shape[0] for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, dtype='float32', \n",
    "                         padding='post', truncating='post', value=-10.)\n",
    "\n",
    "# Scale only valid (non-padded) points using a global StandardScaler\n",
    "all_points = []\n",
    "for seq in X_padded:\n",
    "    valid_rows = seq[~np.all(seq == -10., axis=1)]\n",
    "    all_points.append(valid_rows)\n",
    "all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_points)\n",
    "\n",
    "X_scaled = []\n",
    "for seq in X_padded:\n",
    "    seq_scaled = seq.copy()\n",
    "    valid_mask = ~np.all(seq == -10., axis=1)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        seq_scaled[valid_mask] = scaler.transform(seq[valid_mask])\n",
    "    X_scaled.append(seq_scaled)\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split Data into Train and Test Sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# For reporting, create an inverse label map.\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# -------------------------\n",
    "# 4. Build the Bidirectional LSTM Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-10., input_shape=(max_len, 2)))\n",
    "# Use a Bidirectional LSTM to capture both forward and backward dependencies\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train the Model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Evaluate the Model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracies = {}\n",
    "for i in range(3):\n",
    "    if cm[i].sum() > 0:\n",
    "        acc = cm[i, i] / cm[i].sum()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "    class_accuracies[i] = acc\n",
    "    print(f\"Accuracy for class {i} ({inv_label_map[i]}): {acc:.4f}\")\n",
    "\n",
    "avg_class_acc = np.mean(list(class_accuracies.values()))\n",
    "print(f\"\\nAverage Classification Accuracy: {avg_class_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"low\", \"normal\", \"high\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
