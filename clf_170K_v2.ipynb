{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:18:52.364746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-02-14 20:18:52.364786: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Preprocessing complete. Data shape: (8339, 304, 2)\n",
      "Train shape: (6671, 304, 2) Test shape: (1668, 304, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:21:28.910726: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-02-14 20:21:28.910758: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-02-14 20:21:28.910785: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2025-02-14 20:21:28.911147: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "376/376 [==============================] - 120s 304ms/step - loss: 0.4952 - accuracy: 0.8714 - val_loss: 0.4275 - val_accuracy: 0.8862\n",
      "Epoch 2/10\n",
      "376/376 [==============================] - 95s 253ms/step - loss: 0.4761 - accuracy: 0.8737 - val_loss: 0.4259 - val_accuracy: 0.8862\n",
      "Epoch 3/10\n",
      "376/376 [==============================] - 103s 274ms/step - loss: 0.4776 - accuracy: 0.8737 - val_loss: 0.4324 - val_accuracy: 0.8862\n",
      "Epoch 4/10\n",
      "376/376 [==============================] - 96s 255ms/step - loss: 0.4723 - accuracy: 0.8737 - val_loss: 0.4290 - val_accuracy: 0.8862\n",
      "Epoch 5/10\n",
      "376/376 [==============================] - 92s 244ms/step - loss: 0.4707 - accuracy: 0.8737 - val_loss: 0.4261 - val_accuracy: 0.8862\n",
      "Epoch 6/10\n",
      "363/376 [===========================>..] - ETA: 3s - loss: 0.4766 - accuracy: 0.8726"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 318>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=333'>334</a>\u001b[0m \u001b[39m# --- Example 1: Train and evaluate a standard LSTM model ---\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=334'>335</a>\u001b[0m model_lstm \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mbuild_model(model_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlstm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=335'>336</a>\u001b[0m clf\u001b[39m.\u001b[39;49mtrain_model(model_lstm, X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=336'>337</a>\u001b[0m clf\u001b[39m.\u001b[39mevaluate_model(model_lstm, X_test, y_test)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=338'>339</a>\u001b[0m \u001b[39m# --- Example 2: Train and evaluate an Attention-based model ---\u001b[39;00m\n",
      "\u001b[1;32m/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb Cell 1\u001b[0m in \u001b[0;36mExcelTimeSeriesClassifier.train_model\u001b[0;34m(self, model, X_train, y_train, epochs, batch_size, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=289'>290</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtrain_model\u001b[39m(\u001b[39mself\u001b[39m, model, X_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, validation_split\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=290'>291</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generic training method for a given model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=291'>292</a>\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=292'>293</a>\u001b[0m                         validation_split\u001b[39m=\u001b[39;49mvalidation_split, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/mohammad/Documents/GitHub/CooperStandard/clf_170K_v2.ipynb#W0sZmlsZQ%3D%3D?line=293'>294</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1211\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1212\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1213\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1214\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1215\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1216\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1217\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1218\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    907\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 910\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    912\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    913\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    940\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    941\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    945\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3128\u001b[0m   (graph_function,\n\u001b[1;32m   3129\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3131\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1955\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1956\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1957\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1958\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1960\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1961\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m     args,\n\u001b[1;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1964\u001b[0m     executing_eagerly)\n\u001b[1;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    597\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    601\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    602\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    604\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    606\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    607\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    611\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from bounds import bounds\n",
    "    import ast\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # =============================================================================\n",
    "    # Custom Attention Layer (used by the attention model method)\n",
    "    # =============================================================================\n",
    "    class AttentionLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(\n",
    "                name=\"att_weight\",\n",
    "                shape=(input_shape[-1], input_shape[-1]),\n",
    "                initializer=\"glorot_uniform\",\n",
    "                trainable=True\n",
    "            )\n",
    "            self.b = self.add_weight(\n",
    "                name=\"att_bias\",\n",
    "                shape=(input_shape[-1],),\n",
    "                initializer=\"zeros\",\n",
    "                trainable=True\n",
    "            )\n",
    "            self.u = self.add_weight(\n",
    "                name=\"att_u\",\n",
    "                shape=(input_shape[-1], 1),\n",
    "                initializer=\"glorot_uniform\",\n",
    "                trainable=True\n",
    "            )\n",
    "            super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            uit = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "            ait = tf.tensordot(uit, self.u, axes=1)\n",
    "            ait = tf.squeeze(ait, -1)\n",
    "            a = tf.nn.softmax(ait, axis=1)\n",
    "            a = tf.expand_dims(a, -1)\n",
    "            output = tf.reduce_sum(inputs * a, axis=1)\n",
    "            return output\n",
    "\n",
    "    # =============================================================================\n",
    "    # Main Class Definition\n",
    "    # =============================================================================\n",
    "    class ExcelTimeSeriesClassifier:\n",
    "        def __init__(self, file_name, sheet_names, t5_bounds, pad_value=-10.0):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                file_name (str): Path to the Excel file.\n",
    "                sheet_names (list): List of sheet names to read.\n",
    "                t5_bounds (tuple): A tuple (t5_lb, t5_ub) for thresholding.\n",
    "                pad_value (float): The value used for padding.\n",
    "            \"\"\"\n",
    "            self.file_name = file_name\n",
    "            self.sheet_names = sheet_names\n",
    "            self.t5_bounds = t5_bounds  # e.g., (t5_lb, t5_ub)\n",
    "            self.pad_value = pad_value\n",
    "            \n",
    "            # To be populated during processing:\n",
    "            self.df = None\n",
    "            self.data = None\n",
    "            self.X_scaled = None\n",
    "            self.y = None\n",
    "            self.max_len = None\n",
    "            self.scaler = None\n",
    "\n",
    "        @staticmethod\n",
    "        def safe_literal_eval(value):\n",
    "            \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "            if isinstance(value, str):\n",
    "                value = value.replace(\"nan\", \"None\")\n",
    "            try:\n",
    "                return ast.literal_eval(value)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return None\n",
    "\n",
    "        def read_data(self):\n",
    "            \"\"\"Reads the specified sheets from the Excel file and combines them.\"\"\"\n",
    "            dfs = [pd.read_excel(self.file_name, sheet_name=sheet) for sheet in self.sheet_names]\n",
    "            self.df = pd.concat(dfs, ignore_index=True)\n",
    "            print(\"Data shape:\", self.df.shape)\n",
    "\n",
    "        def organize_data(self):\n",
    "            \"\"\"\n",
    "            Processes self.df to extract the time series (MDR), t5 value, and assign a label.\n",
    "            The label is set as:\n",
    "                - 'low' if t5 < t5_lb,\n",
    "                - 'high' if t5 > t5_ub,\n",
    "                - else 'normal'.\n",
    "            \"\"\"\n",
    "            if self.df is None:\n",
    "                self.read_data()\n",
    "            t5_lb, t5_ub = self.t5_bounds\n",
    "            data = {}\n",
    "            for _, row in self.df.iterrows():\n",
    "                if pd.isna(row['t5']):\n",
    "                    continue\n",
    "                batch_number = row[\"batch_number\"]\n",
    "                data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "                t_S1 = self.safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "                t_S2 = self.safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "                if t_S1 is not None and t_S2 is not None:\n",
    "                    t_vals, S1 = zip(*t_S1)\n",
    "                    t_vals, S2 = zip(*t_S2)\n",
    "                    t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "                    # Exclude the first element as indicated\n",
    "                    MDR = pd.DataFrame({\n",
    "                        \"time\": t_vals[1:],\n",
    "                        \"S1\": S1[1:],\n",
    "                        \"S2\": S2[1:]\n",
    "                    })\n",
    "                    MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "                    MDR.fillna(method=\"bfill\", inplace=True)\n",
    "                    MDR.fillna(method=\"ffill\", inplace=True)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "                if row[\"t5\"] < t5_lb:\n",
    "                    data[batch_number][\"class\"] = \"low\"\n",
    "                elif row[\"t5\"] > t5_ub:\n",
    "                    data[batch_number][\"class\"] = \"high\"\n",
    "                else:\n",
    "                    data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "            # Remove batches with empty or invalid MDR\n",
    "            self.data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "            print(f\"# low: {len([1 for v in self.data.values() if v['class'] == 'low'])}\")\n",
    "            print(f\"# high: {len([1 for v in self.data.values() if v['class'] == 'high'])}\")\n",
    "            print(f\"# normal: {len([1 for v in self.data.values() if v['class'] == 'normal'])}\")\n",
    "\n",
    "        def preprocess_data(self):\n",
    "            \"\"\"\n",
    "            Converts the organized data into padded sequences and scales the valid values.\n",
    "            Sets:\n",
    "            - self.X_scaled: 3D NumPy array (samples, max_timesteps, features)\n",
    "            - self.y: Array of integer labels (0: low, 1: normal, 2: high)\n",
    "            - self.max_len: The maximum sequence length used for padding.\n",
    "            \"\"\"\n",
    "            if self.data is None:\n",
    "                self.organize_data()\n",
    "            X, y = [], []\n",
    "            label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "            for item in self.data.values():\n",
    "                sequence = item[\"MDR\"][[\"S1\", \"S2\"]].values\n",
    "                X.append(sequence)\n",
    "                y.append(label_map[item[\"class\"]])\n",
    "            self.y = np.array(y)\n",
    "\n",
    "            # Pad sequences to the same length\n",
    "            self.max_len = max(seq.shape[0] for seq in X)\n",
    "            X_padded = pad_sequences(X, maxlen=self.max_len, dtype='float32',\n",
    "                                    padding='post', truncating='post', value=self.pad_value)\n",
    "\n",
    "            # Scale only the valid (non-padded) values\n",
    "            all_points = []\n",
    "            for seq in X_padded:\n",
    "                valid_rows = seq[~np.all(seq == self.pad_value, axis=1)]\n",
    "                all_points.append(valid_rows)\n",
    "            all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(all_points)\n",
    "\n",
    "            X_scaled = []\n",
    "            for seq in X_padded:\n",
    "                seq_scaled = seq.copy()\n",
    "                valid_mask = ~np.all(seq == self.pad_value, axis=1)\n",
    "                if np.sum(valid_mask) > 0:\n",
    "                    seq_scaled[valid_mask] = self.scaler.transform(seq[valid_mask])\n",
    "                X_scaled.append(seq_scaled)\n",
    "            self.X_scaled = np.array(X_scaled)\n",
    "            print(\"Preprocessing complete. Data shape:\", self.X_scaled.shape)\n",
    "\n",
    "        def split_data(self, test_size=0.2, random_state=42, stratify=True):\n",
    "            \"\"\"\n",
    "            Splits the processed data into training and test sets.\n",
    "            \n",
    "            Returns:\n",
    "                X_train, X_test, y_train, y_test\n",
    "            \"\"\"\n",
    "            if self.X_scaled is None or self.y is None:\n",
    "                self.preprocess_data()\n",
    "            strat = self.y if stratify else None\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                self.X_scaled, self.y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "            )\n",
    "            print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "            return X_train, X_test, y_train, y_test\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # Model building and training methods\n",
    "        # -------------------------------------------------------------------------\n",
    "        def build_binary_model(self, input_shape):\n",
    "            \"\"\"Builds a binary classification model (one-vs-all).\"\"\"\n",
    "            model = Sequential()\n",
    "            model.add(Masking(mask_value=self.pad_value, input_shape=input_shape))\n",
    "            model.add(LSTM(64))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            return model\n",
    "\n",
    "        def train_binary_models(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.1):\n",
    "            \"\"\"\n",
    "            Trains one binary model per class (0: low, 1: normal, 2: high) in a one-vs-all setting.\n",
    "            \n",
    "            Returns:\n",
    "                A dictionary mapping class index to its trained model.\n",
    "            \"\"\"\n",
    "            from sklearn.utils.class_weight import compute_class_weight\n",
    "            label_names = {0: \"low\", 1: \"normal\", 2: \"high\"}\n",
    "            binary_models = {}\n",
    "            for cls in [0, 1, 2]:\n",
    "                print(f\"\\nTraining binary model for class {cls} ({label_names[cls]})\")\n",
    "                y_train_bin = (y_train == cls).astype(int)\n",
    "                classes_bin = np.unique(y_train_bin)\n",
    "                class_weights = compute_class_weight(class_weight='balanced', classes=classes_bin, y=y_train_bin)\n",
    "                class_weight_dict = {cls_val: weight for cls_val, weight in zip(classes_bin, class_weights)}\n",
    "                model = self.build_binary_model((self.max_len, 2))\n",
    "                model.fit(X_train, y_train_bin, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_split=validation_split, class_weight=class_weight_dict, verbose=1)\n",
    "                binary_models[cls] = model\n",
    "            return binary_models\n",
    "\n",
    "        def build_model(self, model_type=\"lstm\"):\n",
    "            \"\"\"\n",
    "            Builds a model based on the desired architecture.\n",
    "            \n",
    "            Args:\n",
    "                model_type (str): One of \"lstm\", \"attention\", \"cnn_lstm\", or \"bidirectional\".\n",
    "            \n",
    "            Returns:\n",
    "                A compiled Keras model.\n",
    "            \"\"\"\n",
    "            if model_type == \"lstm\":\n",
    "                model = Sequential()\n",
    "                model.add(Masking(mask_value=self.pad_value, input_shape=(self.max_len, 2)))\n",
    "                model.add(LSTM(64))\n",
    "                model.add(Dropout(0.5))\n",
    "                model.add(Dense(3, activation='softmax'))\n",
    "                model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                return model\n",
    "\n",
    "            elif model_type == \"attention\":\n",
    "                model = Sequential()\n",
    "                model.add(Masking(mask_value=self.pad_value, input_shape=(self.max_len, 2)))\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(AttentionLayer())\n",
    "                model.add(Dropout(0.5))\n",
    "                model.add(Dense(3, activation='softmax'))\n",
    "                model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                return model\n",
    "\n",
    "            elif model_type == \"cnn_lstm\":\n",
    "                model = Sequential()\n",
    "                model.add(Masking(mask_value=self.pad_value, input_shape=(self.max_len, 2)))\n",
    "                model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "                model.add(MaxPooling1D(pool_size=2))\n",
    "                model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "                model.add(MaxPooling1D(pool_size=2))\n",
    "                model.add(LSTM(64))\n",
    "                model.add(Dropout(0.5))\n",
    "                model.add(Dense(3, activation='softmax'))\n",
    "                model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                return model\n",
    "\n",
    "            elif model_type == \"bidirectional\":\n",
    "                model = Sequential()\n",
    "                model.add(Masking(mask_value=self.pad_value, input_shape=(self.max_len, 2)))\n",
    "                model.add(Bidirectional(LSTM(64)))\n",
    "                model.add(Dropout(0.5))\n",
    "                model.add(Dense(3, activation='softmax'))\n",
    "                model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                return model\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported model type. Choose from 'lstm', 'attention', 'cnn_lstm', or 'bidirectional'.\")\n",
    "\n",
    "        def train_model(self, model, X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, **kwargs):\n",
    "            \"\"\"Generic training method for a given model.\"\"\"\n",
    "            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_split=validation_split, verbose=1, **kwargs)\n",
    "            return history\n",
    "\n",
    "        def evaluate_model(self, model, X_test, y_test, label_names=[\"low\", \"normal\", \"high\"]):\n",
    "            \"\"\"\n",
    "            Evaluates the model on test data and prints loss, accuracy, confusion matrix, and classification report.\n",
    "            \n",
    "            Returns:\n",
    "                Predicted labels.\n",
    "            \"\"\"\n",
    "            test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "            print(\"Test Loss:\", test_loss)\n",
    "            print(\"Test Accuracy:\", test_acc)\n",
    "            y_pred_prob = model.predict(X_test)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            print(cm)\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "            return y_pred\n",
    "\n",
    "    # =============================================================================\n",
    "    # Example Usage\n",
    "    # =============================================================================\n",
    "    if __name__ == \"__main__\":\n",
    "        # Initialize the classifier with file info and thresholds\n",
    "        file_name = \"DataOn2025Jan08.xlsx\"\n",
    "        sheets = [\"NES170K07Line1\", \"NES170K07Line2\"]\n",
    "        t5_bounds = bounds[\"170K\"]\n",
    "\n",
    "        clf = ExcelTimeSeriesClassifier(file_name, sheets, t5_bounds)\n",
    "        \n",
    "        # Read, organize, and preprocess the data\n",
    "        clf.read_data()\n",
    "        clf.organize_data()\n",
    "        clf.preprocess_data()\n",
    "        \n",
    "        # Split into train/test sets\n",
    "        X_train, X_test, y_train, y_test = clf.split_data(test_size=0.2, random_state=16)\n",
    "        \n",
    "        # --- Example 1: Train and evaluate a standard LSTM model ---\n",
    "        model_lstm = clf.build_model(model_type=\"lstm\")\n",
    "        clf.train_model(model_lstm, X_train, y_train, epochs=10, batch_size=16)\n",
    "        clf.evaluate_model(model_lstm, X_test, y_test)\n",
    "        \n",
    "        # --- Example 2: Train and evaluate an Attention-based model ---\n",
    "        model_att = clf.build_model(model_type=\"attention\")\n",
    "        clf.train_model(model_att, X_train, y_train, epochs=10, batch_size=16)\n",
    "        clf.evaluate_model(model_att, X_test, y_test)\n",
    "        \n",
    "        # --- Example 3: Train and evaluate a CNN-LSTM model ---\n",
    "        model_cnn_lstm = clf.build_model(model_type=\"cnn_lstm\")\n",
    "        clf.train_model(model_cnn_lstm, X_train, y_train, epochs=10, batch_size=16)\n",
    "        clf.evaluate_model(model_cnn_lstm, X_test, y_test)\n",
    "        \n",
    "        # --- Example 4: Train and evaluate a Bidirectional LSTM model ---\n",
    "        model_bi = clf.build_model(model_type=\"bidirectional\")\n",
    "        clf.train_model(model_bi, X_train, y_train, epochs=10, batch_size=16)\n",
    "        clf.evaluate_model(model_bi, X_test, y_test)\n",
    "        \n",
    "        # --- Example 5: Train binary one-vs-all models ---\n",
    "        binary_models = clf.train_binary_models(X_train, y_train, epochs=10, batch_size=16)\n",
    "        # (You would then use each binary modelâ€™s prediction to combine a final decision.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Preprocessing complete. Data shape: (8339, 304, 2)\n",
      "Train shape: (6671, 304, 2) Test shape: (1668, 304, 2)\n",
      "Epoch 1/10\n",
      "376/376 [==============================] - 28s 66ms/step - loss: 1.1793 - accuracy: 0.3507 - val_loss: 1.1456 - val_accuracy: 0.0719\n",
      "Epoch 2/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1257 - accuracy: 0.3138 - val_loss: 1.0745 - val_accuracy: 0.1737\n",
      "Epoch 3/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1160 - accuracy: 0.3008 - val_loss: 1.0976 - val_accuracy: 0.2410\n",
      "Epoch 4/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1047 - accuracy: 0.2849 - val_loss: 1.0077 - val_accuracy: 0.6692\n",
      "Epoch 5/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1043 - accuracy: 0.2702 - val_loss: 1.0700 - val_accuracy: 0.3817\n",
      "Epoch 6/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1052 - accuracy: 0.3402 - val_loss: 1.1577 - val_accuracy: 0.0479\n",
      "Epoch 7/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1031 - accuracy: 0.2176 - val_loss: 1.1094 - val_accuracy: 0.2275\n",
      "Epoch 8/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1009 - accuracy: 0.2274 - val_loss: 1.1133 - val_accuracy: 0.2126\n",
      "Epoch 9/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1022 - accuracy: 0.2764 - val_loss: 1.1428 - val_accuracy: 0.0659\n",
      "Epoch 10/10\n",
      "376/376 [==============================] - 24s 64ms/step - loss: 1.1046 - accuracy: 0.1909 - val_loss: 1.0964 - val_accuracy: 0.2934\n",
      "Weighted F1 Score: 0.36943613886118154\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 36  20  17]\n",
      " [661 383 416]\n",
      " [ 60  31  44]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.05      0.49      0.09        73\n",
      "      normal       0.88      0.26      0.40      1460\n",
      "        high       0.09      0.33      0.14       135\n",
      "\n",
      "    accuracy                           0.28      1668\n",
      "   macro avg       0.34      0.36      0.21      1668\n",
      "weighted avg       0.78      0.28      0.37      1668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bounds import bounds\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, make_scorer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Custom Attention Layer (used by the attention model method)\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"att_bias\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.u = self.add_weight(\n",
    "            name=\"att_u\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        uit = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)\n",
    "        ait = tf.squeeze(ait, -1)\n",
    "        a = tf.nn.softmax(ait, axis=1)\n",
    "        a = tf.expand_dims(a, -1)\n",
    "        output = tf.reduce_sum(inputs * a, axis=1)\n",
    "        return output\n",
    "\n",
    "# Main Class Definition\n",
    "class ExcelTimeSeriesClassifier:\n",
    "    def __init__(self, file_name, sheet_names, t5_bounds, pad_value=-10.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_name (str): Path to the Excel file.\n",
    "            sheet_names (list): List of sheet names to read.\n",
    "            t5_bounds (tuple): A tuple (t5_lb, t5_ub) for thresholding.\n",
    "            pad_value (float): The value used for padding.\n",
    "        \"\"\"\n",
    "        self.file_name = file_name\n",
    "        self.sheet_names = sheet_names\n",
    "        self.t5_bounds = t5_bounds  # e.g., (t5_lb, t5_ub)\n",
    "        self.pad_value = pad_value\n",
    "        \n",
    "        # To be populated during processing:\n",
    "        self.df = None\n",
    "        self.data = None\n",
    "        self.X_scaled = None\n",
    "        self.y = None\n",
    "        self.max_len = None\n",
    "        self.scaler = None\n",
    "\n",
    "    @staticmethod\n",
    "    def safe_literal_eval(value):\n",
    "        \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(\"nan\", \"None\")\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"Reads the specified sheets from the Excel file and combines them.\"\"\"\n",
    "        dfs = [pd.read_excel(self.file_name, sheet_name=sheet) for sheet in self.sheet_names]\n",
    "        self.df = pd.concat(dfs, ignore_index=True)\n",
    "        print(\"Data shape:\", self.df.shape)\n",
    "\n",
    "    def organize_data(self):\n",
    "        \"\"\"\n",
    "        Processes self.df to extract the time series (MDR), t5 value, and assign a label.\n",
    "        The label is set as:\n",
    "            - 'low' if t5 < t5_lb,\n",
    "            - 'high' if t5 > t5_ub,\n",
    "            - else 'normal'.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            self.read_data()\n",
    "        t5_lb, t5_ub = self.t5_bounds\n",
    "        data = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            if pd.isna(row['t5']):\n",
    "                continue\n",
    "            batch_number = row[\"batch_number\"]\n",
    "            data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "            t_S1 = self.safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "            t_S2 = self.safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "            if t_S1 is not None and t_S2 is not None:\n",
    "                t_vals, S1 = zip(*t_S1)\n",
    "                t_vals, S2 = zip(*t_S2)\n",
    "                t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "                # Exclude the first element as indicated\n",
    "                MDR = pd.DataFrame({\n",
    "                    \"time\": t_vals[1:],\n",
    "                    \"S1\": S1[1:],\n",
    "                    \"S2\": S2[1:]\n",
    "                })\n",
    "                MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "                MDR.bfill(inplace=True)\n",
    "                MDR.ffill(inplace=True)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "            if row[\"t5\"] < t5_lb:\n",
    "                data[batch_number][\"class\"] = \"low\"\n",
    "            elif row[\"t5\"] > t5_ub:\n",
    "                data[batch_number][\"class\"] = \"high\"\n",
    "            else:\n",
    "                data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "        # Remove batches with empty or invalid MDR\n",
    "        self.data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "        print(f\"# low: {len([1 for v in self.data.values() if v['class'] == 'low'])}\")\n",
    "        print(f\"# high: {len([1 for v in self.data.values() if v['class'] == 'high'])}\")\n",
    "        print(f\"# normal: {len([1 for v in self.data.values() if v['class'] == 'normal'])}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Converts the organized data into padded sequences and scales the valid values.\n",
    "        Sets:\n",
    "          - self.X_scaled: 3D NumPy array (samples, max_timesteps, features)\n",
    "          - self.y: Array of integer labels (0: low, 1: normal, 2: high)\n",
    "          - self.max_len: The maximum sequence length used for padding.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            self.organize_data()\n",
    "        X, y = [], []\n",
    "        label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "        for item in self.data.values():\n",
    "            sequence = item[\"MDR\"][[\"S1\", \"S2\"]].values\n",
    "            X.append(sequence)\n",
    "            y.append(label_map[item[\"class\"]])\n",
    "        self.y = np.array(y)\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        self.max_len = max(seq.shape[0] for seq in X)\n",
    "        X_padded = pad_sequences(X, maxlen=self.max_len, dtype='float32',\n",
    "                                 padding='post', truncating='post', value=self.pad_value)\n",
    "\n",
    "        # Scale only the valid (non-padded) values\n",
    "        all_points = []\n",
    "        for seq in X_padded:\n",
    "            valid_rows = seq[~np.all(seq == self.pad_value, axis=1)]\n",
    "            all_points.append(valid_rows)\n",
    "        all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(all_points)\n",
    "\n",
    "        X_scaled = []\n",
    "        for seq in X_padded:\n",
    "            seq_scaled = seq.copy()\n",
    "            valid_mask = ~np.all(seq == self.pad_value, axis=1)\n",
    "            if np.sum(valid_mask) > 0:\n",
    "                seq_scaled[valid_mask] = self.scaler.transform(seq[valid_mask])\n",
    "            X_scaled.append(seq_scaled)\n",
    "        self.X_scaled = np.array(X_scaled)\n",
    "        print(\"Preprocessing complete. Data shape:\", self.X_scaled.shape)\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=42, stratify=True):\n",
    "        \"\"\"\n",
    "        Splits the processed data into training and test sets.\n",
    "        \n",
    "        Returns:\n",
    "            X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        if self.X_scaled is None or self.y is None:\n",
    "            self.preprocess_data()\n",
    "        strat = self.y if stratify else None\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X_scaled, self.y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "        print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def build_cnn_lstm_model(self, input_shape):\n",
    "        \"\"\"Builds an enhanced CNN-LSTM model with BatchNormalization and class weights.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Masking(mask_value=self.pad_value, input_shape=input_shape))\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())  # Batch Normalization\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())  # Batch Normalization\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(LSTM(64))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_cnn_lstm_model(self, model, X_train, y_train, epochs=20, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Trains the CNN-LSTM model with class weights.\"\"\"\n",
    "        # Compute class weights to handle imbalanced data\n",
    "        class_labels = np.unique(y_train)\n",
    "        class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train)\n",
    "        class_weight_dict = dict(zip(class_labels, class_weights))  # Convert to dictionary\n",
    "        \n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                            validation_split=validation_split, class_weight=class_weight_dict, verbose=1)\n",
    "        return history\n",
    "\n",
    "    def evaluate_cnn_lstm_model(self, model, X_test, y_test, label_names=[\"low\", \"normal\", \"high\"]):\n",
    "        \"\"\"Evaluates the CNN-LSTM model with F1-score and detailed metrics.\"\"\"\n",
    "        y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cr = classification_report(y_test, y_pred, target_names=label_names)\n",
    "        \n",
    "        print(\"Weighted F1 Score:\", f1)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(cr)\n",
    "        return y_pred\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the classifier\n",
    "    file_name = \"DataOn2025Jan08.xlsx\"\n",
    "    sheets = [\"NES170K07Line1\", \"NES170K07Line2\"]\n",
    "    t5_bounds = bounds[\"170K\"]\n",
    "\n",
    "    clf = ExcelTimeSeriesClassifier(file_name, sheets, t5_bounds)\n",
    "    \n",
    "    # Read, organize, and preprocess the data\n",
    "    clf.read_data()\n",
    "    clf.organize_data()\n",
    "    clf.preprocess_data()\n",
    "    \n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, y_train, y_test = clf.split_data(test_size=0.2, random_state=16, stratify=True)\n",
    "    \n",
    "    # Build, train, and evaluate the CNN-LSTM model\n",
    "    cnn_lstm_model = clf.build_cnn_lstm_model((clf.max_len, 2))\n",
    "    history = clf.train_cnn_lstm_model(cnn_lstm_model, X_train, y_train, epochs=10, batch_size=16)\n",
    "    y_pred = clf.evaluate_cnn_lstm_model(cnn_lstm_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Preprocessing complete. Data shape: (8339, 304, 2)\n",
      "Train shape: (6671, 304, 2) Test shape: (1668, 304, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bounds import bounds\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Custom Attention Layer (used by the attention model method)\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"att_bias\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.u = self.add_weight(\n",
    "            name=\"att_u\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        uit = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)\n",
    "        ait = tf.squeeze(ait, -1)\n",
    "        a = tf.nn.softmax(ait, axis=1)\n",
    "        a = tf.expand_dims(a, -1)\n",
    "        output = tf.reduce_sum(inputs * a, axis=1)\n",
    "        return output\n",
    "\n",
    "# Main Class Definition\n",
    "class ExcelTimeSeriesClassifier:\n",
    "    def __init__(self, file_name, sheet_names, t5_bounds, pad_value=-10.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_name (str): Path to the Excel file.\n",
    "            sheet_names (list): List of sheet names to read.\n",
    "            t5_bounds (tuple): A tuple (t5_lb, t5_ub) for thresholding.\n",
    "            pad_value (float): The value used for padding.\n",
    "        \"\"\"\n",
    "        self.file_name = file_name\n",
    "        self.sheet_names = sheet_names\n",
    "        self.t5_bounds = t5_bounds  # e.g., (t5_lb, t5_ub)\n",
    "        self.pad_value = pad_value\n",
    "        \n",
    "        # To be populated during processing:\n",
    "        self.df = None\n",
    "        self.data = None\n",
    "        self.X_scaled = None\n",
    "        self.y = None\n",
    "        self.max_len = None\n",
    "        self.scaler = None\n",
    "\n",
    "    @staticmethod\n",
    "    def safe_literal_eval(value):\n",
    "        \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(\"nan\", \"None\")\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"Reads the specified sheets from the Excel file and combines them.\"\"\"\n",
    "        dfs = [pd.read_excel(self.file_name, sheet_name=sheet) for sheet in self.sheet_names]\n",
    "        self.df = pd.concat(dfs, ignore_index=True)\n",
    "        print(\"Data shape:\", self.df.shape)\n",
    "\n",
    "    def organize_data(self):\n",
    "        \"\"\"\n",
    "        Processes self.df to extract the time series (MDR), t5 value, and assign a label.\n",
    "        The label is set as:\n",
    "            - 'low' if t5 < t5_lb,\n",
    "            - 'high' if t5 > t5_ub,\n",
    "            - else 'normal'.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            self.read_data()\n",
    "        t5_lb, t5_ub = self.t5_bounds\n",
    "        data = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            if pd.isna(row['t5']):\n",
    "                continue\n",
    "            batch_number = row[\"batch_number\"]\n",
    "            data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "            t_S1 = self.safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "            t_S2 = self.safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "            if t_S1 is not None and t_S2 is not None:\n",
    "                t_vals, S1 = zip(*t_S1)\n",
    "                t_vals, S2 = zip(*t_S2)\n",
    "                t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "                # Exclude the first element as indicated\n",
    "                MDR = pd.DataFrame({\n",
    "                    \"time\": t_vals[1:],\n",
    "                    \"S1\": S1[1:],\n",
    "                    \"S2\": S2[1:]\n",
    "                })\n",
    "                MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "                MDR.bfill(inplace=True)\n",
    "                MDR.ffill(inplace=True)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "            if row[\"t5\"] < t5_lb:\n",
    "                data[batch_number][\"class\"] = \"low\"\n",
    "            elif row[\"t5\"] > t5_ub:\n",
    "                data[batch_number][\"class\"] = \"high\"\n",
    "            else:\n",
    "                data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "        # Remove batches with empty or invalid MDR\n",
    "        self.data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "        print(f\"# low: {len([1 for v in self.data.values() if v['class'] == 'low'])}\")\n",
    "        print(f\"# high: {len([1 for v in self.data.values() if v['class'] == 'high'])}\")\n",
    "        print(f\"# normal: {len([1 for v in self.data.values() if v['class'] == 'normal'])}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Converts the organized data into padded sequences and scales the valid values.\n",
    "        Sets:\n",
    "          - self.X_scaled: 3D NumPy array (samples, max_timesteps, features)\n",
    "          - self.y: Array of integer labels (0: low, 1: normal, 2: high)\n",
    "          - self.max_len: The maximum sequence length used for padding.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            self.organize_data()\n",
    "        X, y = [], []\n",
    "        label_map = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "        for item in self.data.values():\n",
    "            sequence = item[\"MDR\"][[\"S1\", \"S2\"]].values\n",
    "            X.append(sequence)\n",
    "            y.append(label_map[item[\"class\"]])\n",
    "        self.y = np.array(y)\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        self.max_len = max(seq.shape[0] for seq in X)\n",
    "        X_padded = pad_sequences(X, maxlen=self.max_len, dtype='float32',\n",
    "                                 padding='post', truncating='post', value=self.pad_value)\n",
    "\n",
    "        # Scale only the valid (non-padded) values\n",
    "        all_points = []\n",
    "        for seq in X_padded:\n",
    "            valid_rows = seq[~np.all(seq == self.pad_value, axis=1)]\n",
    "            all_points.append(valid_rows)\n",
    "        all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(all_points)\n",
    "\n",
    "        X_scaled = []\n",
    "        for seq in X_padded:\n",
    "            seq_scaled = seq.copy()\n",
    "            valid_mask = ~np.all(seq == self.pad_value, axis=1)\n",
    "            if np.sum(valid_mask) > 0:\n",
    "                seq_scaled[valid_mask] = self.scaler.transform(seq[valid_mask])\n",
    "            X_scaled.append(seq_scaled)\n",
    "        self.X_scaled = np.array(X_scaled)\n",
    "        print(\"Preprocessing complete. Data shape:\", self.X_scaled.shape)\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=42, stratify=True):\n",
    "        \"\"\"\n",
    "        Splits the processed data into training and test sets.\n",
    "        \n",
    "        Returns:\n",
    "            X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        if self.X_scaled is None or self.y is None:\n",
    "            self.preprocess_data()\n",
    "        strat = self.y if stratify else None\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X_scaled, self.y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "        print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def build_cnn_lstm_model(self, input_shape):\n",
    "        \"\"\"Builds an enhanced CNN-LSTM model with BatchNormalization and class weights.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Masking(mask_value=self.pad_value, input_shape=input_shape))\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())  # Batch Normalization\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())  # Batch Normalization\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(LSTM(64))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_cnn_lstm_model(self, model, X_train, y_train, epochs=20, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Trains the CNN-LSTM model with class weights.\"\"\"\n",
    "        # Compute class weights to handle imbalanced data\n",
    "        class_labels = np.unique(y_train)\n",
    "        class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train)\n",
    "        class_weight_dict = dict(zip(class_labels, class_weights))  # Convert to dictionary\n",
    "        \n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                            validation_split=validation_split, class_weight=class_weight_dict, verbose=1)\n",
    "        return history\n",
    "\n",
    "    def evaluate_cnn_lstm_model(self, model, X_test, y_test, label_names=[\"low\", \"normal\", \"high\"]):\n",
    "        \"\"\"Evaluates the CNN-LSTM model with F1-score and detailed metrics.\"\"\"\n",
    "        y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cr = classification_report(y_test, y_pred, target_names=label_names)\n",
    "        \n",
    "        print(\"Weighted F1 Score:\", f1)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(cr)\n",
    "        return y_pred\n",
    "        \n",
    "    def create_normal_dataset(self):\n",
    "        \"\"\"Creates a dataset containing only 'normal' data points.\"\"\"\n",
    "        normal_data = [item[\"MDR\"][[\"S1\", \"S2\"]].values for item in self.data.values() if item[\"class\"] == \"normal\"]\n",
    "        \n",
    "        if not normal_data:\n",
    "            raise ValueError(\"No 'normal' data points found. Cannot train anomaly detection model.\")\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        self.max_len = max(seq.shape[0] for seq in normal_data)\n",
    "        X_padded = pad_sequences(normal_data, maxlen=self.max_len, dtype='float32',\n",
    "                                padding='post', truncating='post', value=self.pad_value)\n",
    "\n",
    "        # Scale only the valid (non-padded) values\n",
    "        all_points = []\n",
    "        for seq in X_padded:\n",
    "            valid_rows = seq[~np.all(seq == self.pad_value, axis=1)]\n",
    "            all_points.append(valid_rows)\n",
    "        all_points = np.concatenate(all_points, axis=0)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(all_points)\n",
    "\n",
    "        X_scaled = []\n",
    "        for seq in X_padded:\n",
    "            seq_scaled = seq.copy()\n",
    "            valid_mask = ~np.all(seq == self.pad_value, axis=1)\n",
    "            if np.sum(valid_mask) > 0:\n",
    "                seq_scaled[valid_mask] = self.scaler.transform(seq[valid_mask])\n",
    "            X_scaled.append(seq_scaled)\n",
    "        X_normal = np.array(X_scaled)\n",
    "        return X_normal\n",
    "    \n",
    "    def build_anomaly_detection_model(self, input_shape, nu=0.05):\n",
    "        \"\"\"Builds a OneClassSVM anomaly detection model.\"\"\"\n",
    "        model = OneClassSVM(kernel='rbf', gamma='auto', nu=nu)\n",
    "        return model\n",
    "\n",
    "    def train_anomaly_detection_model(self, model, X_normal):\n",
    "        \"\"\"Trains the anomaly detection model on 'normal' data.\"\"\"\n",
    "        # Reshape X_normal to 2D array\n",
    "        n_samples, time_steps, n_features = X_normal.shape\n",
    "        X_normal_2d = X_normal.reshape((n_samples * time_steps, n_features))\n",
    "\n",
    "        # Remove rows with pad_value\n",
    "        X_normal_2d = X_normal_2d[~np.any(X_normal_2d == self.pad_value, axis=1)]\n",
    "\n",
    "        model.fit(X_normal_2d)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def evaluate_anomaly_detection_model(self, model, X_test, y_test, label_names=[\"low\", \"normal\", \"high\"]):\n",
    "        \"\"\"Evaluates the anomaly detection model on 'low' and 'high' data.\"\"\"\n",
    "        # Reshape X_test to 2D array\n",
    "        n_samples, time_steps, n_features = X_test.shape\n",
    "        X_test_2d = X_test.reshape((n_samples * time_steps, n_features))\n",
    "\n",
    "        # Remove rows with pad_value\n",
    "        X_test_2d = X_test_2d[~np.any(X_test_2d == self.pad_value, axis=1)]\n",
    "\n",
    "        # Predict anomalies (-1 for anomaly, 1 for inlier)\n",
    "        y_pred = model.predict(X_test_2d)\n",
    "\n",
    "        # Convert predictions to anomaly (1) / normal (0) labels\n",
    "        y_pred_binary = np.where(y_pred == -1, 1, 0)  # 1 for anomaly, 0 for normal\n",
    "\n",
    "        # Create binary labels for the test set (1 for anomaly, 0 for normal)\n",
    "        y_test_binary = np.where(y_test != 1, 1, 0)  # Assuming 'normal' class is labeled as 1\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        f1 = f1_score(y_test_binary[:len(y_pred_binary)], y_pred_binary, average='binary')\n",
    "        cm = confusion_matrix(y_test_binary[:len(y_pred_binary)], y_pred_binary)\n",
    "        cr = classification_report(y_test_binary[:len(y_pred_binary)], y_pred_binary, target_names=[\"normal\", \"anomaly\"])\n",
    "\n",
    "        print(\"Anomaly Detection - Weighted F1 Score:\", f1)\n",
    "        print(\"\\nAnomaly Detection - Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"\\nAnomaly Detection - Classification Report:\")\n",
    "        print(cr)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the classifier\n",
    "    file_name = \"DataOn2025Jan08.xlsx\"\n",
    "    sheets = [\"NES170K07Line1\", \"NES170K07Line2\"]\n",
    "    t5_bounds = bounds[\"170K\"]\n",
    "\n",
    "    clf = ExcelTimeSeriesClassifier(file_name, sheets, t5_bounds)\n",
    "    \n",
    "    # Read, organize, and preprocess the data\n",
    "    clf.read_data()\n",
    "    clf.organize_data()\n",
    "    clf.preprocess_data()\n",
    "    \n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, y_train, y_test = clf.split_data(test_size=0.2, random_state=16, stratify=True)\n",
    "\n",
    "    # Create the 'normal' dataset\n",
    "    X_normal = clf.create_normal_dataset()\n",
    "    \n",
    "    # Build, train, and evaluate the anomaly detection model\n",
    "    anomaly_model = clf.build_anomaly_detection_model(input_shape=(clf.max_len, 2))\n",
    "    anomaly_model = clf.train_anomaly_detection_model(anomaly_model, X_normal)\n",
    "    clf.evaluate_anomaly_detection_model(anomaly_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "# low: 365\n",
      "# high: 677\n",
      "# normal: 7297\n",
      "Total sequences: 8339\n",
      "Class distribution: Counter({0: 7297, 1: 1042})\n",
      "Max sequence length: 304\n",
      "Training on normal sequences: (5837, 304, 2)\n",
      "Validation on normal sequences: (1460, 304, 2)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_2 (Masking)         (None, 304, 2)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 16)                1216      \n",
      "                                                                 \n",
      " repeat_vector_2 (RepeatVect  (None, 304, 16)          0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 304, 16)           2112      \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 304, 2)           34        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,362\n",
      "Trainable params: 3,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "183/183 [==============================] - 90s 445ms/step - loss: 16.6472 - val_loss: 15.3455\n",
      "Epoch 2/100\n",
      "183/183 [==============================] - 78s 428ms/step - loss: 16.2574 - val_loss: 15.1934\n",
      "Epoch 3/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1939 - val_loss: 15.1751\n",
      "Epoch 4/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1748 - val_loss: 15.1549\n",
      "Epoch 5/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1656 - val_loss: 15.1571\n",
      "Epoch 6/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1559 - val_loss: 15.1357\n",
      "Epoch 7/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1520 - val_loss: 15.1209\n",
      "Epoch 8/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1466 - val_loss: 15.1189\n",
      "Epoch 9/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1379 - val_loss: 15.1080\n",
      "Epoch 10/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1320 - val_loss: 15.1240\n",
      "Epoch 11/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1330 - val_loss: 15.0991\n",
      "Epoch 12/100\n",
      "183/183 [==============================] - 78s 429ms/step - loss: 16.1294 - val_loss: 15.1007\n",
      "Epoch 13/100\n",
      "183/183 [==============================] - 79s 434ms/step - loss: 16.1231 - val_loss: 15.1026\n",
      "Epoch 14/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1215 - val_loss: 15.0986\n",
      "Epoch 15/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1218 - val_loss: 15.1019\n",
      "Epoch 16/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1175 - val_loss: 15.0930\n",
      "Epoch 17/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1172 - val_loss: 15.0915\n",
      "Epoch 18/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1145 - val_loss: 15.0938\n",
      "Epoch 19/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1182 - val_loss: 15.0827\n",
      "Epoch 20/100\n",
      "183/183 [==============================] - 79s 430ms/step - loss: 16.1129 - val_loss: 15.0955\n",
      "Epoch 21/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1114 - val_loss: 15.0891\n",
      "Epoch 22/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1102 - val_loss: 15.0839\n",
      "Epoch 23/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1082 - val_loss: 15.0822\n",
      "Epoch 24/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1060 - val_loss: 15.0834\n",
      "Epoch 25/100\n",
      "183/183 [==============================] - 79s 430ms/step - loss: 16.1070 - val_loss: 15.0973\n",
      "Epoch 26/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1080 - val_loss: 15.0816\n",
      "Epoch 27/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1088 - val_loss: 15.0797\n",
      "Epoch 28/100\n",
      "183/183 [==============================] - 79s 429ms/step - loss: 16.1057 - val_loss: 15.0990\n",
      "Epoch 29/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1053 - val_loss: 15.0775\n",
      "Epoch 30/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1052 - val_loss: 15.0807\n",
      "Epoch 31/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1039 - val_loss: 15.0893\n",
      "Epoch 32/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1054 - val_loss: 15.0840\n",
      "Epoch 33/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1037 - val_loss: 15.0762\n",
      "Epoch 34/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1035 - val_loss: 15.0773\n",
      "Epoch 35/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1019 - val_loss: 15.0760\n",
      "Epoch 36/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1017 - val_loss: 15.0767\n",
      "Epoch 37/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1020 - val_loss: 15.0757\n",
      "Epoch 38/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1011 - val_loss: 15.0771\n",
      "Epoch 39/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1011 - val_loss: 15.0781\n",
      "Epoch 40/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1007 - val_loss: 15.0746\n",
      "Epoch 41/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1003 - val_loss: 15.0793\n",
      "Epoch 42/100\n",
      "183/183 [==============================] - 79s 432ms/step - loss: 16.1002 - val_loss: 15.0778\n",
      "Epoch 43/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1011 - val_loss: 15.0750\n",
      "Epoch 44/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.1007 - val_loss: 15.0753\n",
      "Epoch 45/100\n",
      "183/183 [==============================] - 79s 431ms/step - loss: 16.0990 - val_loss: 15.0773\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reconstruction error (mean): 0.08276644\n",
      "Anomaly detection threshold: 0.5277\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7191  106]\n",
      " [1013   29]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.88      0.99      0.93      7297\n",
      "     Anomaly       0.21      0.03      0.05      1042\n",
      "\n",
      "    accuracy                           0.87      8339\n",
      "   macro avg       0.55      0.51      0.49      8339\n",
      "weighted avg       0.79      0.87      0.82      8339\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAEWCAYAAACOk1WwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsQ0lEQVR4nO3de7xVVbn/8c8XRDA1kUukAkFeUBRFJfGSSYr3UExEDRXU4mheouyGdY5mUVl4rZNliXj7IYR3s1OKeMtLgiIIRnAUDhAigqKImuDz+2OOvV1s92YvNmvtddnf9+s1X2vNMccc85lzL3jWHHOsORURmJmZWXVqVeoAzMzMrHic6M3MzKqYE72ZmVkVc6I3MzOrYk70ZmZmVcyJ3szMrIo50Zu1AJKGSfprqeMoJUmzJQ0oUFvrHU9JIWmnQrSd2lst6bOFas9aNid6qxqSFkh6N/0n+aqk8ZK2KnVc9Sl0YqjTdo/U/mY1ZRFxW0QcUYRtDZD0YTrmudMBhd7WBmKo2d+abS+TdL+kw3PrRcTuEfFInm1ttqF6hTyekh6R9NU67W8VES8Xon0zJ3qrNoMiYiugL7A3MLq04TRNY4mmzPwrJabc6am6lZRpVadso/azkfrt099+L+BB4C5JIzam/QLEYFZ2nOitKkXEq8BfyBI+AJL2l/SkpDclvZDbjSupg6QbJf1L0huS7s5Z9jVJ8yWtlHSvpO1zloWkcyTNS+3+tySlZTtJelTSKkmvS5qYyh9Lq7+QzkBPTmfGiyV9T9KrwI2SRkh6Ine/cnsCJG0h6QpJC9M2npC0BVDT/ps1Z9d125J0oKRn03rPSjowZ9kjkn4s6W+S3pb0V0mdmvJ3SG2NkfQ3YA3w2bQP50maB8zL8xivV39DIuLViLgGuBS4vObLRerxGZje7ydpmqS3Ug/AlWn1ho7d3yRdJWkFcGl9fxvgGEkvp7/1L3O2e6mkW3P2p7bXQNIY4GDg12l7v87Z55q/8zaSbpa0PP2tf5jT9oj0dx+bPrevSDo67z+QtQhO9FaVJHUFjgbmp/kdgD8BPwE6AN8G7pDUOa1yC/AJYHfgU8BVab1DgZ8BQ4HtgIXA7XU29yXgc8Ceqd6RqfzHwF+BbYGuwK8AIuILafle6ex3Ypr/dIrtM8DIPHZzLLAvcGBa77vAh0BN++3rO7uW1CEdi2uBjsCVwJ8kdcyp9hXgzHQsNic7Xk11Otn+bE12/AAGA/2B3nke49r6G7HdO8ni71XPsmuAayLik8COwKRU3tCx6w+8DHQBxjSwvROAfsA+wPHAWY0FGBE/AB4Hzk/bO7+ear8CtgE+CxwCnEH2t6nRH5gLdAJ+AdxQ82XTDJzorfrcLeltYBHwGnBJKj8NeCAiHoiIDyPiQWAa2VnYdmRfCs6JiDci4oOIeDStNwwYFxHPRcT7ZJcCDpDUI2ebP4+INyPi/4CpfNSL8AFZ0t4+It6LiLpngHV9CFwSEe9HxLsbqpjO6M4CvhERSyJiXUQ8mWJszLHAvIi4JSLWRsQE4B/AoJw6N0bEP1Mck3L2qT7bp96M3GnLnOXjI2J22tYHqexnEbEytZ/PMc6tn69/pdcO9Sz7ANhJUqeIWB0RTzfWVkT8Ku1DQzFcnmL8P+Bq4NSNiLVekloDpwCjI+LtiFgAXEH25anGwoj4fUSsA24i+7LUZVO3bdXDid6qzeCI2BoYAOxKdpYDWcI9KTcZAZ8n+0+xG7AyIt6op73t+egslIhYDawAdsip82rO+zVAzQDA7wIC/q5sxHdjZ3jLI+K9xncRyParHfC/edbPtd4+JQvJb5/q86+IaF9neidn+aJ61skty+cY19dGY2rWX1nPsrOBXYB/pEsXX2qkrXy2n1tnIdl+bapOQBvW/3s1+LeKiDXpbVkOQrXScKK3qpTOyMeTdW9D9p/wLXWS0ZYR8fO0rIOk9vU09S+yLwkApDPVjsCSPGJ4NSK+FhHbA/8B/EYbHmlf91GS75BdTqjZ9qdzlr0OvEfW7dxYO3Wtt09Jd/LYpyaqL57csnyOcVMes3kCWa/O3I9tPGJeRJxK1rV/OTA5bbeh7eSz/W4577vzUY/Cen9Hsks0+bb9Oh/1DOW2Xay/lVUhJ3qrZlcDh0vaC7gVGCTpSEmtJbVLA+C6RsRS4M9kiXhbSW0k1VyrnQCcKamvpLbAT4FnUhfqBkk6KY0VAHiD7D/0D9P8MrJrrhvyArB72nY7ssFlAETEh8A44EpJ26d9OiDFuDxtp6H2HwB2kfSVNCDsZLJr3/c3tk9F0uRjXB9JXSSdT3bZZnQ6VnXrnCapc1r2Zir+kMaP3YZ8J31+ugHfAGrGXswAviCpu6Rt+PgvQRr8LKTu+EnAGElbS/oM8C2yz7NZXpzorWpFxHLgZuC/ImIR2QCpi8n+M18EfIeP/g2cTnbm9A+ys8BRqY2HgP8E7gCWkp1Bn5JnCJ8DnpG0GriX7Hp6zW+jLwVuSpcRhjYQ/z+By4CHyEab173G/21gFvAsWff05UCr1H07Bvhban//Ou2uIBtAeBFZF/l3gS9FxOt57ldd2+vjv6M/Md+VN/EY53pT0jtkx+QY4KSIGNdA3aOA2elvcw1wSkS829ixa8Q9wHSyxP4n4AaANB5kIjAzLa/7heoaYEgaNX9tPe1eQNYr8DLZZ+D/kX3JM8uLIprSI2ZmZmaVwGf0ZmZmVcyJ3szMrIo50ZuZmVUxJ3ozM7MqVpUPZ+jUqVP06NGj1GFYIcxNP4HuVd9dTM3MrMb06dNfj4jOdcurMtH36NGDadOmlToMK4QBA7LXRx4pZRRmZmVPUt07XgLuujczM6tqVXlGb1Xkhz8sdQRmZhXNid7K28CBpY7AzKyiOdFbeZsxI3vt27eUUZhZE33wwQcsXryY997L98GM1ph27drRtWtX2rRpk1d9J3orb6NGZa8ejGdWkRYvXszWW29Njx49kFTqcCpeRLBixQoWL15Mz54981rHg/HMzKxo3nvvPTp27OgkXyCS6Nix40b1kDjRm5lZUTnJF9bGHk8nejMzsyrma/RmZtZsRt85q6Dt/ezLfRqtI4lvfetbXHHFFQCMHTuW1atXc+mllxY0lg0ZMGAAY8eOpV+/fs22zRpO9Hlqyocznw+gNeKnPy11BGZW4dq2bcudd97J6NGj6dSp00avv3btWjbbrHLTZeVGbi3DgQeWOgIzq3CbbbYZI0eO5KqrrmLMmDHrLVuwYAFnnXUWr7/+Op07d+bGG2+ke/fujBgxgnbt2vH8889z0EEHsXLlSrbYYguef/55XnvtNcaNG8fNN9/MU089Rf/+/Rk/fjwA5557Ls8++yzvvvsuQ4YM4Uc/+lEJ9nh9vkZv5e3JJ7PJzGwTnHfeedx2222sWrVqvfILLriA4cOHM3PmTIYNG8aFF15Yu2zx4sU8+eSTXHnllQC88cYbPPXUU1x11VUcd9xxfPOb32T27NnMmjWLGemeH2PGjGHatGnMnDmTRx99lJkzZzbbPjbEid7K28UXZ5OZ2Sb45Cc/yRlnnMG11167XvlTTz3FV77yFQBOP/10nnjiidplJ510Eq1bt66dHzRoEJLo06cPXbp0oU+fPrRq1Yrdd9+dBQsWADBp0iT22Wcf9t57b2bPns2cOXOKv3ONKGqil9Re0mRJ/5D0kqQDJHWQ9KCkeel121RXkq6VNF/STEn75LQzPNWfJ2l4MWM2M7PqNGrUKG644QbeeeedvOpvueWW6823bdsWgFatWtW+r5lfu3Ytr7zyCmPHjmXKlCnMnDmTY489tizuCFjsM/prgP+JiF2BvYCXgO8DUyJiZ2BKmgc4Gtg5TSOB6wAkdQAuAfoD+wGX1Hw5MDMzy1eHDh0YOnQoN9xwQ23ZgQceyO233w7AbbfdxsEHH9zk9t966y223HJLttlmG5YtW8af//znTY65EIo2GE/SNsAXgBEAEfFv4N+SjgcGpGo3AY8A3wOOB26OiACeTr0B26W6D0bEytTug8BRwIRixW5mZsVR6l8jXXTRRfz617+unf/Vr37FmWeeyS9/+cvawXhNtddee7H33nuz66670q1bNw466KBChLzJijnqviewHLhR0l7AdOAbQJeIWJrqvAp0Se93ABblrL84lTVUvh5JI8l6AujevXvh9sLMzCra6tWra9936dKFNWvW1M5/5jOf4eGHH/7YOjWj6Oub79GjBy+++GK9y+quV+OREj6vo5iJfjNgH+CCiHhG0jV81E0PQESEpCjExiLieuB6gH79+hWkTSsDV19d6gjMzCpaMa/RLwYWR8QzaX4yWeJflrrkSa+vpeVLgG4563dNZQ2VW0vQt68fUWtmtgmKlugj4lVgkaReqegwYA5wL1Azcn44cE96fy9wRhp9vz+wKnXx/wU4QtK2aRDeEanMWoKHHsomMzNrkmLfGe8C4DZJmwMvA2eSfbmYJOlsYCEwNNV9ADgGmA+sSXWJiJWSfgw8m+pdVjMwz1qAn/wkex04sLRxmJlVqKIm+oiYAdR3B//D6qkbwHkNtDMOGFfQ4MzMzFoA3xnPzMysivmhNmZm1nzu+0Zh2xt0Td5V7777bk444QReeukldt1118LGkaetttpqvZ/7NQef0ZuZWYswYcIEPv/5zzNhQsu635oTvZW33/0um8zMNsHq1at54oknuOGGG2pvefvII48wYMAAhgwZwq677sqwYcPIhovBlClT2HvvvenTpw9nnXUW77//PpDdLGf06NH07duXfv368dxzz3HkkUey44478tvf/rZ2W4cddhj77LMPffr04Z577vlYPGeccQZ333137fywYcPqrVcITvRW3nr1yiYzs01wzz33cNRRR7HLLrvQsWNHpk+fDsDzzz/P1VdfzZw5c3j55Zf529/+xnvvvceIESOYOHEis2bNYu3atVx33XW1bXXv3p0ZM2Zw8MEHM2LECCZPnszTTz/NJZdcAkC7du246667eO6555g6dSoXXXRR7ReIGmeffXbtXfRWrVrFk08+ybHHHluUfXeit/J2333ZZGa2CSZMmMApp5wCwCmnnFLbfb/ffvvRtWtXWrVqRd++fVmwYAFz586lZ8+e7LLLLgAMHz6cxx57rLat4447DoA+ffrQv39/tt56azp37kzbtm158803iQguvvhi9txzTwYOHMiSJUtYtmzZevEccsghzJs3j+XLlzNhwgROPPFENtusOMPmPBjPytsVV2SvgwaVNg4zq1grV67k4YcfZtasWUhi3bp1SOLYY49d73GzrVu3Zu3atY2219jjam+77TaWL1/O9OnTadOmDT169Kj3cbVnnHEGt956K7fffvsmPUynMT6jNzOzqjZ58mROP/10Fi5cyIIFC1i0aBE9e/bk8ccfr7d+r169WLBgAfPnzwfglltu4ZBDDsl7e6tWreJTn/oUbdq0YerUqSxcuLDeeiNGjODq9DyP3r17b9xObQSf0ZuZWfPZiJ/DFcqECRP43ve+t17ZiSeeyHXXXceOO+74sfrt2rXjxhtv5KSTTmLt2rV87nOf45xzzsl7e8OGDWPQoEH06dOHfv36NfhTvi5durDbbrsxePDgjdqfjaW6AwSqQb9+/WLatGkFbXP0nbM2ep1SP3e5KgwYkL2W8BGPZtZ0L730ErvttlupwyhLa9asoU+fPjz33HNss802G7VufcdV0vSI+NjdaN11b2Zm1sweeughdtttNy644IKNTvIby133Vt5uuaXUEZiZFdzAgQMbvHZfaE70Vt66dSt1BGZmFc1d91beJk7MJjMzaxKf0Vt5q7kb1cknlzYOM7MK5TN6MzOzKuZEb2ZmVWvFihX07duXvn378ulPf5oddtiBvn370r59+6LcpObSSy9l7NixG7XOVlttVW95zX30N5UTvZmZVa2OHTsyY8YMZsyYwTnnnMM3v/nN2vlWrRpPgfncErfcOdGbmVmLtG7dOr72ta+x++67c8QRR/Duu+8CMGDAAEaNGkW/fv245pprmD59Oocccgj77rsvRx55JEuXLgXg2muvpXfv3uy55561D8wBmDNnDgMGDOCzn/0s1157bW35lVdeyR577MEee+xRe+vbXBHB+eefT69evRg4cCCvvfZaQfbTg/GsvBWg28rMykjN3S5zDR0KX/86rFkDxxzz8eUjRmTT66/DkCHrL9uEu2bOmzePCRMm8Pvf/56hQ4dyxx13cNpppwHw73//m2nTpvHBBx9wyCGHcM8999C5c2cmTpzID37wA8aNG8fPf/5zXnnlldqn1tX4xz/+wdSpU3n77bfp1asX5557LjNnzuTGG2/kmWeeISLo378/hxxyCHvvvXftenfddRdz585lzpw5LFu2jN69e3PWWWc1ef9qONFbeevUqdQRmFmV6tmzJ3379gVg3333ZcGCBbXLTk6/9Jk7dy4vvvgihx9+OJD1Amy33XYA7LnnngwbNozBgwevd7/6mqfitW3blk996lMsW7aMJ554ghNOOIEtt9wSgC9/+cs8/vjj6yX6xx57jFNPPZXWrVuz/fbbc+ihhxZkP53orbyNH5+9jhhRyijMrFA2dAb+iU9seHmnTgV97kXdR9TWdN0DtQk5Ith999156qmnPrb+n/70Jx577DHuu+8+xowZw6xZs+ptt9TX+X2N3srb+PEfJXszs2bWq1cvli9fXpvoP/jgA2bPns2HH37IokWL+OIXv8jll1/OqlWrWL16dYPtHHzwwdx9992sWbOGd955h7vuuouDDz54vTpf+MIXmDhxIuvWrWPp0qVMnTq1IPvgM3ozM7MGbL755kyePJkLL7yQVatWsXbtWkaNGsUuu+zCaaedxqpVq4gILrzwQtq3b99gO/vssw8jRoxgv/32A+CrX/3qet32ACeccAIPP/wwvXv3pnv37hxwwAEF2YeiPqZW0gLgbWAdsDYi+knqAEwEegALgKER8YYkAdcAxwBrgBER8VxqZzjww9TsTyLipg1t14+prSJ+TK1ZRfNjaouj3B5T+8WI6Juz8e8DUyJiZ2BKmgc4Gtg5TSOB6wDSF4NLgP7AfsAlkrZthrjNzMwqXimu0R8P1JyR3wQMzim/OTJPA+0lbQccCTwYESsj4g3gQeCoZo7ZzMysIhX7Gn0Af5UUwO8i4nqgS0QsTctfBbqk9zsAi3LWXZzKGipfj6SRZD0BdO/evZD7YKX0wAOljsDMNlFEkF2dtULY2EvuxU70n4+IJZI+BTwo6R+5CyMi0peATZa+RFwP2TX6QrRpZeATnyh1BGa2Cdq1a8eKFSvo2LGjk30BRAQrVqygXbt2ea9T1EQfEUvS62uS7iK7xr5M0nYRsTR1zdfc428J0C1n9a6pbAkwoE75I8WM28rIb36TvX7966WNw8yapGvXrixevJjly5eXOpSq0a5dO7p27Zp3/aIleklbAq0i4u30/gjgMuBeYDjw8/R6T1rlXuB8SbeTDbxblb4M/AX4ac4AvCOA0cWK28rMpEnZqxO9WUVq06YNPXv2LHUYLVoxz+i7AHelrprNgP8XEf8j6VlgkqSzgYXA0FT/AbKf1s0n+3ndmQARsVLSj4FnU73LImJlEeM2MzOrGkVL9BHxMrBXPeUrgMPqKQ/gvAbaGgeMK3SMZmZm1c63wDUzM6tiTvRmZmZVzPe6t/LmW9+amW0Sn9GbmZlVMSd6K29jx2aTmZk1iRO9lbf7788mMzNrEid6MzOzKuZEb2ZmVsWc6M3MzKqYf15n5W2LLUodgZlZRXOit/L25z+XOgIzs4rmrnszM7Mq5kRv5e3HP84mMzNrEid6K29TpmSTmZk1iRO9mZlZFXOiNzMzq2JO9GZmZlXMP6+z8taxY6kjMDOraE70Vt7uuKPUEZiZVTR33ZuZmVUxJ3orb6NHZ5OZmTWJu+6tvD31VKkjMDOraD6jNzMzq2JO9GZmZlWs6IleUmtJz0u6P833lPSMpPmSJkraPJW3TfPz0/IeOW2MTuVzJR1Z7JjNzMyqRXOc0X8DeCln/nLgqojYCXgDODuVnw28kcqvSvWQ1Bs4BdgdOAr4jaTWzRC3lYOuXbPJzMyapKiJXlJX4FjgD2lewKHA5FTlJmBwen98mictPyzVPx64PSLej4hXgPnAfsWM28rIrbdmk5mZNUmxz+ivBr4LfJjmOwJvRsTaNL8Y2CG93wFYBJCWr0r1a8vrWcfMzMw2oGiJXtKXgNciYnqxtlFneyMlTZM0bfny5c2xSWsOo0Zlk5mZNUkxf0d/EHCcpGOAdsAngWuA9pI2S2ftXYElqf4SoBuwWNJmwDbAipzyGrnr1IqI64HrAfr16xdF2SNrfjNmlDoCM7OKVrQz+ogYHRFdI6IH2WC6hyNiGDAVGJKqDQfuSe/vTfOk5Q9HRKTyU9Ko/J7AzsDfixW3mZlZNSnFnfG+B9wu6SfA88ANqfwG4BZJ84GVZF8OiIjZkiYBc4C1wHkRsa75wzYzM6s8zZLoI+IR4JH0/mXqGTUfEe8BJzWw/hhgTPEiNDMzq06+172Vt112KXUEZmYVzYneytv115c6AjOziuZ73ZuZmVUxJ3orbyNHZpOZmTWJu+6tvP3zn6WOwMysovmM3szMrIo50ZuZmVWxvBK9pIPyKTMzM7Pyku8Z/a/yLDMrrL59s8nMzJpkg4PxJB0AHAh0lvStnEWfBFoXMzAzAK6+utQRmJlVtMZG3W8ObJXqbZ1T/hYfPZjGzMzMytQGE31EPAo8Kml8RCxsppjMPnLaadnrrbeWNg4zswqV7+/o20q6HuiRu05EHFqMoMxqLV5c6gjMzCpavon+j8BvgT8AfkSsmZlZhcg30a+NiOuKGomZmZkVXL4/r7tP0tclbSepQ81U1MjMzMxsk+V7Rj88vX4npyyAzxY2HLM6Djig1BGYmVW0vBJ9RPQsdiBm9frZz0odgZlZRcsr0Us6o77yiLi5sOGYmZlZIeXbdf+5nPftgMOA5wAneiuuE0/MXu+4o7RxmJlVqHy77i/InZfUHri9GAGZrWfFilJHYGZW0Zr6mNp3AF+3NzMzK3P5XqO/j2yUPWQPs9kNmFSsoMzMzKww8r1GPzbn/VpgYUT43qRmZmZlLt9r9I9K6sJHg/LmFS8ksxyHHVbqCMzMKlpe1+glDQX+DpwEDAWekbTBx9RKaifp75JekDRb0o9SeU9Jz0iaL2mipM1Teds0Pz8t75HT1uhUPlfSkU3cV6tE//mf2WRmZk2S72C8HwCfi4jhEXEGsB/Q2P++7wOHRsReQF/gKEn7A5cDV0XETsAbwNmp/tnAG6n8qlQPSb2BU4DdgaOA30hqnWfcZmZmLVq+ib5VRLyWM7+isXUjszrNtklTAIcCk1P5TcDg9P74NE9afpgkpfLbI+L9iHgFmE/2RcNagqOPziYzM2uSfAfj/Y+kvwAT0vzJwAONrZTOvKcDOwH/Dfwv8GZErE1VFgM7pPc7AIsAImKtpFVAx1T+dE6zuevkbmskMBKge/fuee6Wlb133y11BGZmFW2DZ+WSdpJ0UER8B/gdsGeangKub6zxiFgXEX2BrmRn4btucsQNb+v6iOgXEf06d+5crM2YmZlVlMa67q8G3gKIiDsj4lsR8S3grrQsLxHxJjAVOABoL6mmJ6ErsCS9XwJ0A0jLtyG7RFBbXs86ZmZmtgGNJfouETGrbmEq67GhFSV1TrfKRdIWwOHAS2QJv2bE/nDgnvT+Xj56HO4Q4OGIiFR+ShqV3xPYmewXAGZmZtaIxq7Rt9/Asi0aWXc74KZ0nb4VMCki7pc0B7hd0k+A54EbUv0bgFskzQdWko20JyJmS5oEzCG7Wc95EbGukW1btfjSl0odgZlZRWss0U+T9LWI+H1uoaSvkg2ya1BEzAT2rqf8ZeoZNR8R75H9Tr++tsYAYxqJ1arRt79d6gjMzCpaY4l+FHCXpGF8lNj7AZsDJxQxLjMzMyuADSb6iFgGHCjpi8AeqfhPEfFw0SMzAxgwIHt95JFSRmFmVrHyvdf9VLJBdGZmZlZBmvo8ejMzM6sATvRmZmZVzInezMysiuV7r3uz0hg6tNQRmJlVNCd6K29f/3qpIzAzq2juurfytmZNNpmZWZP4jN7K2zHHZK/+Hb2ZWZP4jN7MzKyKOdGbmZlVMSd6MzOzKuZEb2ZmVsU8GM/K24gRpY7AzKyiOdFbeXOiNzPbJO66t/L2+uvZZGZmTeIzeitvQ4Zkr/4dvZlZk/iM3szMrIo50ZuZmVUxJ3ozM7Mq5kRvZmZWxTwYz8rbueeWOgIzs4rmRG/l7eSTSx2BmVlFK1rXvaRukqZKmiNptqRvpPIOkh6UNC+9bpvKJelaSfMlzZS0T05bw1P9eZKGFytmK0OLFmWTmZk1STGv0a8FLoqI3sD+wHmSegPfB6ZExM7AlDQPcDSwc5pGAtdB9sUAuAToD+wHXFLz5cBagNNPzyYzM2uSoiX6iFgaEc+l928DLwE7AMcDN6VqNwGD0/vjgZsj8zTQXtJ2wJHAgxGxMiLeAB4EjipW3GZmZtWkWUbdS+oB7A08A3SJiKVp0atAl/R+ByC3j3ZxKmuovO42RkqaJmna8uXLC7sDZmZmFaroiV7SVsAdwKiIeCt3WUQEEIXYTkRcHxH9IqJf586dC9GkmZlZxStqopfUhizJ3xYRd6biZalLnvT6WipfAnTLWb1rKmuo3MzMzBpRzFH3Am4AXoqIK3MW3QvUjJwfDtyTU35GGn2/P7AqdfH/BThC0rZpEN4RqcxagosuyiYzM2uSYv6O/iDgdGCWpBmp7GLg58AkSWcDC4GhadkDwDHAfGANcCZARKyU9GPg2VTvsohYWcS4rZwMGlTqCMzMKlrREn1EPAGogcWH1VM/gPMaaGscMK5w0VnFmDs3e+3Vq7RxmJlVKN8Zz8rbf/xH9urn0ZuZNYkfamNmZlbFnOjNzMyqmBO9mZlZFXOiNzMzq2IejGfl7Yc/LHUEZmYVzYneytvAgaWOwMysornr3srbjBnZZGZmTeIzeitvo0Zlr/4dvZlZk/iM3szMrIo50ZuZmVUxJ3ozM7Mq5kRvZmZWxTwYz8rbT39a6gjMzCqaE72VtwMPLHUEZmYVzV33Vt6efDKbzMysSXxGb+Xt4ouzV/+O3sysSXxGb2ZmVsWc6M3MzKqYE72ZmVkVc6I3MzOrYh6MZ+Xt6qtLHYGZWUVzorfy1rdvqSMwM6to7rq38vbQQ9lkZmZNUrREL2mcpNckvZhT1kHSg5LmpddtU7kkXStpvqSZkvbJWWd4qj9P0vBixWtl6ic/ySYzM2uSYp7RjweOqlP2fWBKROwMTEnzAEcDO6dpJHAdZF8MgEuA/sB+wCU1Xw7MzMyscUVL9BHxGLCyTvHxwE3p/U3A4JzymyPzNNBe0nbAkcCDEbEyIt4AHuTjXx7MzMysAc19jb5LRCxN718FuqT3OwCLcuotTmUNlX+MpJGSpkmatnz58sJGbWZmVqFKNhgvIgKIArZ3fUT0i4h+nTt3LlSzZmZmFa25f163TNJ2EbE0dc2/lsqXAN1y6nVNZUuAAXXKH2mGOK1c/O53pY7AzKyiNfcZ/b1Azcj54cA9OeVnpNH3+wOrUhf/X4AjJG2bBuEdkcqspejVK5vMzKxJinZGL2kC2dl4J0mLyUbP/xyYJOlsYCEwNFV/ADgGmA+sAc4EiIiVkn4MPJvqXRYRdQf4WTW7777sddCg0sZhZlahipboI+LUBhYdVk/dAM5roJ1xwLgChmaV5IorslcnejOzJvGd8czMzKqYE72ZmVkV80NtNsLgxb/YuBXu6wCDrilOMGZmZnlwoi+iZ15Zyd13ztro9X725T5FiMbMzFoiJ3orb7fcUuoIzMwqmhO9lbdu3RqvY2ZmDfJgPCtvEydmk5mZNYnP6K28XXdd9nryyaWNw8ysQvmM3szMrIo50ZuZmVUxJ3ozM7Mq5kRvZmZWxTwYz8rb5MmljsDMrKI50Vt569Sp1BGYmVU0d91beRs/PpvMzKxJnOitvDnRm5ltEid6MzOzKuZEb2ZmVsWc6M3MzKqYE72ZmVkV88/rrLw98ECpIzAzq2hO9EU2ePEvNn6l+zrAoGsKH0wl+sQnSh2BmVlFc9e9lbff/CabzMysSZzorbxNmpRNZmbWJO66L0PPvLKSu++ctVHr/OzLfYoUjZmZVbKKOaOXdJSkuZLmS/p+qeMxMzOrBBVxRi+pNfDfwOHAYuBZSfdGxJzSRlY+Rm9kD0AN9wSYmVW3ikj0wH7A/Ih4GUDS7cDxQNUm+iaN1m+C0Xd+t1m201Rfe/0dAH7fxC8yG8NfetbnL49m1UERUeoYGiVpCHBURHw1zZ8O9I+I83PqjARGptlewNxN3Gwn4PVNbMM2no97afi4l4aPe/Or5mP+mYjoXLewUs7oGxUR1wPXF6o9SdMiol+h2rP8+LiXho97afi4N7+WeMwrZTDeEqBbznzXVGZmZmYbUCmJ/llgZ0k9JW0OnALcW+KYzMzMyl5FdN1HxFpJ5wN/AVoD4yJidpE3W7DLALZRfNxLw8e9NHzcm1+LO+YVMRjPzMzMmqZSuu7NzMysCZzozczMqliLT/SN3VpXUltJE9PyZyT1KEGYVSeP4z5C0nJJM9L01VLEWU0kjZP0mqQXG1guSdemv8lMSfs0d4zVKI/jPkDSqpzP+n81d4zVRlI3SVMlzZE0W9I36qnTYj7vLTrR59xa92igN3CqpN51qp0NvBEROwFXAZc3b5TVJ8/jDjAxIvqm6Q/NGmR1Gg8ctYHlRwM7p2kkcF0zxNQSjGfDxx3g8ZzP+mXNEFO1WwtcFBG9gf2B8+r5P6bFfN5bdKIn59a6EfFvoObWurmOB25K7ycDh0lSM8ZYjfI57lZgEfEYsHIDVY4Hbo7M00B7Sds1T3TVK4/jbgUWEUsj4rn0/m3gJWCHOtVazOe9pSf6HYBFOfOL+fiHobZORKwFVgEdmyW66pXPcQc4MXWpTZbUrZ7lVlj5/l2s8A6Q9IKkP0vavdTBVJN0uXVv4Jk6i1rM572lJ3orX/cBPSJiT+BBPupVMas2z5Hdo3wv4FfA3aUNp3pI2gq4AxgVEW+VOp5SaemJPp9b69bWkbQZsA2wolmiq16NHveIWBER76fZPwD7NlNsLZlvNV0CEfFWRKxO7x8A2kjqVOKwKp6kNmRJ/raIuLOeKi3m897SE30+t9a9Fxie3g8BHg7fZWhTNXrc61wrO47sGpsV173AGWk08v7AqohYWuqgqp2kT9eM+5G0H9n/yz6Z2ATpeN4AvBQRVzZQrcV83iviFrjF0tCtdSVdBkyLiHvJPiy3SJpPNqDmlNJFXB3yPO4XSjqObPTsSmBEyQKuEpImAAOATpIWA5cAbQAi4rfAA8AxwHxgDXBmaSKtLnkc9yHAuZLWAu8Cp/hkYpMdBJwOzJI0I5VdDHSHlvd59y1wzczMqlhL77o3MzOrak70ZmZmVcyJ3szMrIo50ZuZmVUxJ3ozM7Mq5kRvVgCS1qUnj70o6T5J7UsYywBJBxawvcG5DwSRdJmkgQVot+5T22YUot0NbE+SHpb0yTQfkm7NWb5ZemLi/XXWu1vS03XKLpW0pE7s7SX1kTS+WPtg1hRO9GaF8W568tgeZL/7P6+EsQwA6k306e6OG2sw2VMGAYiI/4qIh5oU2cflPrWtb912U3Ju1dB8QxrYz2OAF3JuhfoOsIekLdL84dS5M1r6wrYvsI2kz9Zp76o6sb8ZEbOArpK6NxajWXNxojcrvKdID8eQtKOk/5E0XdLjknZN5V0k3ZUeZPJCzRm4pG+lXoEXJY1KZT0kvSTp98qerf3XmuQk6UJlz9yeKen29ACPc4BvprPMgyWNl/RbSc8Av0hno9+uCTZtq0d6f0Zq6wVJt6S4jgN+mdrbMbU3JNU/TNLzkmYpe+5621S+QNKPJD2Xlu2a78FL+ztX0s3Ai8DBdea7SfplinuWpJPTegPSMb4XmFNP08OAe+qUPQAcm96fCkyos/zLZM9duJ38b5Z130bUNSu+iPDkydMmTsDq9Noa+CNwVJqfAuyc3vcnu4UywESyB23UrLMN2ZnjLGBLYCtgNtlTt3qQ3SGwb6o/CTgtvf8X0Da9b59eLwW+nRPbeOB+oHUDy19M29gd+CfQKZV3yFl/SJ32hgDtyJ7+tUsqvzlnnxYAF6T3Xwf+UM8xG0D2NMgZOdOOKZYPgf1TvbrzJ5I96Kg10AX4P2C71N47QM8G/kYLga1z/2bAnmSPn26Xtj8AuD+nzoPAwcAuwKyc8kvJzv5r4p6as+wg4L5SfyY9eaqZfEZvVhhbpFttvkqWfB5U9uSsA4E/pmW/I0tIAIcC1wFExLqIWAV8HrgrIt6J7CEnd5IlGYBXImJGej+dLPkBzARuk3Qa2ZeBhvwxItY1sg+Hpnqvp7gae4Z6rxTXP9P8TcAXcpbXPEgkN9666nbd/28qXxjZM8KpZ/7zwIR03JYBjwKfS8v+HhGvNLCtDpE9m7xWRMxMsZ1KdnZfS1IXYGfgibSPH0jaI6fKVTlxfzGn/DVg+wZiMGt2TvRmhfFuRPQFPgOI7Bp9K+DNOolstya2/37O+3V89JyKY4H/BvYBnt3ANfh3ct6vZf1/++2aGFNjamLOjTdf7zQyn+96udY2cH3/XmAsH++2HwpsC7wiaQEffSFoTDuye9ablQUnerMCiog1wIXARWQPynhF0klQO5Bsr1R1CnBuKm8taRvgcWCwpE9I2hI4IZXVKyWtbhExFfgeWff/VsDbwNYbCHMB2RcDJO0D9EzlDwMnSeqYlnVI5Q21NxfoIWmnNH862dl1sT0OnJyOW2eyXoS/57HeXKDugDqAccCPIhtIl+tUskswPSKiB9mllXyuve9CdjnErCw40ZsVWEQ8T9alfirZALCzJb1Ads39+FTtG8AXJc0i69ruHRHPkV3//jvwDNl17ec3sKnWwK2pjeeBayPiTbLBYCfUDMarZ707gA6SZgPnk12XJyJmA2OAR1O8NY/3vB34Thp0t2POfr5H9sSvP6YYPgR+m+dhqnGw1v+J2pA81rmL7Pi+QPbl5LsR8Woe6/2J7Br8eiJicURcm1uWBid+Bng6p94rwCpJ/VPRN+vE3iOVfzFty6ws+Ol1ZtYiSNoOuDkiDi/iNtqS9Wp8PiI2NGbCrNn4jN7MWoSIWAr8XumGOUXSHfi+k7yVE5/Rm5mZVTGf0ZuZmVUxJ3ozM7Mq5kRvZmZWxZzozczMqpgTvZmZWRX7/2jFHc/sHhwhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Anomaly Detection on Imbalanced Time-Series Data Using an LSTM Autoencoder\n",
    "\n",
    "Steps:\n",
    "1. Read and organize the Excel data.\n",
    "2. Extract time-series (S1, S2) from each batch.\n",
    "3. Label data as â€œnormalâ€ (for training) or â€œanomalousâ€ (low/high).\n",
    "4. Scale and pad sequences.\n",
    "5. Build an LSTM autoencoder that is trained only on normal sequences.\n",
    "6. Compute a reconstruction error threshold from the training data.\n",
    "7. Evaluate the model on all data (normal and anomalies) using the reconstruction error.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Read Excel Data and Organize It\n",
    "# ---------------------------\n",
    "\n",
    "# Assumes that you have a dictionary named \"bounds\" defined in bounds.py\n",
    "# which holds the threshold values for your data.\n",
    "from bounds import bounds\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack the tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            # Interpolate and fill missing values\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "            # Drop any rows that still contain NaN values\n",
    "            MDR.dropna(inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "# Organize the data\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(f\"# low: {len({k: v for k, v in data.items() if v['class']=='low'})}\")\n",
    "print(f\"# high: {len({k: v for k, v in data.items() if v['class']=='high'})}\")\n",
    "print(f\"# normal: {len({k: v for k, v in data.items() if v['class']=='normal'})}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Extract Sequences and Prepare Labels\n",
    "# ---------------------------\n",
    "# We will extract the S1 and S2 time-series for each batch.\n",
    "# For anomaly detection, we treat â€œnormalâ€ as class 0 and â€œlowâ€/â€œhighâ€ as anomalies (class 1).\n",
    "\n",
    "all_sequences = []\n",
    "all_labels = []  # 0 for normal, 1 for anomaly\n",
    "original_lengths = []  # To keep track of each sequence's true length (before padding)\n",
    "\n",
    "for batch_id, info in data.items():\n",
    "    # Get the (S1, S2) sequence from the MDR DataFrame\n",
    "    seq = info[\"MDR\"][[\"S1\", \"S2\"]].values.astype(np.float32)\n",
    "    if len(seq) == 0:\n",
    "        continue\n",
    "    all_sequences.append(seq)\n",
    "    original_lengths.append(len(seq))\n",
    "    label = 0 if info[\"class\"] == \"normal\" else 1\n",
    "    all_labels.append(label)\n",
    "\n",
    "print(f\"Total sequences: {len(all_sequences)}\")\n",
    "print(f\"Class distribution: {Counter(all_labels)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Scale the Data (Using Only Normal Data for Fitting)\n",
    "# ---------------------------\n",
    "# We will fit a StandardScaler on the normal sequences (non-anomalous).\n",
    "normal_sequences = [seq for seq, lab in zip(all_sequences, all_labels) if lab == 0]\n",
    "\n",
    "# Concatenate all normal sequence data points to fit the scaler\n",
    "normal_data_points = np.vstack(normal_sequences)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(normal_data_points)\n",
    "\n",
    "# Now apply the scaler to all sequences. If any NaNs appear, replace them with 0.\n",
    "scaled_sequences = []\n",
    "for seq in all_sequences:\n",
    "    scaled_seq = scaler.transform(seq)\n",
    "    if np.isnan(scaled_seq).any():\n",
    "        print(\"NaN found after scaling. Replacing NaN with 0.\")\n",
    "        scaled_seq = np.nan_to_num(scaled_seq, nan=0.0)\n",
    "    scaled_sequences.append(scaled_seq)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Pad the Sequences\n",
    "# ---------------------------\n",
    "# To handle variable-length sequences we pad them to the length of the longest sequence.\n",
    "# We choose a pad value that is unlikely to be produced by the scaler.\n",
    "pad_value = -999.0\n",
    "max_seq_len = max(len(seq) for seq in scaled_sequences)\n",
    "print(\"Max sequence length:\", max_seq_len)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    scaled_sequences,\n",
    "    maxlen=max_seq_len,\n",
    "    dtype='float32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=pad_value\n",
    ")\n",
    "\n",
    "# Convert labels to numpy array\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Split Training (Normal Only) and Validation Data\n",
    "# ---------------------------\n",
    "# We train the autoencoder only on the normal data.\n",
    "# First, get indices of normal samples.\n",
    "normal_idx = np.where(all_labels == 0)[0]\n",
    "X_normal = padded_sequences[normal_idx]\n",
    "\n",
    "# Split normal data into training and validation sets\n",
    "X_train, X_val = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
    "print(\"Training on normal sequences:\", X_train.shape)\n",
    "print(\"Validation on normal sequences:\", X_val.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Build the LSTM Autoencoder\n",
    "# ---------------------------\n",
    "latent_dim = 16\n",
    "\n",
    "# Use gradient clipping in the optimizer to help with exploding gradients\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "\n",
    "model = Sequential([\n",
    "    # The Masking layer will ignore the padded timesteps (with value = pad_value)\n",
    "    Masking(mask_value=pad_value, input_shape=(max_seq_len, 2)),\n",
    "    # Use the default activation ('tanh') for numerical stability\n",
    "    LSTM(latent_dim, return_sequences=False),\n",
    "    RepeatVector(max_seq_len),\n",
    "    LSTM(latent_dim, return_sequences=True),\n",
    "    TimeDistributed(Dense(2))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mae')\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Train the Autoencoder\n",
    "# ---------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stop],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.legend()\n",
    "plt.title(\"Training History\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Determine a Reconstruction Error Threshold\n",
    "# ---------------------------\n",
    "# Compute reconstruction errors on the training (normal) set.\n",
    "def compute_reconstruction_error(original, reconstructed, pad_val=pad_value):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error per sequence,\n",
    "    ignoring the padded timesteps.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    # Loop over each sequence\n",
    "    for orig_seq, recon_seq in zip(original, reconstructed):\n",
    "        # Create a mask: True for timesteps that are NOT padded.\n",
    "        mask = ~np.all(orig_seq == pad_val, axis=-1)\n",
    "        if np.sum(mask) == 0:\n",
    "            errors.append(0.0)\n",
    "        else:\n",
    "            error = np.mean(np.abs(orig_seq[mask] - recon_seq[mask]))\n",
    "            errors.append(error)\n",
    "    return np.array(errors)\n",
    "\n",
    "# Get reconstruction error for X_train\n",
    "X_train_pred = model.predict(X_train)\n",
    "train_errors = compute_reconstruction_error(X_train, X_train_pred)\n",
    "print(\"Training reconstruction error (mean):\", np.mean(train_errors))\n",
    "\n",
    "# Set threshold as mean + 3*std (adjust multiplier as needed)\n",
    "threshold = np.mean(train_errors) + 3 * np.std(train_errors)\n",
    "print(\"Anomaly detection threshold: {:.4f}\".format(threshold))\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Evaluate on All Data (Normal and Anomalies)\n",
    "# ---------------------------\n",
    "X_all = padded_sequences  # All sequences (normal and anomalies)\n",
    "y_true = all_labels       # 0: normal, 1: anomaly\n",
    "\n",
    "# Reconstruct all sequences\n",
    "X_all_pred = model.predict(X_all)\n",
    "all_errors = compute_reconstruction_error(X_all, X_all_pred)\n",
    "\n",
    "# Classify sequences as anomalous if reconstruction error > threshold\n",
    "y_pred = (all_errors > threshold).astype(int)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Anomaly\"]))\n",
    "\n",
    "# ---------------------------\n",
    "# 10. Visualize the Error Distribution\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(all_errors[y_true == 0], bins=30, alpha=0.6, label='Normal')\n",
    "plt.hist(all_errors[y_true == 1], bins=30, alpha=0.6, label='Anomaly')\n",
    "plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel(\"Reconstruction Error (MAE)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Reconstruction Error Distribution\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20528, 43)\n",
      "Total runs after organization: 8339\n",
      "Normal runs: 7297\n",
      "Abnormal runs: 1042\n",
      "Training runs: 5837\n",
      "Validation runs: 1460\n",
      "Training min (S1, S2): [-4.71 -4.88]\n",
      "Training max (S1, S2): [13.64  7.28]\n",
      "Padding all sequences to length: 304\n",
      "Padded training data shape: (5837, 304, 2)\n",
      "Padded validation data shape: (1460, 304, 2)\n",
      "Padded testing data shape: (1042, 304, 2)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_4 (Masking)         (None, 304, 2)            0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 64)                17152     \n",
      "                                                                 \n",
      " repeat_vector_4 (RepeatVect  (None, 304, 64)          0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 304, 64)           33024     \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 304, 2)           130       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,306\n",
      "Trainable params: 50,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0651\n",
      "Epoch 001: Train MSE: 0.0165, Val MSE: 0.0165, Test MSE: 0.0168\n",
      "183/183 [==============================] - 123s 652ms/step - loss: 0.0651 - val_loss: 0.0545\n",
      "Epoch 2/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 260869360.0000\n",
      "Epoch 002: Train MSE: 0.0219, Val MSE: 0.0220, Test MSE: 0.0224\n",
      "183/183 [==============================] - 118s 647ms/step - loss: 260869360.0000 - val_loss: 0.0585\n",
      "Epoch 3/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0576\n",
      "Epoch 003: Train MSE: 0.0144, Val MSE: 0.0144, Test MSE: 0.0146\n",
      "183/183 [==============================] - 118s 645ms/step - loss: 0.0576 - val_loss: 0.0529\n",
      "Epoch 4/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 54045.1562\n",
      "Epoch 004: Train MSE: 0.0159, Val MSE: 0.0159, Test MSE: 0.0161\n",
      "183/183 [==============================] - 118s 646ms/step - loss: 54045.1562 - val_loss: 0.0545\n",
      "Epoch 5/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0566\n",
      "Epoch 005: Train MSE: 0.0155, Val MSE: 0.0155, Test MSE: 0.0157\n",
      "183/183 [==============================] - 118s 646ms/step - loss: 0.0566 - val_loss: 0.0537\n",
      "Epoch 6/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0564\n",
      "Epoch 006: Train MSE: 0.0157, Val MSE: 0.0157, Test MSE: 0.0158\n",
      "183/183 [==============================] - 118s 647ms/step - loss: 0.0564 - val_loss: 0.0534\n",
      "Epoch 7/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0562\n",
      "Epoch 007: Train MSE: 0.0150, Val MSE: 0.0150, Test MSE: 0.0152\n",
      "183/183 [==============================] - 118s 646ms/step - loss: 0.0562 - val_loss: 0.0532\n",
      "Epoch 8/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 4.7900\n",
      "Epoch 008: Train MSE: 15405043154944.0000, Val MSE: 0.0367, Test MSE: 0.0389\n",
      "183/183 [==============================] - 118s 644ms/step - loss: 4.7900 - val_loss: 0.0708\n",
      "Epoch 9/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0745\n",
      "Epoch 009: Train MSE: 0.0158, Val MSE: 0.0159, Test MSE: 0.0161\n",
      "183/183 [==============================] - 118s 645ms/step - loss: 0.0745 - val_loss: 0.0535\n",
      "Epoch 10/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0533\n",
      "Epoch 010: Train MSE: 0.0126, Val MSE: 0.0126, Test MSE: 0.0129\n",
      "183/183 [==============================] - 120s 657ms/step - loss: 0.0533 - val_loss: 0.0503\n",
      "Epoch 11/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0492\n",
      "Epoch 011: Train MSE: 0.0045, Val MSE: 0.0045, Test MSE: 0.0049\n",
      "183/183 [==============================] - 118s 645ms/step - loss: 0.0492 - val_loss: 0.0443\n",
      "Epoch 12/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0472\n",
      "Epoch 012: Train MSE: 0.0035, Val MSE: 0.0035, Test MSE: 0.0037\n",
      "183/183 [==============================] - 119s 650ms/step - loss: 0.0472 - val_loss: 0.0438\n",
      "Epoch 13/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0469\n",
      "Epoch 013: Train MSE: 0.0029, Val MSE: 0.0031, Test MSE: 0.0033\n",
      "183/183 [==============================] - 118s 644ms/step - loss: 0.0469 - val_loss: 0.0436\n",
      "Epoch 14/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0466\n",
      "Epoch 014: Train MSE: 0.0036, Val MSE: 0.0036, Test MSE: 0.0039\n",
      "183/183 [==============================] - 118s 647ms/step - loss: 0.0466 - val_loss: 0.0437\n",
      "Epoch 15/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0463\n",
      "Epoch 015: Train MSE: 0.0033, Val MSE: 0.0033, Test MSE: 0.0035\n",
      "183/183 [==============================] - 118s 647ms/step - loss: 0.0463 - val_loss: 0.0431\n",
      "Epoch 16/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0462\n",
      "Epoch 016: Train MSE: 0.0026, Val MSE: 0.0026, Test MSE: 0.0029\n",
      "183/183 [==============================] - 118s 645ms/step - loss: 0.0462 - val_loss: 0.0430\n",
      "Epoch 17/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0460\n",
      "Epoch 017: Train MSE: 0.0023, Val MSE: 0.0024, Test MSE: 0.0026\n",
      "183/183 [==============================] - 118s 646ms/step - loss: 0.0460 - val_loss: 0.0428\n",
      "Epoch 18/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 018: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 646ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 019: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 645ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 020: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 647ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 021: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 646ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 022: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 647ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 023: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 646ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 024: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 645ms/step - loss: nan - val_loss: nan\n",
      "Epoch 25/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 025: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 646ms/step - loss: nan - val_loss: nan\n",
      "Epoch 26/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 026: Train MSE: nan, Val MSE: nan, Test MSE: nan\n",
      "183/183 [==============================] - 118s 647ms/step - loss: nan - val_loss: nan\n",
      "Epoch 27/100\n",
      "183/183 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 027: Train MSE: 0.0023, Val MSE: 0.0024, Test MSE: 0.0026\n",
      "183/183 [==============================] - 118s 646ms/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Read Excel Data and Organize It\n",
    "# ---------------------------\n",
    "# Assumes that you have a dictionary named \"bounds\" defined in bounds.py\n",
    "# which holds the threshold values for your data.\n",
    "from bounds import bounds\n",
    "\n",
    "file_name = \"DataOn2025Jan08.xlsx\"\n",
    "df1 = pd.read_excel(file_name, sheet_name=\"NES170K07Line2\")\n",
    "df2 = pd.read_excel(file_name, sheet_name=\"NES170K07Line1\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Get t5 thresholds from bounds dictionary\n",
    "t5_lb = bounds[\"170K\"][0]\n",
    "t5_ub = bounds[\"170K\"][1]\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation, replacing 'nan' with None.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"nan\", \"None\")\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def organized_data(df, t5_lb, t5_ub):\n",
    "    \"\"\"\n",
    "    Process each row to extract the time-series (MDR), target t5,\n",
    "    and assign a region label ('low', 'normal', 'high') based on thresholds.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['t5']):\n",
    "            continue\n",
    "        batch_number = row[\"batch_number\"]\n",
    "        data[batch_number] = {\"MDR\": None, \"t5\": row[\"t5\"], \"class\": None}\n",
    "\n",
    "        t_S1 = safe_literal_eval(row[\"MDRTorqueS1\"])\n",
    "        t_S2 = safe_literal_eval(row[\"MDRTorqueS2\"])\n",
    "        if t_S1 is not None and t_S2 is not None:\n",
    "            # Unpack the tuples (time, value)\n",
    "            t_vals, S1 = zip(*t_S1)\n",
    "            t_vals, S2 = zip(*t_S2)\n",
    "            t_vals, S1, S2 = list(t_vals), list(S1), list(S2)\n",
    "            # Exclude the first element as indicated\n",
    "            MDR = pd.DataFrame({\n",
    "                \"time\": t_vals[1:],\n",
    "                \"S1\": S1[1:],\n",
    "                \"S2\": S2[1:],\n",
    "            })\n",
    "            # Interpolate and fill missing values\n",
    "            MDR.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "            MDR.fillna(method=\"bfill\", inplace=True)\n",
    "            MDR.fillna(method=\"ffill\", inplace=True)\n",
    "            # Drop any rows that still contain NaN values\n",
    "            MDR.dropna(inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        data[batch_number][\"MDR\"] = MDR\n",
    "\n",
    "        # Assign class label based on t5 thresholds\n",
    "        if row[\"t5\"] < t5_lb:\n",
    "            data[batch_number][\"class\"] = \"low\"\n",
    "        elif row[\"t5\"] > t5_ub:\n",
    "            data[batch_number][\"class\"] = \"high\"\n",
    "        else:\n",
    "            data[batch_number][\"class\"] = \"normal\"\n",
    "\n",
    "    # Remove batches with empty MDR\n",
    "    data = {k: v for k, v in data.items() if v[\"MDR\"] is not None and not v[\"MDR\"].empty}\n",
    "    return data\n",
    "\n",
    "# Organize the data\n",
    "data = organized_data(df, t5_lb, t5_ub)\n",
    "print(\"Total runs after organization:\", len(data))\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Split Data into Training/Validation (normal) and Testing (abnormal)\n",
    "# ---------------------------\n",
    "# Normal runs (t5 between t5_lb and t5_ub) are used for training and validation.\n",
    "normal_data = {k: v for k, v in data.items() if v[\"class\"] == \"normal\"}\n",
    "abnormal_data = {k: v for k, v in data.items() if v[\"class\"] != \"normal\"}\n",
    "\n",
    "# Extract the MDR time series (only S1 and S2) as numpy arrays.\n",
    "normal_sequences = [v[\"MDR\"][[\"S1\", \"S2\"]].values for v in normal_data.values()]\n",
    "abnormal_sequences = [v[\"MDR\"][[\"S1\", \"S2\"]].values for v in abnormal_data.values()]\n",
    "\n",
    "print(\"Normal runs:\", len(normal_sequences))\n",
    "print(\"Abnormal runs:\", len(abnormal_sequences))\n",
    "\n",
    "# Split normal data into training (80%) and validation (20%).\n",
    "train_seq, val_seq = train_test_split(normal_sequences, test_size=0.2, random_state=42)\n",
    "print(\"Training runs:\", len(train_seq))\n",
    "print(\"Validation runs:\", len(val_seq))\n",
    "# Testing data will be all abnormal runs.\n",
    "test_seq = abnormal_sequences\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Normalize Data Using Training Min/Max\n",
    "# ---------------------------\n",
    "# Compute global min and max for S1 and S2 from the training data.\n",
    "train_concat = np.concatenate(train_seq, axis=0)  # shape (total_timesteps, 2)\n",
    "min_vals = train_concat.min(axis=0)\n",
    "max_vals = train_concat.max(axis=0)\n",
    "print(\"Training min (S1, S2):\", min_vals)\n",
    "print(\"Training max (S1, S2):\", max_vals)\n",
    "\n",
    "def normalize_seq(seq, min_vals, max_vals):\n",
    "    \"\"\"Normalize a sequence given min and max values (per feature),\n",
    "    avoiding division by zero if a feature is constant.\"\"\"\n",
    "    denom = max_vals - min_vals\n",
    "    # Avoid division by zero: if any denom is 0, set it to 1.\n",
    "    denom = np.where(denom == 0, 1, denom)\n",
    "    return (seq - min_vals) / denom\n",
    "\n",
    "# Normalize all sequences.\n",
    "train_seq = [normalize_seq(seq, min_vals, max_vals) for seq in train_seq]\n",
    "val_seq   = [normalize_seq(seq, min_vals, max_vals) for seq in val_seq]\n",
    "test_seq  = [normalize_seq(seq, min_vals, max_vals) for seq in test_seq]\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Pad Sequences to Handle Varying Lengths\n",
    "# ---------------------------\n",
    "# Find the maximum sequence length across all datasets.\n",
    "all_lengths = [len(seq) for seq in train_seq + val_seq + test_seq]\n",
    "max_length = max(all_lengths)\n",
    "print(\"Padding all sequences to length:\", max_length)\n",
    "\n",
    "# IMPORTANT: Since after normalization valid values lie roughly in [0,1],\n",
    "# we choose a pad value (here -1) that is outside that range.\n",
    "train_seq_padded = pad_sequences(train_seq, maxlen=max_length, dtype=\"float32\",\n",
    "                                 padding=\"post\", truncating=\"post\", value=-1.0)\n",
    "val_seq_padded   = pad_sequences(val_seq, maxlen=max_length, dtype=\"float32\",\n",
    "                                 padding=\"post\", truncating=\"post\", value=-1.0)\n",
    "test_seq_padded  = pad_sequences(test_seq, maxlen=max_length, dtype=\"float32\",\n",
    "                                 padding=\"post\", truncating=\"post\", value=-1.0)\n",
    "\n",
    "print(\"Padded training data shape:\", train_seq_padded.shape)\n",
    "print(\"Padded validation data shape:\", val_seq_padded.shape)\n",
    "print(\"Padded testing data shape:\", test_seq_padded.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Build the LSTM Autoencoder Model\n",
    "# ---------------------------\n",
    "# The model uses a Masking layer (to ignore the padded timesteps) followed by an encoder LSTM,\n",
    "# a RepeatVector (to repeat the latent code for each timestep), and a decoder LSTM.\n",
    "model = Sequential([\n",
    "    # Tell the network to ignore time steps with the pad value (-1.0)\n",
    "    Masking(mask_value=-1.0, input_shape=(max_length, 2)),\n",
    "    LSTM(64, activation=\"relu\", return_sequences=False),\n",
    "    RepeatVector(max_length),\n",
    "    LSTM(64, activation=\"relu\", return_sequences=True),\n",
    "    TimeDistributed(Dense(2))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Define a Callback to Compute & Print Masked Reconstruction MSE\n",
    "# ---------------------------\n",
    "class MSECallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, train_data, val_data, test_data):\n",
    "        super(MSECallback, self).__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def compute_masked_mse(self, y_true, y_pred):\n",
    "        # Convert to tensors if not already.\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        # Create a mask: we assume that the pad value is -1.0 (check the first feature)\n",
    "        mask = tf.not_equal(y_true[:, :, 0], -1.0)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        # Compute per-timestep MSE (averaged over features)\n",
    "        se = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "        mse = tf.reduce_sum(se * mask) / tf.reduce_sum(mask)\n",
    "        return mse\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_pred = self.model.predict(self.train_data, verbose=0)\n",
    "        val_pred   = self.model.predict(self.val_data, verbose=0)\n",
    "        test_pred  = self.model.predict(self.test_data, verbose=0)\n",
    "\n",
    "        train_mse = self.compute_masked_mse(self.train_data, train_pred)\n",
    "        val_mse   = self.compute_masked_mse(self.val_data, val_pred)\n",
    "        test_mse  = self.compute_masked_mse(self.test_data, test_pred)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1:03d}: Train MSE: {train_mse.numpy():.4f}, \"\n",
    "              f\"Val MSE: {val_mse.numpy():.4f}, Test MSE: {test_mse.numpy():.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Train the Autoencoder\n",
    "# ---------------------------\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "mse_callback = MSECallback(train_seq_padded, val_seq_padded, test_seq_padded)\n",
    "\n",
    "history = model.fit(train_seq_padded, train_seq_padded,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_seq_padded, val_seq_padded),\n",
    "                    callbacks=[early_stopping, mse_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
